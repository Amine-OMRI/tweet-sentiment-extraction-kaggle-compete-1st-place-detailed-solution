{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "first_level_model_roberta_base.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "etmylaX2Fb-j",
        "BrP6RBNKFqd9",
        "sPit1dlVHEIs"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2a5a6a4853c647d5ab4e6f2e421c5b21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ea33407f48d84529a3cedce8ff4eb717",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_86e84d68c27d472b8a7a0d57a6e22c43",
              "IPY_MODEL_a042d53d0d374fbba87c5126bc89edb1"
            ]
          }
        },
        "ea33407f48d84529a3cedce8ff4eb717": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "86e84d68c27d472b8a7a0d57a6e22c43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fb2ad4da78f8468f8b5e0bacdfdb6a44",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 571,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 571,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_67b4d3d78dee4cc39c7679217f24bca9"
          }
        },
        "a042d53d0d374fbba87c5126bc89edb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f7174c2b850b4684b80adf245f073187",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 571/571 [00:49&lt;00:00, 11.6B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b40d221cc11e42abacb6d79d87b64e15"
          }
        },
        "fb2ad4da78f8468f8b5e0bacdfdb6a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "67b4d3d78dee4cc39c7679217f24bca9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f7174c2b850b4684b80adf245f073187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b40d221cc11e42abacb6d79d87b64e15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da3165f8a7bb4f1ba6002a3dd1839b23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ad49480862ce44e696e8ab8d60f4850c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fd76536cf49f4f9e9dcca7b8c6c9c804",
              "IPY_MODEL_97fb8388273247cea7095fce3ccd4789"
            ]
          }
        },
        "ad49480862ce44e696e8ab8d60f4850c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd76536cf49f4f9e9dcca7b8c6c9c804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2fdf7552d81344c6b23bbd1e82f34b80",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 496313727,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 496313727,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6991e98fa56846f98fdf918568cafd89"
          }
        },
        "97fb8388273247cea7095fce3ccd4789": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_79c265a4eeda434db25543b711583219",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 496M/496M [00:11&lt;00:00, 44.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c5c53ebd65b84df79429c42c41a456f0"
          }
        },
        "2fdf7552d81344c6b23bbd1e82f34b80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6991e98fa56846f98fdf918568cafd89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "79c265a4eeda434db25543b711583219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c5c53ebd65b84df79429c42c41a456f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "311e02e4ab79455f96a6a69d882cf44c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8fbea1556436405c82e84a37e15ee0c3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_be8768226ed74c338a38576cc1639b61",
              "IPY_MODEL_f195b12fa5be48fca00f7c2056222609"
            ]
          }
        },
        "8fbea1556436405c82e84a37e15ee0c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "be8768226ed74c338a38576cc1639b61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d40ab0eeaee6478aa0eec0bc7a5d883f",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 172,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 172,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2f1b9d41b21047ec94130b11e14337d6"
          }
        },
        "f195b12fa5be48fca00f7c2056222609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_71e0c96ecb90475a9767aa517b1ff34d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 172/172 [01:13&lt;00:00,  2.33it/s, jaccard=0.716]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_83643d9c675e43d0adf21bb7e7bb4bd2"
          }
        },
        "d40ab0eeaee6478aa0eec0bc7a5d883f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2f1b9d41b21047ec94130b11e14337d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "71e0c96ecb90475a9767aa517b1ff34d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "83643d9c675e43d0adf21bb7e7bb4bd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e67e719ed34a4b07a0591d376313ceeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a19299dbe1cb457fb61a2bd60a044123",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e2037ef67ba3407e9883a32f1a9ae9e9",
              "IPY_MODEL_ef8f28882f6a4689ac35c8b7dc69dc8a"
            ]
          }
        },
        "a19299dbe1cb457fb61a2bd60a044123": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e2037ef67ba3407e9883a32f1a9ae9e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_80baee64c83f4c05b13026fdd6c3928f",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 172,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 172,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_95fe9696a2b74d7ea8c85a71ed0ea401"
          }
        },
        "ef8f28882f6a4689ac35c8b7dc69dc8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1dd0410fc9704a9caa064a47ee8936bc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 172/172 [00:29&lt;00:00,  5.74it/s, jaccard=0.711]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ba08cfa1331d47489b03c860ed273728"
          }
        },
        "80baee64c83f4c05b13026fdd6c3928f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "95fe9696a2b74d7ea8c85a71ed0ea401": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1dd0410fc9704a9caa064a47ee8936bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ba08cfa1331d47489b03c860ed273728": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c649b48206e8461ab986a31eff41663d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_de48ecab2e9a43a2984091f852f12b25",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d9ccaad734f8466f9c609ba4c19ae320",
              "IPY_MODEL_79deeeeeaa964a06bc9f3ad5462ca312"
            ]
          }
        },
        "de48ecab2e9a43a2984091f852f12b25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d9ccaad734f8466f9c609ba4c19ae320": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1790d4b810e6479fb0986a33c7d6d89d",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 172,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 172,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3e0abbd9b372444a9ecb25a616239688"
          }
        },
        "79deeeeeaa964a06bc9f3ad5462ca312": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8388209c0eea421a864e786bf0e88db2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 172/172 [01:10&lt;00:00,  2.42it/s, jaccard=0.716]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e28785f8a2c4d26b9dedb98f993c35c"
          }
        },
        "1790d4b810e6479fb0986a33c7d6d89d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3e0abbd9b372444a9ecb25a616239688": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8388209c0eea421a864e786bf0e88db2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e28785f8a2c4d26b9dedb98f993c35c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6ae593eea4ea4bf29ccb9207bf39a987": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0a8fc24b90b944d1ba355ecbb00bfa58",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dc414fdaee294c9db8c677b27a81707f",
              "IPY_MODEL_b53ea0fc5b7e42f6961cc98b3b257128"
            ]
          }
        },
        "0a8fc24b90b944d1ba355ecbb00bfa58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dc414fdaee294c9db8c677b27a81707f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9f73581c6f304a2381a54c3aded4fb52",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 172,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 172,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ad0874d836e3493cb84f49eb88448e72"
          }
        },
        "b53ea0fc5b7e42f6961cc98b3b257128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_86d93fca9e4a417187f9512e505ccf8c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 172/172 [00:29&lt;00:00,  5.74it/s, jaccard=0.717]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0c571a2eff364548b85ad2d1a1ccc4cd"
          }
        },
        "9f73581c6f304a2381a54c3aded4fb52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ad0874d836e3493cb84f49eb88448e72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "86d93fca9e4a417187f9512e505ccf8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0c571a2eff364548b85ad2d1a1ccc4cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e34c097bd1a8400cab7a702b06a9143c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1eb203e1db144517ae6bf803be68d76b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c555a66b7f44451e82c325428f234bc4",
              "IPY_MODEL_5ac0aaff737641eca49308824bb94603"
            ]
          }
        },
        "1eb203e1db144517ae6bf803be68d76b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c555a66b7f44451e82c325428f234bc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1ec1aa1825834a7c96fddcafac8ee72b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 172,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 172,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ae9af73a41084fc2bbf736f4afd9f5dd"
          }
        },
        "5ac0aaff737641eca49308824bb94603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_24ea29ea46fe45f5876e16e9bbc5daca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 172/172 [29:37&lt;00:00, 10.33s/it, jaccard=0.711]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_806bf2cfe63e4b909522ad2c6b6f6d8c"
          }
        },
        "1ec1aa1825834a7c96fddcafac8ee72b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ae9af73a41084fc2bbf736f4afd9f5dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "24ea29ea46fe45f5876e16e9bbc5daca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "806bf2cfe63e4b909522ad2c6b6f6d8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ed2e8946b1004d4ca98dfadfa33115f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_983766538d1c4e94b5c1599a68ec02d5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_50c37752036240f098ea6979b4f01416",
              "IPY_MODEL_1bcc8d683c504de2a1d2ac53c49b84a0"
            ]
          }
        },
        "983766538d1c4e94b5c1599a68ec02d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "50c37752036240f098ea6979b4f01416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1b0d52f94b944e74b65b3bbd854e58ee",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 571,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 571,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8df86989d6fe4dcdac94e453f03ebc3e"
          }
        },
        "1bcc8d683c504de2a1d2ac53c49b84a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d1ad2c4487d84b8c94a19ce2e53a393a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 571/571 [00:00&lt;00:00, 2.33kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_41678fc6278e4acc9446a04a5d936445"
          }
        },
        "1b0d52f94b944e74b65b3bbd854e58ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8df86989d6fe4dcdac94e453f03ebc3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d1ad2c4487d84b8c94a19ce2e53a393a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "41678fc6278e4acc9446a04a5d936445": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "23007c4d13ba48839736aaa552a33750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_95f23dc7fd694a1aaf802da9e4f697aa",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7faab71086174139b707fb5332ea2906",
              "IPY_MODEL_9cf511a065854bfeb286a3ab5d2be7ab"
            ]
          }
        },
        "95f23dc7fd694a1aaf802da9e4f697aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7faab71086174139b707fb5332ea2906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_997ea70e664b4d2aa193ba8d6f3d5569",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 496313727,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 496313727,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ced557f1d8244f6abaabf8e2011dab49"
          }
        },
        "9cf511a065854bfeb286a3ab5d2be7ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cac3e239cfa34eb3b7d6824b92049515",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 496M/496M [00:07&lt;00:00, 67.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_342d70f33ebb469c9f18a9f3cfefb843"
          }
        },
        "997ea70e664b4d2aa193ba8d6f3d5569": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ced557f1d8244f6abaabf8e2011dab49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cac3e239cfa34eb3b7d6824b92049515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "342d70f33ebb469c9f18a9f3cfefb843": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1023bec17f3048f2994587ab2ca923ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4f836dfd3d614d37a6e12828b9e26dea",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0288fee9c3674c198d5e21621aac5273",
              "IPY_MODEL_56eabcc0b14d4fa6a0b5b2323b3a27a5"
            ]
          }
        },
        "4f836dfd3d614d37a6e12828b9e26dea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0288fee9c3674c198d5e21621aac5273": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9c97ceae839b48029d9f89ed8021d1a2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 571,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 571,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d0732c192a784c53b86a68f3a16254a6"
          }
        },
        "56eabcc0b14d4fa6a0b5b2323b3a27a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_556a4510c3234b38a87c3743b8df9c97",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 571/571 [00:08&lt;00:00, 69.9B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ec4bd0f01be341e88fc41673825fbaca"
          }
        },
        "9c97ceae839b48029d9f89ed8021d1a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d0732c192a784c53b86a68f3a16254a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "556a4510c3234b38a87c3743b8df9c97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ec4bd0f01be341e88fc41673825fbaca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2d6a2ea3e1454d28bd5a9760e19a4ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6cad7320919248cab53cba7a3eb47225",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_18287140c5324a7e85b01460e5e889f5",
              "IPY_MODEL_fb4870326d1545dcaab261730965bc99"
            ]
          }
        },
        "6cad7320919248cab53cba7a3eb47225": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "18287140c5324a7e85b01460e5e889f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_11cd33748ed9445c921d533c922a2128",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 496313727,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 496313727,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4ae80b3c967142b1b773b080ed310264"
          }
        },
        "fb4870326d1545dcaab261730965bc99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_31bf737378c04156a4ced60a3b781572",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 496M/496M [00:07&lt;00:00, 62.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8344b11a1c42484883bb312ebfa677c6"
          }
        },
        "11cd33748ed9445c921d533c922a2128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4ae80b3c967142b1b773b080ed310264": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "31bf737378c04156a4ced60a3b781572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8344b11a1c42484883bb312ebfa677c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amine-OMRI/tweet-sentiment-extraction-kaggle-compete-1st-place-detailed-solution/blob/main/first_level_model_roberta_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOeAFVsEAUTq"
      },
      "source": [
        "La solution a été implémentée en utilisant une version antérieure des bibliothèques \n",
        "\n",
        "1.   tokenizers==0.7.0\n",
        "2.   transformers==2.9.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mk0dtujy2tO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91641d8b-38a9-42a2-ac40-5fb256387450"
      },
      "source": [
        "!pip install tokenizers==0.7.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 9.4MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnBIJXJjdySa"
      },
      "source": [
        "!pip install transformers==2.9.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhTiLFp1dzyt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a361850c-0d28-4758-89f8-6371287de6dc"
      },
      "source": [
        "!pip install torchcontrib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchcontrib\n",
            "  Downloading https://files.pythonhosted.org/packages/72/36/45d475035ab35353911e72a03c1c1210eba63b71e5a6917a9e78a046aa10/torchcontrib-0.0.2.tar.gz\n",
            "Building wheels for collected packages: torchcontrib\n",
            "  Building wheel for torchcontrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchcontrib: filename=torchcontrib-0.0.2-cp36-none-any.whl size=7531 sha256=b3b8c0c03c872b46b148aa2d40deaa5f449e0aa54839f3c66578743e2a9e6e8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/06/7b/a5f5920bbf4f12a2c927e438fac17d4cd9560f8336b00e9a99\n",
            "Successfully built torchcontrib\n",
            "Installing collected packages: torchcontrib\n",
            "Successfully installed torchcontrib-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7Cxkuo8DM_r"
      },
      "source": [
        "[torchcontrib](https://github.com/pytorch/contrib) librairie contient des implémentations d'idées issues de récents articles sur l'apprentissage machine, ici elle a été utilisée pour réaliser #Stochastic Weight Averaging (SWA) Que je vous présenteai ultérieurement dans le Notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVg2eB1FOZqE",
        "outputId": "be741b19-d382-4daf-d502-f7b8057baf6b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmUw-vAr6c_6"
      },
      "source": [
        "#Pour Commencer\n",
        "**Ce notebook représente l'un des modèles de premier niveau réalisé par [Heartkilla](https://www.kaggle.com/aruchomu) : Artsem Zhyvalkouski le code source est sous `src/1st_level/roberta_base/` sur le [repo git](https://github.com/heartkilla/kaggle_tweet/tree/master/src/1st_level/roberta_base) où ils ont mis le code de leur solution**\n",
        "\n",
        "---\n",
        "Pour faire tourner la solution de la compétition, j'ai chargé quelques fichiers sur mon Gdrive qui sont : \n",
        "\n",
        "\n",
        "*   Les fichiers de données :</br>\n",
        "    TRAINING_FILE = /content/drive/MyDrive/very_final/data/**train_folds.csv**</br>\n",
        "    TEST_FILE = /content/drive/MyDrive/very_final/data/**test.csv**</br>\n",
        " \n",
        "\n",
        "*   Les fichiers de configuration du tokeniser :</br>\n",
        "\n",
        "    /content/drive/MyDrive/very_final/roberta_tokenizer/**vocab.json**</br>\n",
        "    /content/drive/MyDrive/very_final/roberta_tokenizer/**merges.txt**</br>\n",
        "\n",
        "J'ai également **importer** les fichiers de **code** directement sur **Colab** en tant que fichiers d'entrée afin de pouvoir les importer et les utiliser, en même temps que j'ai copié leur code afin de pouvoir le commenter et remplir l'objectif de travail demandé.\n",
        "* `config.py`\n",
        "* `dataset.py`\n",
        "* `engine.py`\n",
        "* `evaluate.py`\n",
        "* `infer.py`\n",
        "* `models.py`\n",
        "* `train.py`\n",
        "* `utils.py`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jro8AaTYD78D"
      },
      "source": [
        "#config.py\n",
        "\n",
        "---\n",
        "\n",
        "Le fichier **config.py** (configuration) contient les paramètres et les réglages initiaux des modules et des méthodes de la solution. L'importation du fichier config.py permet d'utiliser les variables et les fonctions du fichier config.py dans les autres fichiers de la solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljAHrlvLDKkS"
      },
      "source": [
        "import tokenizers\n",
        "\n",
        "# Paths\n",
        "\n",
        "TOKENIZER_PATH = '/content/drive/MyDrive/very_final/roberta_tokenizer'\n",
        "TRAINING_FILE = '/content/drive/MyDrive/very_final/data/train_folds.csv'\n",
        "TEST_FILE = '/content/drive/MyDrive/very_final/data/test.csv'\n",
        "SUB_FILE = '/content/drive/MyDrive/very_final/data/sample_submission.csv'\n",
        "MODEL_SAVE_PATH = '/content/drive/MyDrive/very_final/roberta_base/model_save'\n",
        "TRAINED_MODEL_PATH = '/content/drive/MyDrive/very_final/roberta_base/model_save'\n",
        "\n",
        "# Model config\n",
        "##    Le tout premier modèle qui a été utilisé dans les modèles de 1er niveau\n",
        "##    est roberta-base pour le modèle d'AQ par deepset.ai (Squad pretrained \n",
        "##    weights)) pré-entrainé sur la base SQuAD 2.0\n",
        "MODEL_CONFIG = 'deepset/roberta-base-squad2'\n",
        "#---------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Model params\n",
        "# Global Seed to initialize the pseudo-random number generator\n",
        "# Pour assurer d'avoir les memes resultats d'une lancée a une autre\n",
        "SEED = 25\n",
        "# Nombre des folds pour l'entrainement par fold\n",
        "N_FOLDS = 5\n",
        "# Nombre d'EPOCHS de lentrainment des modéles \n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 4e-5\n",
        "PATIENCE = None\n",
        "EARLY_STOPPING_DELTA = None\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = 32\n",
        "MAX_LEN = 96  # actually = 86\n",
        "\n",
        "## Expliqué ci-dessous\n",
        "TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab_file=f'{TOKENIZER_PATH}/vocab.json',\n",
        "    merges_file=f'{TOKENIZER_PATH}/merges.txt',\n",
        "    lowercase=True,\n",
        "    add_prefix_space=True)\n",
        "# 768 est la dimension des Embeddings \n",
        "HIDDEN_SIZE = 768\n",
        "N_LAST_HIDDEN = 12\n",
        "HIGH_DROPOUT = 0.5\n",
        "SOFT_ALPHA = 0.6\n",
        "WARMUP_RATIO = 0.25\n",
        "WEIGHT_DECAY = 0.001\n",
        "#Stochastic Weight Averaging (SWA) :\n",
        "# les paramétres de l'optimiser \n",
        "USE_SWA = False\n",
        "SWA_RATIO = 0.9\n",
        "SWA_FREQ = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R42twUwALMX7"
      },
      "source": [
        "Source : **huggingface tokenizers**\n",
        "Fournit une implémentation des tokenizers les plus utilisés aujourd'hui, en mettant l'accent sur les performances et la polyvalence.\n",
        "Utilisé pour :\n",
        "* Entraîner de nouveaux vocabulaires et tokeniser à l'aide de 4 tokenizers pre-made (Bert WordPiece et les 3 versions les plus courantes de BPE).\n",
        "* Extrêmement rapide (tant pour l'entraînement que pour la tokenisation), grâce à l'implémentation de Rust. Il le faut moins de 20 secondes pour convertir un Go de texte en tokens sur le processeur d'un serveur.\n",
        "* Facile à utiliser, mais aussi extrêmement polyvalent.\n",
        "* Conçu pour la recherche et la production.\n",
        "* La normalisation s'accompagne d'un suivi des alignements. Il est toujours possible d'obtenir la partie de la phrase originale qui correspond à un jeton donné.\n",
        "* Effectue tout le prétraitement : Tronquer, Pad, ajouter les tokens spéciaux dont votre modèle a besoin.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lulkLxMkWKEh"
      },
      "source": [
        "\n",
        "Le tokenizer de RoBERTa est basé sur le tokenizer GPT-2, les fichiers vocab/merges sont constitués lors de l'entrainement du BBPE [(Byte-level Byte-Pair-Encoding)](https://arxiv.org/pdf/1909.03341.pdf) et utilisés pour encoder les sentences, le tokenizer tokenize d'abord en se basant sur le fichier **merges.txt**.</br>\n",
        "\n",
        "Voici un exemple : sentence = \"What's up with the tokenizer?\"</br>\n",
        "\n",
        "```\n",
        "['What', \"'s\", 'Ġup', 'Ġwith', 'Ġthe', 'Ġtoken', 'izer', '?']</br>\n",
        "\n",
        "```\n",
        "le caractère ```Ġ``` signifie un espace</br>\n",
        "\n",
        "\n",
        "Et ensuite, selon les valeurs dans le fichier **vocab.json**, ces tokens sont alors remplacés par leurs indices :</br>\n",
        "```\n",
        "[   'What',     \"'s\",    'Ġup',  'Ġwith',   'Ġthe', 'Ġtoken',   'izer',      '?']\n",
        "---- becomes ----\n",
        "[     2061,      338,      510,      351,      262,    11241,     7509,       30]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtM5UFGcEAxb"
      },
      "source": [
        "#dataset.py\n",
        "\n",
        "---\n",
        "le fichier dataset.py pour assurer que toutes les données des tweets soient stockées au même endroit et soient utilisées pour charger les données avec le **dataloader** par la suite.\n",
        "\n",
        "* Il contient également une implémentation d'un  Map-style datasets (the **TweetDataset** class)  qui implémente les méthodes **__getitem__()** et **__len__()** et qui représente toutes les propriétés : les ids des tweets, les offsets, orig_start/orig_end, start_labels/end_labels, mask, token_type_ids, ...\n",
        "\n",
        "* Egalement une implémentation de la méthode **process_data** qui traitera toutes ces propriétés et les extraira à partir des données d'entrée (tweets, selected_text, sentiment) en utilisant le tokeniser crée dans le fichier config.py.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocl0o4iYUoaa"
      },
      "source": [
        "Voyons voir a quoi ressemblent les données d'entrainement.</br>\n",
        "##Déboguage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "AAihgxQaUp0e",
        "outputId": "1730321e-10ec-4dc1-f9f6-9fde3322a8b3"
      },
      "source": [
        "import config\n",
        "import pandas as pd\n",
        "df_train = pd.read_csv(config.TRAINING_FILE)\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>kfold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>d0c214ad3a</td>\n",
              "      <td>good mornig to everone... it`s a great morning...</td>\n",
              "      <td>good mornig to everone... it`s a great morning...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7d093817af</td>\n",
              "      <td>LOL. You know me. I aim to please.</td>\n",
              "      <td>I aim to please.</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21eacf7e58</td>\n",
              "      <td>Was at Ruby Skye last night as well! Superb s...</td>\n",
              "      <td>Superb set by Steve.</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>d0f94d66ab</td>\n",
              "      <td>does not like ups much today...</td>\n",
              "      <td>does not like</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a025e21634</td>\n",
              "      <td>Nothing like In `n` Out and a LOST marathon af...</td>\n",
              "      <td>long day of work.</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID  ... kfold\n",
              "0  d0c214ad3a  ...     0\n",
              "1  7d093817af  ...     0\n",
              "2  21eacf7e58  ...     0\n",
              "3  d0f94d66ab  ...     0\n",
              "4  a025e21634  ...     0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvUeziUUXrR6"
      },
      "source": [
        "* **===>  `selected_text` est la valeur à prédire**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3zseDwveZ9Z",
        "outputId": "fc6583f9-79d4-47cb-dcf9-2d18d730d57c"
      },
      "source": [
        "df_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27480, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aArwkeaidCEs",
        "outputId": "753bb562-89ca-499b-f48b-0e1eed0985c0"
      },
      "source": [
        "## déboguer le code et travailler avec le troisième tweet\n",
        "tweet = df_train.text.values[3]\n",
        "selected_text = df_train.selected_text[3]\n",
        "tweet = ' ' + ' '.join(str(tweet).split())\n",
        "selected_text = ' ' + ' '.join(str(selected_text).split())\n",
        "tweet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' does not like ups much today...'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lurbGoDRlWTg",
        "outputId": "78414d10-742f-4be9-c0eb-99b887924dfb"
      },
      "source": [
        "selected_text "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' does not like'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8v8WVUmMUDXW",
        "outputId": "1d3dd874-a829-499e-826d-388eb0ae3e74"
      },
      "source": [
        "selected_text[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'d'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhAPoBb1eJ4_",
        "outputId": "f88236c1-061c-41d2-a17f-536741b6b77a"
      },
      "source": [
        "## Ne comptez pas le premier espace provenant du \" ' '.join(), \n",
        "## donc nous faisons -1 pour obtenir la vraie longueur\n",
        "len_sel_text = len(selected_text) - 1\n",
        "## récpèrerl'idice de début et de fin de selected_text\n",
        "idx_0 = None\n",
        "idx_1 = None\n",
        "## i : indice du caractére, e : c'est le caractére SI 'e' est le caractére \n",
        "## d'indice 1 (just apré lespace qu'on a rajouté)\n",
        "for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
        "  if ' ' + tweet[ind:ind + len_sel_text] == selected_text:\n",
        "    idx_0 = ind\n",
        "    idx_1 = ind + len_sel_text - 1\n",
        "    break\n",
        "print('l\\'indexe de début:',idx_0,'l\\'indexe de fin:', idx_1, 'du text séléctionné qui représente le target')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "l'indexe de début: 1 l'indexe de fin: 13 du text séléctionné qui représente le target\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo9w6FBTzzGV"
      },
      "source": [
        "* Assigner `1` à chaque caractère du tweet s'il fait partie du selected_text sinon `0`, comme dans l'exemple ci-dessus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8TAUopaxQwT",
        "outputId": "f42a04f0-ffb4-4b24-93ed-00a73fbe202d"
      },
      "source": [
        "## Attribuez 1 pour chaque caractère dans sel_text et des zéros aux autres caractères\n",
        "char_targets = [0] * len(tweet)\n",
        "if idx_0 is not None and idx_1 is not None:\n",
        "  for ct in range(idx_0, idx_1 + 1):\n",
        "    char_targets[ct] = 1\n",
        "char_targets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HSAaSoETzWlq",
        "outputId": "ebd72489-f65f-4093-8fdf-0ec31ea1af15"
      },
      "source": [
        "tweet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' does not like ups much today...'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKrcw3qNzELF",
        "outputId": "a9663b15-e4db-4ff2-a2cb-34131a4e9742"
      },
      "source": [
        "## Utiliser le tokenizer créé sur le fichier config.py pour tokeniser le tweet\n",
        "tokenized_tweet = TOKENIZER.encode(tweet)\n",
        "input_ids_original = tokenized_tweet.ids\n",
        "input_ids_original\n",
        "## Selon les valeurs dans le fichier vocab.json, ces tokens sont alors remplacés par leurs indices :"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[473, 45, 101, 12744, 203, 452, 734]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKtJb3Hk0t4F"
      },
      "source": [
        "* Une **nouvelle méthode** pour les tokenizers : **`tokenize_with_offsets`**. En plus de renvoyer les tokens, elle renvoie les intervalles dans le texte original auxquels les tokens correspondent, cette méthode nous permet de récupérer le token ou le mot qui correspond a un **id** donné."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIrwQTaSztnP",
        "outputId": "fc5425b2-7bdf-4834-bfab-0fa46be539a2"
      },
      "source": [
        "tweet_offsets = tokenized_tweet.offsets\n",
        "tweet_offsets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 5), (5, 9), (9, 14), (14, 18), (18, 23), (23, 29), (29, 32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7nI3hgd0PAB"
      },
      "source": [
        "* Récupérer le texte (les tokens) original en se basant sur les offsets (leur intervalle) : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ar3yFd101EBi",
        "outputId": "5d41d9a9-6f67-4274-c44c-520c6a62d163"
      },
      "source": [
        "tweet[tweet_offsets[0][0]:tweet_offsets[0][1]] + tweet[tweet_offsets[1][0]:tweet_offsets[1][1]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' does not'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjfwO6vQ45Pj"
      },
      "source": [
        "* Ce code récupère les **target_ids** qui sont les id des tokens cibles qui représentent le texte sélectionné (chaque caractère est représenté (codé) par le chiffre `\"1\"`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUOFWQ1k1GWR",
        "outputId": "5c5225ed-5c77-4fb7-87ce-6de69be4d59d"
      },
      "source": [
        "target_ids = []\n",
        "for i, (offset_0, offset_1) in enumerate(tweet_offsets):\n",
        "  print(i,'offset (',offset_0,',', offset_1,')')\n",
        "  print(char_targets[offset_0:offset_1])\n",
        "  print(sum(char_targets[offset_0:offset_1]))\n",
        "  if sum(char_targets[offset_0:offset_1]) > 0:\n",
        "    target_ids.append(i)\n",
        "print(\"----------------------------------------\")\n",
        "print('target_ids : ',target_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 offset ( 0 , 5 )\n",
            "[0, 1, 1, 1, 1]\n",
            "4\n",
            "1 offset ( 5 , 9 )\n",
            "[1, 1, 1, 1]\n",
            "4\n",
            "2 offset ( 9 , 14 )\n",
            "[1, 1, 1, 1, 1]\n",
            "5\n",
            "3 offset ( 14 , 18 )\n",
            "[0, 0, 0, 0]\n",
            "0\n",
            "4 offset ( 18 , 23 )\n",
            "[0, 0, 0, 0, 0]\n",
            "0\n",
            "5 offset ( 23 , 29 )\n",
            "[0, 0, 0, 0, 0, 0]\n",
            "0\n",
            "6 offset ( 29 , 32 )\n",
            "[0, 0, 0]\n",
            "0\n",
            "----------------------------------------\n",
            "target_ids :  [0, 1, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmpceIfOAIwz"
      },
      "source": [
        "* **Soft Jaccard labels :**</br>\n",
        "Custom loss `Jaccard-based Soft Labels`: Étant donné que la Cross Entropy n'optimise pas directement l'indice de Jaccard, **Heartkilla** a essayé différentes fonctions de Loss pour pénaliser davantage les prédictions lointaines que les prédictions proches, il a donc trouvé une Loss personnalisée en calculant l'indice de Jaccard au niveau du token. Il a ensuite utilisé ces nouveaux labels cibles et a optimisé la divergence.</br> \n",
        "Alpha c'est un paramètre permettant d'équilibrer l'étiquetage habituel basé sur la Cross Entropy et l'indice de Jaccard </br>\n",
        "<img src = 'https://camo.githubusercontent.com/3925753ce615ec71960dad457401aedefc7b611a2b11a3cb86eb060772dce880/68747470733a2f2f7777772e676f6f676c65617069732e636f6d2f646f776e6c6f61642f73746f726167652f76312f622f6b6167676c652d757365722d636f6e74656e742f6f2f696e626f7825324632303030353435253246393334316265646532383236336263663065396262323539616337393033333825324653637265656e25323053686f74253230323032302d30352d3330253230617425323031372e33312e32322e706e673f67656e65726174696f6e3d3135393234303530323835353638343226616c743d6d65646961'></br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5aOqwjyBi6m"
      },
      "source": [
        "## La mesure d'évaluation qui a été mentionnée sur le présent de la competition \n",
        "def jaccard_array(a, b):\n",
        "    \"\"\"Calculates Jaccard on arrays.\"\"\"\n",
        "    a = set(a)\n",
        "    b = set(b)\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCHfYQVO51D_"
      },
      "source": [
        "import numpy as np\n",
        "targets_start = target_ids[0]\n",
        "targets_end = target_ids[-1]\n",
        "n = len(input_ids_original)\n",
        "## id des tokens dans le tweet\n",
        "sentence = np.arange(n)\n",
        "## id  des tokens qui forment le label (le selected_text)\n",
        "answer = sentence[targets_start:targets_end + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR-6ZPNr6FmR",
        "outputId": "8b115a18-fc34-40b7-86d4-d2f58d423895"
      },
      "source": [
        "sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ecJSNNGiewI"
      },
      "source": [
        "* Le tableaux retourné a ce niveau représente les indices du target (text séléctionné) dans la sentence orginal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBehZdAO6I1s",
        "outputId": "996ef573-850b-4627-8d59-764093084b34"
      },
      "source": [
        "answer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDUUnvWKjKQb"
      },
      "source": [
        "* C'est bien la target selected_text: '` does not like` '"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alRiyywnGFXw",
        "outputId": "6124bedb-7309-48e1-ae85-7e92bdb9330f"
      },
      "source": [
        "sentence[targets_start:targets_end + 1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QKyPhqaB-sl",
        "outputId": "7885a8f4-43c8-4722-fc4b-360fc35563e1"
      },
      "source": [
        "for i in range(targets_end + 1):\n",
        "  ## calculate the jaccard indexe\n",
        "  ## answer = array([0, 1, 2]) qui est les indice du atrget selected_text: 'does not like'\n",
        "  jac = jaccard_array(answer, sentence[i:targets_end + 1])\n",
        "  print(\"a=\"+str(answer),\"b=\"+str(sentence[i:targets_end + 1]),\"==> jaccard indexe\",jac)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a=[0 1 2] b=[0 1 2] ==> jaccard indexe 1.0\n",
            "a=[0 1 2] b=[1 2] ==> jaccard indexe 0.6666666666666666\n",
            "a=[0 1 2] b=[2] ==> jaccard indexe 0.3333333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijAn528-kDk6"
      },
      "source": [
        "* jaccard indexe du token (0,2) =1.0 puisque le texte correspondant à l'index de **(0)** jusqu'à la target_end **(2)** dans la phrase originale (`' does not like ups much today...'`) est exactement le taget qui est `'does not like'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtoxHWZvBmY9",
        "outputId": "4c456098-050a-46bd-ed2b-2891b3cf533e"
      },
      "source": [
        "start_labels = np.zeros(n)\n",
        "print(str(start_labels))\n",
        "print(\"-------------------------------------------------------------\")\n",
        "for i in range(targets_end + 1):\n",
        "    ## calculate the jaccard indexe \n",
        "    jac = jaccard_array(answer, sentence[i:targets_end + 1])\n",
        "    print('jaccard indexe du tokens dans lintervalle ['+ str(i) +':'+ str(targets_end)+'] = '+ str(jac))\n",
        "    ## le carré ici est utile pour s'assurer que nous n'obtenons pas de valeurs négatives\n",
        "    start_labels[i] = jac + jac ** 2\n",
        "print(\"-------------------------------------------------------------\")\n",
        "print(str(start_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0.]\n",
            "-------------------------------------------------------------\n",
            "jaccard indexe du tokens dans lintervalle [0:2] = 1.0\n",
            "jaccard indexe du tokens dans lintervalle [1:2] = 0.6666666666666666\n",
            "jaccard indexe du tokens dans lintervalle [2:2] = 0.3333333333333333\n",
            "-------------------------------------------------------------\n",
            "[2.         1.11111111 0.44444444 0.         0.         0.\n",
            " 0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMbCpBG5bTeX"
      },
      "source": [
        "* À ce niveau, nous appliquons simplement la formule des **Jaccard-based Soft Labels** mentionnée ci-dessus pour créer nos labels.</br>\n",
        "* **=========>**   On peut remarquer que le score jaccard a été **pénalisé** pour les cas où les index ne correspondent pas au selected_text(notre cible)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKomTTSabjVx",
        "outputId": "c6bd7eb5-bfe9-4d20-9eac-a05986c7aeb2"
      },
      "source": [
        "## Alpha est un paramètre d'équilibre entre la CE (cross-enthropy) et le Jaccard-based labeling\n",
        "start_labels = (1 - config.SOFT_ALPHA) * start_labels / start_labels.sum()\n",
        "print(start_labels)\n",
        "start_labels[targets_start] += config.SOFT_ALPHA\n",
        "print(\"-------------------------------------------------------------\")\n",
        "print(start_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.33 0.05 0.02 0.   0.   0.   0.  ]\n",
            "-------------------------------------------------------------\n",
            "[0.93 0.05 0.02 0.   0.   0.   0.  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FCF7h7bhIAW"
      },
      "source": [
        "* **De la même manière, les labels de fin (`targets_end`) ont été codés**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_7PFUJSezg8",
        "outputId": "cd098baf-8d27-4e10-e00d-407f9ac9efa6"
      },
      "source": [
        "end_labels = np.zeros(n)\n",
        "print(str(end_labels))\n",
        "print(\"-------------------------------------------------------------\")\n",
        "for i in range(targets_start, n):\n",
        "  jac = jaccard_array(answer, sentence[targets_start:i + 1])\n",
        "  end_labels[i] = jac + jac ** 2\n",
        "  print('jaccard indexe du tokens dans lintervalle ['+ str(i) +':'+ str(targets_end)+'] = '+ str(jac))\n",
        "\n",
        "end_labels = (1 - config.SOFT_ALPHA) * end_labels / end_labels.sum()\n",
        "end_labels[targets_end] += config.SOFT_ALPHA\n",
        "print(\"-------------------------------------------------------------\")\n",
        "print(end_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0.]\n",
            "-------------------------------------------------------------\n",
            "jaccard indexe du tokens dans lintervalle [0:2] = 0.3333333333333333\n",
            "jaccard indexe du tokens dans lintervalle [1:2] = 0.6666666666666666\n",
            "jaccard indexe du tokens dans lintervalle [2:2] = 1.0\n",
            "jaccard indexe du tokens dans lintervalle [3:2] = 0.75\n",
            "jaccard indexe du tokens dans lintervalle [4:2] = 0.6\n",
            "jaccard indexe du tokens dans lintervalle [5:2] = 0.5\n",
            "jaccard indexe du tokens dans lintervalle [6:2] = 0.42857142857142855\n",
            "-------------------------------------------------------------\n",
            "[0.02472467 0.06181167 0.711261   0.07301503 0.05340528 0.04172287\n",
            " 0.03405949]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sk_Yem83fpJV",
        "outputId": "196bc1b5-20bb-4234-dc4c-eac4b9ea5018"
      },
      "source": [
        "## Les nouveaux labels qui seront utilisés pour améliorer et garantir que le modèle apprendra correctement\n",
        "start_labels = [0, 0, 0, 0] + list(start_labels) + [0]\n",
        "end_labels = [0, 0, 0, 0] + list(end_labels) + [0]\n",
        "print(\"Nouveaux start_labels: \"+str(start_labels)+'\\nNouveaux end_labels: '+\n",
        "      str(end_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nouveaux start_labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.9299999999999999, 0.05000000000000002, 0.020000000000000004, 0.0, 0.0, 0.0, 0.0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Nouveaux end_labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.024724666086919502, 0.06181166521729876, 0.7112609973911377, 0.07301502953793415, 0.05340527874774612, 0.04172287402167666, 0.03405948899728707, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBysJpqttFy6"
      },
      "source": [
        "* Pour résoudre la compétition, ils ont déjà utilisé le concept de **Question Answering `QA`** et le modèle `Roberta` a été utilisé pour cela et il a eu en entré la concaténation de listes d'identifiants de tweet et de sentiments **encodés**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n7MaNnZ9jN8L",
        "outputId": "4e57104e-4b92-4fc6-aa78-da9c49aebf8b"
      },
      "source": [
        "sentiment = df_train.sentiment[3]\n",
        "sentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCCFPOD4rgEG",
        "outputId": "5bd5aa61-90b3-47a3-d588-1beabbb61187"
      },
      "source": [
        "## Encoder le feature de sentiment\n",
        "sentiment_id = {'positive': 1313,\n",
        "                'negative': 2430,\n",
        "                'neutral': 7974}\n",
        "sentiment_id[sentiment]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2430"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuSj33-ortNX",
        "outputId": "d3ed67d8-6bc5-4d60-af3a-18f1dfc4ab4a"
      },
      "source": [
        "input_ids_original"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[473, 45, 101, 12744, 203, 452, 734]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj7MYLuhiIsP",
        "outputId": "769f6377-426e-4674-da13-e68acf8ebbb9"
      },
      "source": [
        "## l'input pour RoBERTa\n",
        "input_ids = [0] + [sentiment_id[sentiment]] + [2] + \\\n",
        "[2] + input_ids_original + [2]\n",
        "input_ids\n",
        "## Voila a quoi ressemble l'entré du modeèle :"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 2430, 2, 2, 473, 45, 101, 12744, 203, 452, 734, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9s9mVCIgVKN"
      },
      "source": [
        "##Le code initial du fichier **`dataset.py`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRAPl9oODntS"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import config\n",
        "\n",
        "## La mesure d'évaluation qui a été mentionnée sur le présent de la competition \n",
        "def jaccard_array(a, b):\n",
        "    \"\"\"Calculates Jaccard on arrays.\"\"\"\n",
        "    a = set(a)\n",
        "    b = set(b)\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "\n",
        "def process_data(tweet, selected_text, sentiment,\n",
        "                 tokenizer, max_len):\n",
        "    \"\"\"Preprocesses one data sample and returns a dict\n",
        "    with targets and other useful info.\n",
        "    \"\"\"\n",
        "    ## Pour un tweet donné:\n",
        "    ## récpèrer le text des tweet sous forme d'une str séparée par espace (commence par ' ' déja)\n",
        "    tweet = ' ' + ' '.join(str(tweet).split())\n",
        "    ## récpèrer le text des selected_text\n",
        "    selected_text = ' ' + ' '.join(str(selected_text).split())\n",
        "\n",
        "    ## récpèrer le len de selected_text (-1 puisque python commence à partir de 0)\n",
        "    len_sel_text = len(selected_text) - 1\n",
        "\n",
        "    ## récpèrer l'idice de début et de fin de selected_text\n",
        "    idx_0 = None\n",
        "    idx_1 = None\n",
        "    ## i : indice du caractére, e c'est le caractére SI e est le caractére \n",
        "    ## d'indice 1 (just apré lespace qu'ona rajouté au début de selected_text)\n",
        "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
        "        ## récupérer l'idx de début et fin du selected_text dans le text du tweet\n",
        "        if ' ' + tweet[ind:ind + len_sel_text] == selected_text:\n",
        "            idx_0 = ind\n",
        "            idx_1 = ind + len_sel_text - 1\n",
        "            break\n",
        "\n",
        "    ## Assignez 1 à chaque caractère du tweet s'il fait partie du selected_text\n",
        "    ## sinon 0, comme dans l'exemple ci-dessus.    \n",
        "    char_targets = [0] * len(tweet)\n",
        "    if idx_0 is not None and idx_1 is not None:\n",
        "        for ct in range(idx_0, idx_1 + 1):\n",
        "            char_targets[ct] = 1\n",
        "\n",
        "    ## tokeniser le texte du tweet en utilisant le tokeniser que nous avons \n",
        "    ## créé sur le fichier config.py\n",
        "    tokenized_tweet = tokenizer.encode(tweet)\n",
        "    ## récupérer les indices affectés par le tokeniser à chaque jeton\n",
        "    input_ids_original = tokenized_tweet.ids\n",
        "    ## Cette methode \".offsets\" permet de récupérer les intervalles dans le\n",
        "    ## texte original auxquels les tokens correspondent.\n",
        "    tweet_offsets = tokenized_tweet.offsets\n",
        "\n",
        "    ## Ce code récupère les target_ids qui sont les id des tokens cibles qui\n",
        "    ## représentent le texte sélectionné (chaque caractère est représenté par 1)\n",
        "    target_ids = []\n",
        "    for i, (offset_0, offset_1) in enumerate(tweet_offsets):\n",
        "        if sum(char_targets[offset_0:offset_1]) > 0:\n",
        "            target_ids.append(i)\n",
        "    ## idx début du text du target\n",
        "    targets_start = target_ids[0]\n",
        "    ## idx fin du text du target\n",
        "    targets_end = target_ids[-1]\n",
        "\n",
        "    # Sentimadd_prefix_spaceent 'word' id in vocab\n",
        "    ## Encoder le feature de sentiment\n",
        "    sentiment_id = {'positive': 1313,\n",
        "                    'negative': 2430,\n",
        "                    'neutral': 7974}\n",
        "\n",
        "    # Soft Jaccard labels\n",
        "    #C'est la méthode d'étiquetage personnalisée qui a été adoptée par les compétiteurs. \n",
        "    # ----------------------------------\n",
        "    n = len(input_ids_original)\n",
        "    sentence = np.arange(n)\n",
        "    answer = sentence[targets_start:targets_end + 1]\n",
        "    start_labels = np.zeros(n)\n",
        "    for i in range(targets_end + 1):\n",
        "        jac = jaccard_array(answer, sentence[i:targets_end + 1])\n",
        "        start_labels[i] = jac + jac ** 2\n",
        "        \n",
        "    ## Alpha est un paramètre d'équilibre entre l'étiquetage (labeling) Cross Enthropy \n",
        "    ## habituel et l'étiquetage basé sur la carte Jaccard (Jaccard-based labeling).\n",
        "    start_labels = (1 - config.SOFT_ALPHA) * start_labels / start_labels.sum()\n",
        "    start_labels[targets_start] += config.SOFT_ALPHA\n",
        "\n",
        "    end_labels = np.zeros(n)\n",
        "    for i in range(targets_start, n):\n",
        "        jac = jaccard_array(answer, sentence[targets_start:i + 1])\n",
        "        end_labels[i] = jac + jac ** 2\n",
        "    end_labels = (1 - config.SOFT_ALPHA) * end_labels / end_labels.sum()\n",
        "    end_labels[targets_end] += config.SOFT_ALPHA\n",
        "    ## Les nouveaux labels qui seront utilisés pour améliorer et garantir que le modèle apprendra correctement\n",
        "    start_labels = [0, 0, 0, 0] + list(start_labels) + [0]\n",
        "    end_labels = [0, 0, 0, 0] + list(end_labels) + [0]\n",
        "    # ----------------------------------\n",
        "\n",
        "    ## l'input pour RoBERTa\n",
        "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + \\\n",
        "                [2] + input_ids_original + [2]\n",
        "    ## Pas de types de token dans RoBERTa (tous a 0)\n",
        "    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_original) + 1)\n",
        "    ## Mask de l'input sans padding\n",
        "    mask = [1] * len(token_type_ids)\n",
        "    ## Identifiants des caractères de début et de fin pour chaque mot, y compris les nouveaux tokens\n",
        "    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n",
        "    ## Ids des mots dans le tweet qui ont un caractère cible, y compris les nouveaux tokens\n",
        "    targets_start += 4\n",
        "    targets_end += 4\n",
        "    orig_start = 4\n",
        "    orig_end = len(input_ids_original) + 3\n",
        "\n",
        "    ## Avant que RoBERTa puisse traiter ces données en entrée, nous devrons rendre \n",
        "    ## tous les vecteurs de même taille en ajoutant (padding ) des phrases plus courtes avec le token id 0. \n",
        "    ## Après le padding, nous avons une matrice / un tenseur qui est prêt à être passé à RoBERTa.\n",
        "    ## Input padding: new mask, token type ids, tweet offsets\n",
        "    ## s'il y'en à du padding \n",
        "    padding_len = max_len - len(input_ids)\n",
        "    if padding_len > 0:\n",
        "      ## on récupère les input_ids, mask, token_type_ids, tweet_offsets, end_offsets\n",
        "      input_ids = input_ids + ([1] * padding_len)\n",
        "      mask = mask + ([0] * padding_len)\n",
        "      token_type_ids = token_type_ids + ([0] * padding_len)\n",
        "      tweet_offsets = tweet_offsets + ([(0, 0)] * padding_len)\n",
        "      start_labels = start_labels + ([0] * padding_len)\n",
        "      end_labels = end_labels + ([0] * padding_len)\n",
        "    ## Compute le targets_select\n",
        "    targets_select = [0] * len(token_type_ids)\n",
        "    for i in range(len(targets_select)):\n",
        "        if i in target_ids:\n",
        "            targets_select[i + 4] = 1\n",
        "\n",
        "    ## la sortie pour un tweet donné\n",
        "    return {'ids': input_ids,\n",
        "            'mask': mask,\n",
        "            'token_type_ids': token_type_ids,\n",
        "            'start_labels': start_labels,\n",
        "            'end_labels': end_labels,\n",
        "            'orig_start': orig_start,\n",
        "            'orig_end': orig_end,\n",
        "            'orig_tweet': tweet,\n",
        "            'orig_selected': selected_text,\n",
        "            'sentiment': sentiment,\n",
        "            'offsets': tweet_offsets,\n",
        "            'targets_select': targets_select}\n",
        "\n",
        "\n",
        "## Une classe de pratique pour que toutes les données des tweets soient stockées \n",
        "## au même endroit et qui sera utilisé pour charger les donéées avec dataloader aprés\n",
        "\n",
        "## Map-style datasets\n",
        "## A map-style dataset is one that implements the __getitem__() and __len__() \n",
        "## protocols, and represents a map from (possibly non-integral) indices/keys to data samples.\n",
        "class TweetDataset:\n",
        "    '''\n",
        "    définir un objet(classe) qui contient toutes les données des tweets et implémenter \n",
        "    la méthode (pre-buil) _len_  qui retourne le nombre de tweets et la méthode \n",
        "    __getitem__ qui traite les données des tweets et calcule toutes les  tenseur \n",
        "    (torch.tensor) qui sera alimenté par le modèle : 'ids', 'mask', 'token_type_ids', \n",
        "    'start_labels', 'end_labels', 'orig_start', 'orig_end', 'orig_tweet', \n",
        "    'orig_selected', 'sentiment', 'offsets', 'targets_select'.\n",
        "    '''\n",
        "    def __init__(self, tweets, sentiments, selected_texts):\n",
        "        ## numpy.ndarray contenant les tweet\n",
        "        self.tweets = tweets\n",
        "        ## numpy.ndarray contenant les sentiments\n",
        "        self.sentiments = sentiments\n",
        "        ## numpy.ndarray contenant les targets (selected_text) qui reflètent les sentiments\n",
        "        self.selected_texts = selected_texts\n",
        "        ## max len de linput token au transformer model\n",
        "        self.max_len = config.MAX_LEN\n",
        "        self.tokenizer = config.TOKENIZER\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tweets)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        \"\"\"Returns preprocessed data sample as dict with\n",
        "        data converted to tensors.\n",
        "        \"\"\"\n",
        "        data = process_data(self.tweets[item],\n",
        "                            self.selected_texts[item],\n",
        "                            self.sentiments[item],\n",
        "                            self.tokenizer,\n",
        "                            self.max_len)\n",
        "        ## toutes les données ont été converties (casted) en torche.tensor pour\n",
        "        ## être traitées par le modèle et pour suivre (subir) l'opération pendant le \n",
        "        ## passage en avant et en arrière (forward / backward pass)\n",
        "\n",
        "        return {'ids': torch.tensor(data['ids'], dtype=torch.long),\n",
        "                'mask': torch.tensor(data['mask'], dtype=torch.long),\n",
        "                'token_type_ids': torch.tensor(data['token_type_ids'],\n",
        "                                               dtype=torch.long),\n",
        "                'start_labels': torch.tensor(data['start_labels'],\n",
        "                                             dtype=torch.float),\n",
        "                'end_labels': torch.tensor(data['end_labels'],\n",
        "                                           dtype=torch.float),\n",
        "                'orig_start': data['orig_start'],\n",
        "                'orig_end': data['orig_end'],\n",
        "                'orig_tweet': data['orig_tweet'],\n",
        "                'orig_selected': data['orig_selected'],\n",
        "                'sentiment': data['sentiment'],\n",
        "                'offsets': torch.tensor(data['offsets'], dtype=torch.long),\n",
        "                'targets_select': torch.tensor(data['targets_select'],\n",
        "                                               dtype=torch.float)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OkiOiOzHEql"
      },
      "source": [
        "deux techniques ont été utilisées afin d'assurer la même longueur des séquences d'entrée:\n",
        "* **Le masquage (Masking):** est un moyen d'indiquer aux couches de traitement des séquences que certains blocs sont manquants dans une entrée et qu'ils doivent donc être ignorés lors du traitement des données.\n",
        "\n",
        "* **Le padding** est une forme spéciale de masquage où les étapes masquées se trouvent au au début d'une séquence. Le remplissage (padding) vient de la nécessité de coder les données de la séquence en lots consécutifs (contiguous batches): afin que toutes les séquences d'un batch soient conformes à une longueur standard donnée, il est nécessaire de remplir ou de tronquer certaines séquences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_Q6Y5KPFUGh"
      },
      "source": [
        "# engine.py\n",
        "\n",
        "---\n",
        "Le fichier engine.py contient une definition de la fonction **`loss_fn`** qui va être utilisée pour calculer la valeur de la perte pendant l'entrainement et l'évaluation du modèle. ainsi que l'implementation des deux fonction:</br>\n",
        "* **`train_fn`** : qui permet de mettre le model en mode entrainment\n",
        "* **`eval_fn`**: qui permet de mettre le model en mode evaluation \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6q7M3XA5H7F"
      },
      "source": [
        "* **[KLDIVLOSS (The Kullback-Leibler divergence loss measure)](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html):** La divergence de Kullback-Leibler est une mesure de distance utile pour les distributions continues et est souvent utile pour effectuer une régression directe sur l'espace des distributions de sortie continues (échantillonnées discrètement).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2UMFZtzFUgB"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import tqdm\n",
        "\n",
        "import utils\n",
        "\n",
        "## Definition de la fonction loss\n",
        "def loss_fn(start_logits, end_logits,\n",
        "            start_positions, end_positions):\n",
        "    ## Appliquer la fonction \\log(\\text{Softmax}(x))log(Softmax(x)) à une entrée \n",
        "    ## n-dimensionnelle Tenseur ici dim = 1\n",
        "    ## Softmax : car elle déterminera les probabilités pour chaque ID de token d'être start_target et la même chose pour le end_target\n",
        "    m = torch.nn.LogSoftmax(dim=1)\n",
        "    ## La mesure de la divergence de Kullback-Leibler est une mesure de distance\n",
        "    ## utile pour les distributions continues  \n",
        "    loss_fct = torch.nn.KLDivLoss()\n",
        "    ## calculer la loss par apport a la prédiction du caractére de début du target(selected_text)\n",
        "    start_loss = loss_fct(m(start_logits), start_positions)\n",
        "    ## calculer la loss par apport a la prédiction du caractére de fin du target\n",
        "    end_loss = loss_fct(m(end_logits), end_positions)\n",
        "    ## La valeur de loss totale et la somme des deux loss par apprt a la prédiction \n",
        "    ## caractére de debut et fin du target\n",
        "    total_loss = (start_loss + end_loss)\n",
        "    return total_loss\n",
        "\n",
        "## définition de la fonction train\n",
        "def train_fn(data_loader, model, optimizer, device, scheduler=None):\n",
        "    ## model.train() permet de mettre le modèle en mode train (il calcule les gradients)\n",
        "    model.train()\n",
        "    ## Permet de stocke la valeur moyenne actuelle et applique deux méthodes :\n",
        "    ## reset : qui remet toutes les valeurs à zéro \n",
        "    ## update : qui met à jour l'objet en y ajoutant de nouvelles valeurs, ici il s'agit de la valeur de la loss\n",
        "    losses = utils.AverageMeter()\n",
        "   \n",
        "    ## tqdm nous permettre de créer une progressbar en fonction de la longueur des données\n",
        "    tk0 = tqdm.tqdm(data_loader, total=len(data_loader))\n",
        "    \n",
        "    print(\"-------------------------------------------------------------------\")\n",
        "    print(tk0)\n",
        "    \n",
        "    for bi, d in enumerate(tk0):\n",
        "        ## recupérer l'id du tweet\n",
        "        ids = d['ids']\n",
        "        ## récupérer les ids des tokens\n",
        "        token_type_ids = d['token_type_ids']\n",
        "        ## récupérer le mask du tweet\n",
        "        mask = d['mask']\n",
        "        ## la valeur du start/end lable calculer en utilisant Jaccard-based labeling\n",
        "        start_labels = d['start_labels']\n",
        "        end_labels = d['end_labels']\n",
        "\n",
        "        ## nn.Module.to function permet de déplacer le modèle\\tensors vers le GPU\n",
        "        ids = ids.to(device, dtype=torch.long)\n",
        "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        mask = mask.to(device, dtype=torch.long)\n",
        "        start_labels = start_labels.to(device, dtype=torch.float)\n",
        "        end_labels = end_labels.to(device, dtype=torch.float)\n",
        "\n",
        "        ## mettre les gradients à zéro avant de commencer à faire de la backpropragation  \n",
        "        model.zero_grad()\n",
        "\n",
        "        ## applique un forward pass et récuperer l'output\n",
        "        outputs_start, outputs_end = \\\n",
        "            model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
        "        ## Calculer la valeur de la loss\n",
        "        loss = loss_fn(outputs_start, outputs_end,\n",
        "                       start_labels, end_labels)\n",
        "        ## Calculer les gradiants\n",
        "        loss.backward()\n",
        "        ## Ajuster les poids de notre modele\n",
        "        optimizer.step()\n",
        "        ## un programmateur de taux d'apprentissage basé sur le temps\n",
        "        ## - il est contrôlé par le paramètre de décroissance(decay) de l'optimiser\n",
        "        scheduler.step()\n",
        "        ## mettre a jour la valeur sauvegardé de la loss \n",
        "        losses.update(loss.item(), ids.size(0))\n",
        "        tk0.set_postfix(loss=losses.avg)\n",
        "\n",
        "## définition de la fonction de l'évaluation\n",
        "def eval_fn(data_loader, model, device):\n",
        "    ## model.eval() met le modèle en mode évaluation (il calcule pas les gradients)\n",
        "    model.eval()\n",
        "    ## récupérer la valeur de la loss\n",
        "    losses = utils.AverageMeter()\n",
        "    ## récupérer la valeur de la métric de jaccards\n",
        "    jaccards = utils.AverageMeter()\n",
        "    ## Le wrapper \"with torch.no_grad()\" met temporairement tous les tensors avec require_grad à false\n",
        "    with torch.no_grad():\n",
        "        ## passer les donnnée d'évaluation avec une progressbar\n",
        "        tk0 = tqdm.tqdm(data_loader, total=len(data_loader))\n",
        "        for bi, d in enumerate(tk0):\n",
        "            ids = d['ids']\n",
        "            token_type_ids = d['token_type_ids']\n",
        "            mask = d['mask']\n",
        "            start_labels = d['start_labels']\n",
        "            end_labels = d['end_labels']\n",
        "            orig_start = d['orig_start']\n",
        "            orig_end = d['orig_end']\n",
        "            orig_selected = d['orig_selected']\n",
        "            orig_tweet = d['orig_tweet']\n",
        "            offsets = d['offsets']\n",
        "            \n",
        "            ## nn.Module.to function permet de déplacer le modèle\\tensors vers le GPU\n",
        "            ids = ids.to(device, dtype=torch.long)\n",
        "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            start_labels = start_labels.to(device, dtype=torch.float)\n",
        "            end_labels = end_labels.to(device, dtype=torch.float)\n",
        "\n",
        "            outputs_start, outputs_end = \\\n",
        "                model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
        "            loss = loss_fn(outputs_start, outputs_end,\n",
        "                           start_labels, end_labels)\n",
        "            ## récupérer les outputs start/stop prédites\n",
        "            outputs_start = outputs_start.cpu().detach().numpy()\n",
        "            outputs_end = outputs_end.cpu().detach().numpy()\n",
        "            ## lancé le calcul de lindices de jaccard qui permet d'evaluer le modele\n",
        "            jaccard_scores = []\n",
        "            for px, tweet in enumerate(orig_tweet):\n",
        "                ## recupérer la valeur réelle du target\n",
        "                selected_tweet = orig_selected[px]\n",
        "                jaccard_score, _ = \\\n",
        "                    utils.calculate_jaccard(original_tweet=tweet,\n",
        "                                            target_string=selected_tweet,\n",
        "                                            start_logits=outputs_start[px, :],\n",
        "                                            end_logits=outputs_end[px, :],\n",
        "                                            orig_start=orig_start[px],\n",
        "                                            orig_end=orig_end[px],\n",
        "                                            offsets=offsets[px])\n",
        "                jaccard_scores.append(jaccard_score)\n",
        "            ## mettre a jour la valeur de jaccard\n",
        "            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
        "            ## pareil pour la valeur de la loss\n",
        "            losses.update(loss.item(), ids.size(0))\n",
        "            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n",
        "\n",
        "    print(f'Jaccard = {jaccards.avg}')\n",
        "\n",
        "    return jaccards.avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etmylaX2Fb-j"
      },
      "source": [
        "# evaluate.py\n",
        "\n",
        "---\n",
        "La même implémentation de la fonction d'évaluation qui a été implémentée dans le fichier **engin.py**, celle-ci peut être utilisée pour effectuer une évaluation directement en utilisant le fichier **evaluate.py**.</br>\n",
        "* L'**`évaluation`** et la classe **`Infer`** qui effectue un forward pass pour prédir les labels de début/fin du selected_text, tous travaillent par **`Folds`** et le nombre de plis (Folds) est initialisé dans la classe config.py, ils ont utilisé **`5`** plis pour faire l'évaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oReUS_pFcZN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490,
          "referenced_widgets": [
            "2a5a6a4853c647d5ab4e6f2e421c5b21",
            "ea33407f48d84529a3cedce8ff4eb717",
            "86e84d68c27d472b8a7a0d57a6e22c43",
            "a042d53d0d374fbba87c5126bc89edb1",
            "fb2ad4da78f8468f8b5e0bacdfdb6a44",
            "67b4d3d78dee4cc39c7679217f24bca9",
            "f7174c2b850b4684b80adf245f073187",
            "b40d221cc11e42abacb6d79d87b64e15",
            "da3165f8a7bb4f1ba6002a3dd1839b23",
            "ad49480862ce44e696e8ab8d60f4850c",
            "fd76536cf49f4f9e9dcca7b8c6c9c804",
            "97fb8388273247cea7095fce3ccd4789",
            "2fdf7552d81344c6b23bbd1e82f34b80",
            "6991e98fa56846f98fdf918568cafd89",
            "79c265a4eeda434db25543b711583219",
            "c5c53ebd65b84df79429c42c41a456f0",
            "311e02e4ab79455f96a6a69d882cf44c",
            "8fbea1556436405c82e84a37e15ee0c3",
            "be8768226ed74c338a38576cc1639b61",
            "f195b12fa5be48fca00f7c2056222609",
            "d40ab0eeaee6478aa0eec0bc7a5d883f",
            "2f1b9d41b21047ec94130b11e14337d6",
            "71e0c96ecb90475a9767aa517b1ff34d",
            "83643d9c675e43d0adf21bb7e7bb4bd2",
            "e67e719ed34a4b07a0591d376313ceeb",
            "a19299dbe1cb457fb61a2bd60a044123",
            "e2037ef67ba3407e9883a32f1a9ae9e9",
            "ef8f28882f6a4689ac35c8b7dc69dc8a",
            "80baee64c83f4c05b13026fdd6c3928f",
            "95fe9696a2b74d7ea8c85a71ed0ea401",
            "1dd0410fc9704a9caa064a47ee8936bc",
            "ba08cfa1331d47489b03c860ed273728",
            "c649b48206e8461ab986a31eff41663d",
            "de48ecab2e9a43a2984091f852f12b25",
            "d9ccaad734f8466f9c609ba4c19ae320",
            "79deeeeeaa964a06bc9f3ad5462ca312",
            "1790d4b810e6479fb0986a33c7d6d89d",
            "3e0abbd9b372444a9ecb25a616239688",
            "8388209c0eea421a864e786bf0e88db2",
            "6e28785f8a2c4d26b9dedb98f993c35c",
            "6ae593eea4ea4bf29ccb9207bf39a987",
            "0a8fc24b90b944d1ba355ecbb00bfa58",
            "dc414fdaee294c9db8c677b27a81707f",
            "b53ea0fc5b7e42f6961cc98b3b257128",
            "9f73581c6f304a2381a54c3aded4fb52",
            "ad0874d836e3493cb84f49eb88448e72",
            "86d93fca9e4a417187f9512e505ccf8c",
            "0c571a2eff364548b85ad2d1a1ccc4cd",
            "e34c097bd1a8400cab7a702b06a9143c",
            "1eb203e1db144517ae6bf803be68d76b",
            "c555a66b7f44451e82c325428f234bc4",
            "5ac0aaff737641eca49308824bb94603",
            "1ec1aa1825834a7c96fddcafac8ee72b",
            "ae9af73a41084fc2bbf736f4afd9f5dd",
            "24ea29ea46fe45f5876e16e9bbc5daca",
            "806bf2cfe63e4b909522ad2c6b6f6d8c"
          ]
        },
        "outputId": "98e79f68-c3d7-4978-c812-dcd75770c65d"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import tqdm.autonotebook as tqdm\n",
        "\n",
        "import utils\n",
        "import config\n",
        "import models\n",
        "import dataset\n",
        "\n",
        "## définition de la fonction de l'évaluation\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    ## récupérer la valeur de la métric de jaccards\n",
        "    jaccards = utils.AverageMeter()\n",
        "    ## comme j'avais commenter sur le code ci-dessous\n",
        "    ## Le wrapper \"with torch.no_grad()\" met temporairement tous les tensors avec require_grad à false\n",
        "    with torch.no_grad():\n",
        "        tk0 = tqdm.tqdm(data_loader, total=len(data_loader))\n",
        "        for bi, d in enumerate(tk0):\n",
        "            ids = d['ids']\n",
        "            token_type_ids = d['token_type_ids']\n",
        "            mask = d['mask']\n",
        "            start_labels = d['start_labels']\n",
        "            end_labels = d['end_labels']\n",
        "            orig_start = d['orig_start']\n",
        "            orig_end = d['orig_end']\n",
        "            orig_selected = d['orig_selected']\n",
        "            orig_tweet = d['orig_tweet']\n",
        "            offsets = d['offsets']\n",
        "            ## déplacer le modèle\\tensors vers le GPU/CPU\n",
        "            ids = ids.to(device, dtype=torch.long)\n",
        "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            start_labels = start_labels.to(device, dtype=torch.float)\n",
        "            end_labels = end_labels.to(device, dtype=torch.float)\n",
        "\n",
        "            outputs_start, outputs_end = \\\n",
        "                model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
        "\n",
        "            outputs_start = outputs_start.cpu().detach().numpy()\n",
        "            outputs_end = outputs_end.cpu().detach().numpy()\n",
        "\n",
        "            jaccard_scores = []\n",
        "            for px, tweet in enumerate(orig_tweet):\n",
        "                selected_tweet = orig_selected[px]\n",
        "                jaccard_score, _ = \\\n",
        "                    utils.calculate_jaccard(original_tweet=tweet,\n",
        "                                            target_string=selected_tweet,\n",
        "                                            start_logits=outputs_start[px, :],\n",
        "                                            end_logits=outputs_end[px, :],\n",
        "                                            orig_start=orig_start[px],\n",
        "                                            orig_end=orig_end[px],\n",
        "                                            offsets=offsets[px])\n",
        "                jaccard_scores.append(jaccard_score)\n",
        "\n",
        "            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
        "            tk0.set_postfix(jaccard=jaccards.avg)\n",
        "\n",
        "    return jaccards.avg\n",
        "\n",
        "\n",
        "def run(fold):\n",
        "    ## Lecture des données de l'entrainement\n",
        "    dfx = pd.read_csv(config.TRAINING_FILE)\n",
        "\n",
        "    ## Lecture des données de la validation\n",
        "    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n",
        "    ## Les types de tenseurs CUDA, qui implémentent la même fonction que les \n",
        "    ## tenseurs CPU, mais qui utilisent les GPU pour le calcul\n",
        "    device = torch.device('cuda')\n",
        "    ## le modèle hérite de la class PreTrainedModel\n",
        "    ## c'est un modéle pré-entrainé sur Squad2\n",
        "    ## le modéle précisé dans la class config c'est bien 'deepset/roberta-base-squad2' \n",
        "    model_config = transformers.RobertaConfig.from_pretrained(config.MODEL_CONFIG)\n",
        "    ##  Pour assurer que le modèle rendre tous les hidden_state (weights).\n",
        "    model_config.output_hidden_states = True\n",
        "    \n",
        "    ## Crée une instance de la classe TweetModel avec la config crée just avant\n",
        "    model = models.TweetModel(conf=model_config)\n",
        "    model.to(device)\n",
        "    ## La fonction load_state_dict() prend un objet du dictionnaire, tet elle\n",
        "    ## charge le state_dict sérialisé et sauvegardé du modèle\n",
        "    model.load_state_dict(torch.load(\n",
        "        f'{config.TRAINED_MODEL_PATH}/model_{fold}.bin'))\n",
        "    \n",
        "    ## model.eval() met le modèle en mode évaluation (il calcule pas les gradients)\n",
        "    model.eval()\n",
        "\n",
        "    ## Préparé les tweets de validation selon la methode dataset.TweetDataset() qui prépare toutes les données des tweets\n",
        "    valid_dataset = dataset.TweetDataset(\n",
        "        tweets=df_valid.text.values,\n",
        "        sentiments=df_valid.sentiment.values,\n",
        "        selected_texts=df_valid.selected_text.values)\n",
        "    ## chargement des données de validation en utilisant dataLoader de Pytorch \n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=config.VALID_BATCH_SIZE,\n",
        "        num_workers=4,\n",
        "        shuffle=False)\n",
        "    ## Le fait de définir l'argument num_workers comme un nombre entier positif\n",
        "    ## activera le chargement de données multiprocessus avec le nombre spécifié de processus de chargement des travailleurs\n",
        "    ## calculer l'indice de jaccard\n",
        "    jaccard = eval_fn(valid_data_loader, model, device)\n",
        "\n",
        "    return jaccard\n",
        "\n",
        "##  if __name__ == \"main\" ' bloc pour empêcher l'exécution de (certain) code lors\n",
        "##  de l'importation du module. En bref, __name__ est une variable définie pour \n",
        "##  chaque script qui définit si le script est exécuté en tant que module principal \n",
        "##  ou s'il est exécuté en tant que module importé.\n",
        "if __name__ == '__main__':\n",
        "    utils.seed_everything(config.SEED)\n",
        "    ## Lise qui va contenir le score de chaque folds\n",
        "    fold_scores = []\n",
        "    ## N_FOLDS est a 5\n",
        "    for i in range(config.N_FOLDS):\n",
        "        fold_score = run(i)\n",
        "        fold_scores.append(fold_score)\n",
        "    ## Afficher les score de chaque folds et le score moyen\n",
        "    for i in range(config.N_FOLDS):\n",
        "        print(f'Fold={i}, Jaccard = {fold_scores[i]}')\n",
        "    print(f'Mean = {np.mean(fold_scores)}')\n",
        "    print(f'Std = {np.std(fold_scores)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a5a6a4853c647d5ab4e6f2e421c5b21",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=571.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da3165f8a7bb4f1ba6002a3dd1839b23",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=496313727.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "311e02e4ab79455f96a6a69d882cf44c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=172.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e67e719ed34a4b07a0591d376313ceeb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=172.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c649b48206e8461ab986a31eff41663d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=172.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ae593eea4ea4bf29ccb9207bf39a987",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=172.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e34c097bd1a8400cab7a702b06a9143c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=172.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold=0, Jaccard = 0.7163026420895469\n",
            "Fold=1, Jaccard = 0.7108215431021135\n",
            "Fold=2, Jaccard = 0.7158955035127735\n",
            "Fold=3, Jaccard = 0.7166221978997807\n",
            "Fold=4, Jaccard = 0.7109265704332721\n",
            "Mean = 0.7141136914074974\n",
            "Std = 0.002655369837057321\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-Eedd3Jg3c6"
      },
      "source": [
        "**TORCH.UTILS.DATA**\n",
        "Au cœur de l'utilitaire de chargement de données PyTorch se trouve la classe ```torch.utils.data.DataLoader```. Elle représente un Python itérable sur un ensemble de données, avec le support de:</br>\n",
        "* map-style et iterable-style datasets,\n",
        "\n",
        "* la personnalisation de l'ordre de chargement des données,\n",
        "\n",
        "* le dosage automatique,\n",
        "\n",
        "* chargement de données à un ou plusieurs processus,\n",
        "\n",
        "* épinglage automatique de la mémoire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrP6RBNKFqd9"
      },
      "source": [
        "# infer.py\n",
        "\n",
        "---\n",
        "Ce code représente l'implémentation d'une Forward passe pour prédire le texte sélectionné (start/end_labels) des tweets donné, on peut dire que cela effectue la même tâche que la méthode .predict() en ML puisque ya pas la notion de l'apprentissage c'est just une propagation dans le modele et récupération de l'output.</br>\n",
        "* L'**`évaluation`** et la methode **`run()`** de la classe **`Infer`** qui effectue un forward pass pour prédir les labels de début/fin du selected_text, tous travaillent par **`Folds`** et le nombre de plis (Folds) est initialisé dans la classe config.py, ils ont utilisé **`5`**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlQWmDOcQHvc"
      },
      "source": [
        "**Débogage**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rma-6txbQF0P"
      },
      "source": [
        "import pickle\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "import tqdm\n",
        "\n",
        "import config\n",
        "import models\n",
        "import dataset\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHdlXPTWPc_4"
      },
      "source": [
        "## Charger les données de test\n",
        "df_test = pd.read_csv(config.TEST_FILE)\n",
        "df_test.loc[:, 'selected_text'] = df_test.text.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ2ekEZbQfrw"
      },
      "source": [
        "* **`TORCH.CUDA`**\n",
        "Ce paquet ajoute la prise en charge des types de tenseurs CUDA, qui implémentent la même fonction que les tenseurs CPU, mais qui utilisent les GPU pour le calcul.\n",
        "* En utilisant **`torch.device(\"cuda\")`**, elle permet de préciser que le dispositif est un GPU sans spécifier particulièrement le nom ou l'id du dispositif (0,1,2,3)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94B8gmIARzbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "161214af-7730-4ca1-d2d3-ce700f0a46e9"
      },
      "source": [
        "print(\"[INFO]: le nombre de GPU disponibles:\",torch.cuda.device_count(),\"et le nom de dispositif utilisé:\",torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]: le nombre de GPU disponibles: 1 et le nom de dispositif utilisé: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Reh_TGmqQ3ch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "120f15c1-40e7-48ad-9d8c-4836cef5d2e5"
      },
      "source": [
        "device = torch.device('cuda')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_N4QPpQQqQW"
      },
      "source": [
        "## Instancier le modèle de transformateur avec la configuration définie dans le fichier config.py\n",
        "model_config = transformers.RobertaConfig.from_pretrained(config.MODEL_CONFIG)\n",
        "## output_hidden_states (bool, optional, defaults to False) - Indique si le modèle\n",
        "## doit ou non retourner tous les états cachés.\n",
        "model_config.output_hidden_states = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4Os_p7qUTWd"
      },
      "source": [
        "fold_models = []\n",
        "## Comme l'entrainement et l'évaluation, Infer (forwad pass (equivalent de \n",
        "## la methode .predict()) a également été fait par des folds qui sont définis\n",
        "## dans le fichier de configuration à 5\n",
        "for i in range(config.N_FOLDS):\n",
        "  ## Instancier le modèle TweetModel avec la configuration créer ci-dessus (model_config)\n",
        "  model = models.TweetModel(conf=model_config)\n",
        "  ## Mettre le modèle sur le GPU device spécifié\n",
        "  model.to(device)\n",
        "  ## Charger le modèle sauvegardé après l'entraînement et l'évaluation.\n",
        "  model.load_state_dict(torch.load(f'{config.TRAINED_MODEL_PATH}/model_{i}.bin'),\n",
        "                        strict=False)\n",
        "  ## Mettre le modèle en mode d'évaluation afin que nous ignorions l'apprentissage \n",
        "  ## puisque nous prédisons sans avoir besoin de calculs, backprop, grads\n",
        "  model.eval()\n",
        "  ## Après l'entraînement par plis(folds=5), nous obtenons/sauvegardons un modèle pour chaque pli\n",
        "  fold_models.append(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtO5TLyqFp-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "459773e4-c234-4cfb-c1d4-9665f468a2f0"
      },
      "source": [
        "import pickle\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "import tqdm\n",
        "\n",
        "import config\n",
        "import models\n",
        "import dataset\n",
        "import utils\n",
        "\n",
        "\n",
        "def run():\n",
        "    df_test = pd.read_csv(config.TEST_FILE)\n",
        "    df_test.loc[:, 'selected_text'] = df_test.text.values\n",
        "    ## préciser que le dispositif est un GPU\n",
        "    device = torch.device('cuda')\n",
        "    model_config = transformers.RobertaConfig.from_pretrained(config.MODEL_CONFIG)\n",
        "    model_config.output_hidden_states = True\n",
        "    \n",
        "    fold_models = []\n",
        "    for i in range(config.N_FOLDS):\n",
        "        model = models.TweetModel(conf=model_config)\n",
        "        model.to(device)\n",
        "        model.load_state_dict(torch.load(\n",
        "            f'{config.TRAINED_MODEL_PATH}/model_{i}.bin'),\n",
        "            strict=False)\n",
        "        model.eval()\n",
        "        fold_models.append(model)\n",
        "    ## TweetDataset est un map-style et iterable-style datasets\n",
        "    test_dataset = dataset.TweetDataset(\n",
        "        tweets=df_test.text.values,\n",
        "        sentiments=df_test.sentiment.values,\n",
        "        selected_texts=df_test.selected_text.values)\n",
        "    ## data_loader permet d'automatiser le chargement des données\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        shuffle=False,\n",
        "        batch_size=config.VALID_BATCH_SIZE,\n",
        "        num_workers=4) ## shuffle=False puisuqe on est en mode evaluation donc pas besoin d'un chuffle\n",
        "\n",
        "    char_pred_test_start = []\n",
        "    char_pred_test_end = []\n",
        "    ## Pas de calcu des gradiants, c'est un forward pass de notre modèle\n",
        "    with torch.no_grad():\n",
        "        ## Faites instantanément apparaître un indicateur de progression intelligent\n",
        "        ## sur les boucles - il suffit d'envelopper n'importe quel itérable avec tqdm(iterable)\n",
        "        tk0 = tqdm.tqdm(data_loader, total=len(data_loader))\n",
        "        for bi, d in enumerate(tk0):\n",
        "            ids = d['ids']\n",
        "            token_type_ids = d['token_type_ids']\n",
        "            mask = d['mask']\n",
        "            orig_tweet = d['orig_tweet']\n",
        "            offsets = d['offsets']\n",
        "            \n",
        "            ids = ids.to(device, dtype=torch.long)\n",
        "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "\n",
        "            outputs_start_folds = []\n",
        "            outputs_end_folds = []\n",
        "\n",
        "            for i in range(config.N_FOLDS):\n",
        "                outputs_start, outputs_end = \\\n",
        "                    fold_models[i](ids=ids,\n",
        "                                   mask=mask,\n",
        "                                   token_type_ids=token_type_ids)\n",
        "                outputs_start_folds.append(outputs_start)\n",
        "                outputs_end_folds.append(outputs_end)\n",
        "\n",
        "            outputs_start = sum(outputs_start_folds) / config.N_FOLDS\n",
        "            outputs_end = sum(outputs_end_folds) / config.N_FOLDS\n",
        "            \n",
        "            outputs_start = torch.softmax(outputs_start, dim=-1).cpu().detach().numpy()\n",
        "            outputs_end = torch.softmax(outputs_end, dim=-1).cpu().detach().numpy()\n",
        "            ## Affecter les prababilitées de l'output (La liste des probabilités affectés aux tokens) outputs_start/outputs_end au char\n",
        "            ## pour passer au char level puisque le Transformers sont token level\n",
        "            ## chaque caractére prends la probavilitée affecté au token auquel il appartient\n",
        "            ## La probabilité corresponde a la possibilité que ce token et le debut ou la fin du selected_text (target)\n",
        "            for px, tweet in enumerate(orig_tweet):\n",
        "                char_pred_test_start.append(\n",
        "                    utils.token_level_to_char_level(tweet, offsets[px], outputs_start[px]))\n",
        "                char_pred_test_end.append(\n",
        "                    utils.token_level_to_char_level(tweet, offsets[px], outputs_end[px]))\n",
        "    ## Serialiser et sauvegarder les output de la prédiction\n",
        "    with open('/content/drive/MyDrive/very_final/pickles/roberta-char_pred_test_start.pkl', 'wb') as handle:\n",
        "        pickle.dump(char_pred_test_start, handle)\n",
        "    with open('/content/drive/MyDrive/very_final/pickles/roberta-char_pred_test_end.pkl', 'wb') as handle:\n",
        "        pickle.dump(char_pred_test_end, handle)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 111/111 [01:02<00:00,  1.78it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU3Wq7BkOCRg"
      },
      "source": [
        "* **`token_level_to_char_level`** cette méthode affectera les probabilités prédites de chaque token à chaque caractère lui appartenant et je l'ai détaillée et expliquée sur le notebook **`Character Level Model`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUTPKlGDGpj8"
      },
      "source": [
        "# models.py\n",
        "\n",
        "---\n",
        "ce fichier contient une implémentation de la classe de modèle **`TweetModel`** qui héritée des transformateurs **`BertPreTrainedModel`** et la méthode **`forward`** qui récupère la sortie logits juste avant la couche des embeddings et effectue un**` Max-pooling`** et un **`Average_pooling`**</br>\n",
        "<img src = 'https://github.com/Amine-OMRI/tweet-sentiment-extraction-kaggle-compete-1st-place-detailed-solution/blob/main/model_architecture.png?raw=true'></br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF7B-M5K210p"
      },
      "source": [
        "\n",
        "* Ici, ils veulent charger le modèle roberta à partir d'un ensemble donné de poids (weights) en faisant appel à from_pretrained sur la classe TweetModel ; c'est pourquoi cette classe hérite de transformers.BertPreTrainedModel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2Ps0RfL406X",
        "outputId": "ca00ec38-3c89-42e3-c511-e010fa91e07a"
      },
      "source": [
        "classifier = torch.nn.Linear(config.HIDDEN_SIZE * 2, 2)\n",
        "classifier"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=1536, out_features=2, bias=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0kVj0sFGo6k"
      },
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "import config\n",
        "\n",
        "## Ici, ils veulent charger le modèle roberta à partir d'un ensemble donné de poids\n",
        "## (weights) en faisant appel à from_pretrained sur la classe TweetModel ; c'est\n",
        "## pourquoi cette classe hérite de transformers.BertPreTrainedModel.\n",
        "class TweetModel(transformers.BertPreTrainedModel):\n",
        "    ## Instantaition du modele\n",
        "    def __init__(self, conf):\n",
        "        super(TweetModel, self).__init__(conf)\n",
        "        self.roberta = transformers.RobertaModel.from_pretrained(\n",
        "            config.MODEL_CONFIG,\n",
        "            config=conf)\n",
        "        ## Utiler la valeure spécifiée dans le fichier config.py pour un dropout de 0.5\n",
        "        self.high_dropout = torch.nn.Dropout(config.HIGH_DROPOUT)\n",
        "        ## 768 est la dimension des Embeddings \n",
        "        ## 2 par ce que on a un problème de classification soit le token c'est le text sélectionné (target) soit non\n",
        "        ## HIDDEN_SIZE 768 * 2 par ce que a la sortie de roberta on aura un tensor de 1536\n",
        "        ## Une couche (1536,2): Linear(in_features=1536, out_features=2, bias=True)\n",
        "        self.classifier = torch.nn.Linear(config.HIDDEN_SIZE * 2, 2)\n",
        "        torch.nn.init.normal_(self.classifier.weight, std=0.02)\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        # sequence_output of N_LAST_HIDDEN + Embedding states\n",
        "        # (N_LAST_HIDDEN + 1, batch_size, num_tokens, 768)\n",
        "        _, _, out = self.roberta(ids, attention_mask=mask,\n",
        "                                 token_type_ids=token_type_ids)\n",
        "        \n",
        "        ## Récupére les valeus de toutes les couches sans la couche des embeddings.\n",
        "        out = torch.stack(\n",
        "            tuple(out[-i - 1] for i in range(config.N_LAST_HIDDEN)), dim=0)\n",
        "        ## Avg pooling\n",
        "        out_mean = torch.mean(out, dim=0)\n",
        "        ## Max pooling\n",
        "        out_max, _ = torch.max(out, dim=0)\n",
        "        out = torch.cat((out_mean, out_max), dim=-1)\n",
        "\n",
        "\n",
        "        # Multisample Dropout: https://arxiv.org/abs/1905.09788 expliqué just en bas\n",
        "        ## logit céest la couche qui vient just avant la couche Dense\n",
        "        logits = torch.mean(torch.stack([\n",
        "            self.classifier(self.high_dropout(out))\n",
        "            for _ in range(5)\n",
        "        ], dim=0), dim=0)\n",
        "        ## puique on'a deux output dans la loits (start_lable / end_label)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "\n",
        "        # (batch_size, num_tokens)\n",
        "        ## .squeeze() pou applatire (flatteniser) les Nd tensors\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        return start_logits, end_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5zCx9T1lqBD"
      },
      "source": [
        "**Multi Sample Dropout (MSD):** C'est l'une des techniques qu'ils ont utilisées et que je trouve si intéressante. En fait, il applique un dropout plusieurs fois avec différents masques et ensuite il calcule la moyenne des résultats</br>\n",
        "  Le dropout initial crée un sous-ensemble choisi au hasard (appelé dropout sample) à partir des données d'entrée de chaque itération d'entrainement, tandis que le MSD crée plusieurs échantillon de dropout. La loss est calculée pour chaque échantillon, puis la moyenne des losses des échantillons est calculée pour obtenir la Loss finale [(plus de détails ici)](https://arxiv.org/pdf/1905.09788.pdf).</br>\n",
        "  ![alt text](https://github.com/Amine-OMRI/tweet-sentiment-extraction-kaggle-compete-1st-place-detailed-solution/blob/main/Multi-Sample-Dropout.png?raw=true)</br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPit1dlVHEIs"
      },
      "source": [
        "# utils.py\n",
        "\n",
        "---\n",
        "\n",
        "le fichier **utils.py**contient toutes les implémentations de toutes les fonctions qui seront utilisées dans de nombreux fichiers de code, telle que :\n",
        "* la fonction qui fixe le seed global **seed_everything** \n",
        "la fonction qui calcule les probabilités de niveau de caractères token_level_to_char_level\n",
        "* la fonction qui calcule la métrique de l'évaluation mentionnée dans les énoncées de la competition, qui est **jaccard**\n",
        "* la fonction qui calcule le score final du Jaccard en utilisant les prédictions **calculate_jaccard**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xc2lgxiHTsQ"
      },
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "\n",
        "\n",
        "def token_level_to_char_level(text, offsets, preds):\n",
        "    probas_char = np.zeros(len(text))\n",
        "    for i, offset in enumerate(offsets):\n",
        "        if offset[0] or offset[1]:\n",
        "            probas_char[offset[0]:offset[1]] = preds[i]\n",
        "\n",
        "    return probas_char\n",
        "\n",
        "\n",
        "def jaccard(str1, str2):\n",
        "    \"\"\"Original metric implementation.\"\"\"\n",
        "    a = set(str1.lower().split())\n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "\n",
        "def get_best_start_end_idx(start_logits, end_logits,\n",
        "                           orig_start, orig_end):\n",
        "    \"\"\"Return best start and end indices following BERT paper.\"\"\"\n",
        "    best_logit = -np.inf\n",
        "    best_idxs = None\n",
        "    start_logits = start_logits[orig_start:orig_end + 1]\n",
        "    end_logits = end_logits[orig_start:orig_end + 1]\n",
        "    for start_idx, start_logit in enumerate(start_logits):\n",
        "        for end_idx, end_logit in enumerate(end_logits[start_idx:]):\n",
        "            logit_sum = start_logit + end_logit\n",
        "            if logit_sum > best_logit:\n",
        "                best_logit = logit_sum\n",
        "                best_idxs = (orig_start + start_idx,\n",
        "                             orig_start + start_idx + end_idx)\n",
        "    return best_idxs\n",
        "\n",
        "\n",
        "def calculate_jaccard(original_tweet, target_string,\n",
        "                      start_logits, end_logits,\n",
        "                      orig_start, orig_end,\n",
        "                      offsets, \n",
        "                      verbose=False):\n",
        "    \"\"\"Calculates final Jaccard score using predictions.\"\"\"\n",
        "    start_idx, end_idx = get_best_start_end_idx(\n",
        "        start_logits, end_logits, orig_start, orig_end)\n",
        "\n",
        "    filtered_output = ''\n",
        "    for ix in range(start_idx, end_idx + 1):\n",
        "        filtered_output += original_tweet[offsets[ix][0]:offsets[ix][1]]\n",
        "        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n",
        "            filtered_output += ' '\n",
        "\n",
        "    # Return orig tweet if it has less then 2 words\n",
        "    if len(original_tweet.split()) < 2:\n",
        "        filtered_output = original_tweet\n",
        "\n",
        "    if len(filtered_output.split()) == 1:\n",
        "        filtered_output = filtered_output.replace('!!!!', '!')\n",
        "        filtered_output = filtered_output.replace('..', '.')\n",
        "        filtered_output = filtered_output.replace('...', '.')\n",
        "\n",
        "    filtered_output = filtered_output.replace('ïï', 'ï')\n",
        "    filtered_output = filtered_output.replace('¿¿', '¿')\n",
        "\n",
        "    jac = jaccard(target_string.strip(), filtered_output.strip())\n",
        "    return jac, filtered_output\n",
        "\n",
        "\n",
        "class AverageMeter:\n",
        "    \"\"\"Computes and stores the average and current value.\"\"\"\n",
        "    ## Permet de stocke la valeur moyenne actuelle et applique deux méthodes :\n",
        "    ## reset : qui remet toutes les valeurs à zéro \n",
        "    ## update : qui met à jour l'objet en y ajoutant de nouvelles valeurs, ici il s'agit de la valeur de la loss\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDzmFycLG6Rc"
      },
      "source": [
        "#train.py\n",
        "\n",
        "---\n",
        "Ce fichier de code exécute l'entrainement sur les données d'entranement et de la validation sur les données de validation et affiche les valeurs de jaccard et la loss\n",
        "\n",
        "1) Il a utilisé le GPU Colab Pro pour RoBERTa-large et il a fallu environ 6h pour l'entraîner avec 5 folds et 4 époques sans optimisation particulière. \n",
        "\n",
        "[Hikkiiii](https://www.kaggle.com/wochidadonggua) a également entraîner RoBERTa-large, 2V100, APEX(O1), il a fallu environ 220s par époque \n",
        "\n",
        "2) RoBERTa-base-squad2 est disponible pré-entrainé par [HuggingFace.](https://huggingface.co/deepset/roberta-base-squad2)\n",
        "\n",
        "\n",
        "\n",
        "* L'**`apprentissage`**, l'**`évaluation`** et la classe **`Infer`** qui effectue un forward pass pour prédir les labels de début/fin du selected_text, tous travaillent par **`Folds`** et le nombre de plis (Folds) est initialisé dans la classe config.py, ils ont utilisé **`5`** plis pour chaque processus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0lwwdVZgxBo"
      },
      "source": [
        "##Déboguage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkXID0zIxpE5"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import torch\n",
        "import torchcontrib\n",
        "\n",
        "import config\n",
        "import dataset\n",
        "import models\n",
        "import engine\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HT0b6QoG61j"
      },
      "source": [
        "Une autre technique qui a été utilisée comme **Optimiser** l'est :\n",
        "* **SWA :** la technique SWA [(Stochastic Weight Averaging)](https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/) récemment proposée, et sa nouvelle implémentation dans torchcontrib. La SWA est une procédure simple qui améliore la généralisation du deep learning sur la Descente de Gradient Stochastique (SGD) sans coût supplémentaire, et peut être utilisée en remplacement de tout autre **optimiseur dans PyTorch**. Le SWA a une large gamme d'applications et de fonctionnalités.</br>\n",
        "Il a été démontré que SWA améliore considérablement la généralisation des tâches de vision par ordinateur, y compris les VGG, les ResNets, les Wide ResNets et les DenseNets sur ImageNet et les CIFAR benchmarks.\n",
        "\n",
        "En bref, le SWA effectue une moyenne égale des poids traversés par le SGD avec un programme d'apprentissage modifié."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCEBaXLxemvi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7296cf24-a9a0-4ad3-91ef-ec82750a81df"
      },
      "source": [
        "for i in range(config.N_FOLDS):\n",
        "  print(\"fold %s\"%i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold 0\n",
            "fold 1\n",
            "fold 2\n",
            "fold 3\n",
            "fold 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhejMuHRezCN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "221d73b9-61ae-4990-e866-328ab058f1bf"
      },
      "source": [
        "print(config.TRAINING_FILE)\n",
        "dfx = pd.read_csv(config.TRAINING_FILE)\n",
        "dfx.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/very_final/data/train_folds.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>kfold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>d0c214ad3a</td>\n",
              "      <td>good mornig to everone... it`s a great morning...</td>\n",
              "      <td>good mornig to everone... it`s a great morning...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7d093817af</td>\n",
              "      <td>LOL. You know me. I aim to please.</td>\n",
              "      <td>I aim to please.</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21eacf7e58</td>\n",
              "      <td>Was at Ruby Skye last night as well! Superb s...</td>\n",
              "      <td>Superb set by Steve.</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>d0f94d66ab</td>\n",
              "      <td>does not like ups much today...</td>\n",
              "      <td>does not like</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a025e21634</td>\n",
              "      <td>Nothing like In `n` Out and a LOST marathon af...</td>\n",
              "      <td>long day of work.</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID  ... kfold\n",
              "0  d0c214ad3a  ...     0\n",
              "1  7d093817af  ...     0\n",
              "2  21eacf7e58  ...     0\n",
              "3  d0f94d66ab  ...     0\n",
              "4  a025e21634  ...     0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35AXIHbqhOLo"
      },
      "source": [
        "* L'entrainement et l'évaluation ont été faites par Folds (**5 folds**) Le jeu de données a également été étiqueté avec un numéro de référence a un fold entre 0 et 4 et tous les plis contiennent le même nombre de lignes (5496)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S85v7ZLfTDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2e9c588-fa3b-4324-e916-60cb7e21e829"
      },
      "source": [
        "dfx[['kfold']].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "kfold\n",
              "4        5496\n",
              "3        5496\n",
              "2        5496\n",
              "1        5496\n",
              "0        5496\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMn1oGkJhgrb"
      },
      "source": [
        "* Tout comme la validation croisée, ils ont 5 folds, ils en gardent un (fold) pour la validation et les 4 autres fold pour l'entrainement du modèle et chacun des 5 fold sera utilisé une fois comme un fold de validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBFc09taeVJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c06bef7-4789-411d-8e09-de705aea01cf"
      },
      "source": [
        "## Pour le fold 0, les autre folds (1,2,3,4) vont etre utilisés pour l'entrainement\n",
        "df_train = dfx[dfx.kfold != 0].reset_index(drop=True)\n",
        "## Le fold 0 sera pour la validation\n",
        "df_valid = dfx[dfx.kfold == 0].reset_index(drop=True)\n",
        "df_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21984, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IZPnqSGlQEn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3e3bdf4-32c1-432d-a537-7771543fb896"
      },
      "source": [
        "## les tweets\n",
        "type(df_train.text.values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCui4AOsjE3x"
      },
      "source": [
        "## Créer un Map-style datasets (class TweetDataset) sur les données d'entrainement\n",
        "train_dataset = dataset.TweetDataset(\n",
        "    tweets=df_train.text.values, ## Text du tweet (numpy.ndarray de string)\n",
        "    sentiments=df_train.sentiment.values, ## Sentiment\n",
        "    selected_texts=df_train.selected_text.values) ## Le target: texte séléctionné "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrxYii-8jOH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84620de6-71aa-4795-beb6-1f9b13fd4b77"
      },
      "source": [
        "print(\"Map-style datasets:\",train_dataset,\"de longeure\",len(train_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Map-style datasets: <dataset.TweetDataset object at 0x7f09a61f25f8> de longeure 21984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRK5sknouPHy"
      },
      "source": [
        "* En principe, le DataLoader fonctionne avec l'objet Dataset (objet itérable). Dans notre cas, cet objet est TweetDataset. Pour utiliser le DataLoader, nous devons faire entrer nos données dans l'enveloppe du Dataset. Pour ce faire, il faut implémenter deux méthodes : __getitem__ et __len__. La __getitem__ prend un index et renvoie un objet dictionnaire de `{'ids', 'mask', 'token_type_ids', 'start_labels', 'end_labels', 'orig_start', 'orig_end', 'orig_tweet', 'orig_selected', 'sentiment', 'offsets', 'targets_select'}`. Ids. Le __len__ est juste le __len habituel qui renvoie la taille du tweet numpy.ndarray ces deux méthodes sont implémentées dans le fichier **`dataset.py`**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdHtGdc8aX8l"
      },
      "source": [
        "## batch_size (Taille du lot): \n",
        "## Cette option permet de déterminer le nombre de documents traités dans chaque lot (32).\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.TRAIN_BATCH_SIZE,\n",
        "    num_workers=4,  ## Ils ont utilisé le multiprocessing pour charger les données et les injecter au GPU.\n",
        "    shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134,
          "referenced_widgets": [
            "ed2e8946b1004d4ca98dfadfa33115f5",
            "983766538d1c4e94b5c1599a68ec02d5",
            "50c37752036240f098ea6979b4f01416",
            "1bcc8d683c504de2a1d2ac53c49b84a0",
            "1b0d52f94b944e74b65b3bbd854e58ee",
            "8df86989d6fe4dcdac94e453f03ebc3e",
            "d1ad2c4487d84b8c94a19ce2e53a393a",
            "41678fc6278e4acc9446a04a5d936445",
            "23007c4d13ba48839736aaa552a33750",
            "95f23dc7fd694a1aaf802da9e4f697aa",
            "7faab71086174139b707fb5332ea2906",
            "9cf511a065854bfeb286a3ab5d2be7ab",
            "997ea70e664b4d2aa193ba8d6f3d5569",
            "ced557f1d8244f6abaabf8e2011dab49",
            "cac3e239cfa34eb3b7d6824b92049515",
            "342d70f33ebb469c9f18a9f3cfefb843"
          ]
        },
        "id": "_tS4Y1MxakhE",
        "outputId": "eaec372d-cecd-461f-e3da-a4c902ce915d"
      },
      "source": [
        "device = torch.device('cuda')\n",
        "## Instantiation du model da ce cas c'est le Roberta Base(786 embedding dim) \n",
        "## comme il est configuré dans le fichier config.py 'deepset/roberta-base-squad2', \n",
        "## ce modèle est pré-entrainé su la base de données Sqaud2\n",
        "model_config = transformers.RobertaConfig.from_pretrained(config.MODEL_CONFIG)\n",
        "## Activez le retour des statuts cachés de toutes les couches\n",
        "model_config.output_hidden_states = True\n",
        "## \n",
        "model = models.TweetModel(conf=model_config)\n",
        "model = model.to(device)\n",
        "print(\"------------------> here\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed2e8946b1004d4ca98dfadfa33115f5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=571.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23007c4d13ba48839736aaa552a33750",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=496313727.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "------------------> here\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QtDUC2J76qo"
      },
      "source": [
        "num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_5F6jmA7_Ij",
        "outputId": "4aff0f9f-e324-4f20-ad85-3c56c24a4f62"
      },
      "source": [
        "print(\"Nombre ditération pour lentrainement %s = la longeur de la base \\nd'entrainemet %s  divisé par la taille du batch %s multuplié \\npar le nombre depoches %s\"%(num_train_steps,len(df_train),config.TRAIN_BATCH_SIZE,config.EPOCHS))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nombre ditération pour lentrainement 2748 = la longeur de la base \n",
            "d'entrainemet 21984  divisé par la taille du batch 32 multuplié \n",
            "par le nombre depoches 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzG9ibn79Clb"
      },
      "source": [
        "## Récupérer les paramètres (weights) de l'optimizer\n",
        "param_optimizer = list(model.named_parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7qd60f59EDw"
      },
      "source": [
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "## Initialistion des parametres de loptemizer puis on les récupère du modèle pré-entrainé sur SQuad2\n",
        "optimizer_parameters = [\n",
        "                        {'params': [p for n, p in param_optimizer\n",
        "                                    if not any(nd in n for nd in no_decay)],\n",
        "                         'weight_decay': config.WEIGHT_DECAY},\n",
        "                        {'params': [p for n, p in param_optimizer\n",
        "                                    if any(nd in n for nd in no_decay)],\n",
        "                         'weight_decay': 0.0}]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbFG9itN-Hhx",
        "outputId": "8e27b890-1278-45a9-f147-07a85322fff8"
      },
      "source": [
        "## l'optimiseur en fonction du temps \n",
        "base_opt = transformers.AdamW(optimizer_parameters,\n",
        "                              lr=config.LEARNING_RATE)\n",
        "base_opt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdamW (\n",
              "Parameter Group 0\n",
              "    betas: (0.9, 0.999)\n",
              "    correct_bias: True\n",
              "    eps: 1e-06\n",
              "    lr: 4e-05\n",
              "    weight_decay: 0.001\n",
              "\n",
              "Parameter Group 1\n",
              "    betas: (0.9, 0.999)\n",
              "    correct_bias: True\n",
              "    eps: 1e-06\n",
              "    lr: 4e-05\n",
              "    weight_decay: 0.0\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS6v-r94aqRO"
      },
      "source": [
        "## SWA : la technique SWA (Stochastic Weight Averaging) est présenter au dessus avant le déboguage\n",
        "optimizer = torchcontrib.optim.SWA(\n",
        "    base_opt,\n",
        "    swa_start=int(num_train_steps * config.SWA_RATIO),\n",
        "    swa_freq=config.SWA_FREQ,\n",
        "    swa_lr=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eGaNRj3ayOR"
      },
      "source": [
        "## Scheduler permet de contrôler automatiquement les taux d'apprentissage\n",
        "scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=int(num_train_steps * config.WARMUP_RATIO),\n",
        "    num_training_steps=num_train_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gdUwXcXa1Z_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9cb7c2e-6b14-4207-fff9-8c5992f2e8d2"
      },
      "source": [
        "engine.train_fn(train_data_loader, model, optimizer,device, scheduler=scheduler)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/687 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "100%|██████████| 687/687 [07:13<00:00,  1.59it/s, loss=0.0219]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rQoVdMKESFy"
      },
      "source": [
        "Deboguage de la méthode **`train_fn`** du fichier **`engine.py`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF0L-7hoB2_b"
      },
      "source": [
        "import tqdm\n",
        "tk0 = tqdm.tqdm(train_data_loader, total=len(train_data_loader))\n",
        "for bi, d in enumerate(tk0):\n",
        "  print(\"-------------------------------------------------------------\")\n",
        "  print(bi)\n",
        "  print(\"-------------------------------------------------------------\")\n",
        "  print(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDT_UCrGF4dn"
      },
      "source": [
        "##Le code initial du fichier **`train.py`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1023bec17f3048f2994587ab2ca923ff",
            "4f836dfd3d614d37a6e12828b9e26dea",
            "0288fee9c3674c198d5e21621aac5273",
            "56eabcc0b14d4fa6a0b5b2323b3a27a5",
            "9c97ceae839b48029d9f89ed8021d1a2",
            "d0732c192a784c53b86a68f3a16254a6",
            "556a4510c3234b38a87c3743b8df9c97",
            "ec4bd0f01be341e88fc41673825fbaca",
            "2d6a2ea3e1454d28bd5a9760e19a4ca7",
            "6cad7320919248cab53cba7a3eb47225",
            "18287140c5324a7e85b01460e5e889f5",
            "fb4870326d1545dcaab261730965bc99",
            "11cd33748ed9445c921d533c922a2128",
            "4ae80b3c967142b1b773b080ed310264",
            "31bf737378c04156a4ced60a3b781572",
            "8344b11a1c42484883bb312ebfa677c6"
          ]
        },
        "id": "xS47DeGUGyt1",
        "outputId": "1bcfe209-c4d3-425e-a5a4-811fac7f4167"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import torch\n",
        "import torchcontrib\n",
        "\n",
        "import config\n",
        "import dataset\n",
        "import models\n",
        "import engine\n",
        "import utils\n",
        "\n",
        "## Ce code permet de lancé l'entrainement sur les 5 folds \n",
        "def run(fold):\n",
        "   \n",
        "    dfx = pd.read_csv(config.TRAINING_FILE)\n",
        "    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n",
        "    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n",
        "\n",
        "    ## Créer un Map-style datasets sur les données d'entrainement\n",
        "    train_dataset = dataset.TweetDataset(\n",
        "        tweets=df_train.text.values,\n",
        "        sentiments=df_train.sentiment.values,\n",
        "        selected_texts=df_train.selected_text.values)\n",
        "    \n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.TRAIN_BATCH_SIZE,\n",
        "        num_workers=4,  ## Ils ont utilisé le multiprocessing pour charger les données et les injecter au GPU.\n",
        "        shuffle=True)\n",
        "    \n",
        "    ## Créer un Map-style datasets sur les données d'entrainement \n",
        "    valid_dataset = dataset.TweetDataset(\n",
        "        tweets=df_valid.text.values,\n",
        "        sentiments=df_valid.sentiment.values,\n",
        "        selected_texts=df_valid.selected_text.values)\n",
        "\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=config.VALID_BATCH_SIZE,\n",
        "        num_workers=4,  ## Ils ont utilisé le multiprocessing pour charger les données et les injecter au GPU.\n",
        "        shuffle=False)\n",
        "\n",
        "    device = torch.device('cuda')\n",
        "    model_config = transformers.RobertaConfig.from_pretrained(\n",
        "        config.MODEL_CONFIG)\n",
        "    model_config.output_hidden_states = True\n",
        "    model = models.TweetModel(conf=model_config)\n",
        "    model = model.to(device)\n",
        "    print(\"------------------> here\")\n",
        "    \n",
        "    ## Nombre d'iteration c'est le nombre des données d'entré (tweets) divisé par\n",
        "    ## la taille du batch dans le fichier config.py\n",
        "    num_train_steps = int(\n",
        "        len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
        "    ## Récupérer les paramètres (les poids) de l'optimizer\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "\n",
        "    ## puisqu'il est recommandé d'utiliser cet optimiseur pour le fine tuning\n",
        "    ## (modification sur l'architecture), puisque c'est ainsi que le modèle a \n",
        "    ## été entraîné et de conserver les mêmes comportements que ceux mentionnés \n",
        "    ## sur le repo git de BERT (https://github.com/google-research/bert/blob/master/optimization.py#L65)\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    optimizer_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer\n",
        "                    if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': config.WEIGHT_DECAY},\n",
        "        {'params': [p for n, p in param_optimizer\n",
        "                    if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': 0.0}]\n",
        "    ## AdamW: un optimiseur adaptatif avec utilisation d'une échelle de taux\n",
        "    ## d'apprentissage pour moduler l'évolution du taux d'apprentissage de\n",
        "    ## l'optimiseur en fonction du temps \n",
        "    base_opt = transformers.AdamW(optimizer_parameters,\n",
        "                                  lr=config.LEARNING_RATE)\n",
        "    ## SWA : la technique SWA (Stochastic Weight Averaging) est présenter au dessus de cette cellule\n",
        "    optimizer = torchcontrib.optim.SWA(\n",
        "        base_opt,\n",
        "        swa_start=int(num_train_steps * config.SWA_RATIO),\n",
        "        swa_freq=config.SWA_FREQ,\n",
        "        swa_lr=None)\n",
        "    ## Scheduler permet de contrôler automatiquement les taux d'apprentissage\n",
        "    scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=int(num_train_steps * config.WARMUP_RATIO),\n",
        "        num_training_steps=num_train_steps)\n",
        "\n",
        "    print(f'Training is starting for fold={fold}')\n",
        "    ## chaque process (l'entrainement, l'évaluation et la prédiction) sont fait sur 5 folds\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        ## Lancer l'entrainement\n",
        "        engine.train_fn(train_data_loader, model, optimizer,device, scheduler=scheduler)\n",
        "        ## Lancer l'évaluation\n",
        "        jaccard = engine.eval_fn(valid_data_loader, model, device)\n",
        "\n",
        "    if config.USE_SWA:\n",
        "        ## Utiliser l'optimiseur SWA\n",
        "        ## Le SWA effectue une moyenne égale des poids traversés par le SGD\n",
        "        optimizer.swap_swa_sgd()\n",
        "    ## Sauvegardé le modèle après l'entraînement et l'valuation de chaque fold model_0 ==> fold 0,...\n",
        "    torch.save(model.state_dict(),\n",
        "               f'{config.MODEL_SAVE_PATH}/model_{fold}.bin')\n",
        "    ## Renvoie le résultat de la métrique utilisée qui est le score jaccard modifié\n",
        "    return jaccard\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    utils.seed_everything(seed=config.SEED)\n",
        "\n",
        "    fold_scores = []\n",
        "    for i in range(config.N_FOLDS):\n",
        "        fold_score = run(i)\n",
        "        fold_scores.append(fold_score)\n",
        "\n",
        "    print('\\nScores without SWA:')\n",
        "    for i in range(config.N_FOLDS):\n",
        "        print(f'Fold={i}, Jaccard = {fold_scores[i]}')\n",
        "    print(f'Mean = {np.mean(fold_scores)}')\n",
        "    print(f'Std = {np.std(fold_scores)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1023bec17f3048f2994587ab2ca923ff",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=571.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d6a2ea3e1454d28bd5a9760e19a4ca7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=496313727.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------> here\n",
            "Training is starting for fold=0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "100%|██████████| 687/687 [07:04<00:00,  1.62it/s, loss=0.0218]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.92it/s, jaccard=0.703, loss=0.01]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7029905612369929\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:14<00:00,  1.58it/s, loss=0.0104]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.92it/s, jaccard=0.712, loss=0.00929]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7118255485009135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.00902]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.93it/s, jaccard=0.717, loss=0.00939]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7170824580463284\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.00808]\n",
            "100%|██████████| 172/172 [00:35<00:00,  4.89it/s, jaccard=0.716, loss=0.00943]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7163026420895469\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------> here\n",
            "Training is starting for fold=1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
            "100%|██████████| 687/687 [07:14<00:00,  1.58it/s, loss=0.0196]\n",
            "100%|██████████| 172/172 [00:35<00:00,  4.90it/s, jaccard=0.696, loss=0.0099]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.6963974280475891\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.59it/s, loss=0.01]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.94it/s, jaccard=0.705, loss=0.00921]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7054961819176117\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.0088]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.94it/s, jaccard=0.711, loss=0.00945]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7110974016990879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:14<00:00,  1.58it/s, loss=0.00799]\n",
            "100%|██████████| 172/172 [00:35<00:00,  4.90it/s, jaccard=0.711, loss=0.00939]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7108215431021135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------> here\n",
            "Training is starting for fold=2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.0199]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.95it/s, jaccard=0.701, loss=0.0105]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7007900525880214\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.59it/s, loss=0.0103]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.92it/s, jaccard=0.707, loss=0.00936]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7070440963345973\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.00895]\n",
            "100%|██████████| 172/172 [00:35<00:00,  4.91it/s, jaccard=0.713, loss=0.00921]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7134881141772793\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.00821]\n",
            "100%|██████████| 172/172 [00:35<00:00,  4.91it/s, jaccard=0.716, loss=0.00934]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7158955035127735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------> here\n",
            "Training is starting for fold=3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
            "100%|██████████| 687/687 [07:14<00:00,  1.58it/s, loss=0.0211]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.92it/s, jaccard=0.694, loss=0.0106]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.6944610752763127\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.0102]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.93it/s, jaccard=0.709, loss=0.00972]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7094754330307022\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.00889]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.92it/s, jaccard=0.717, loss=0.00957]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7172411822420536\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.00803]\n",
            "100%|██████████| 172/172 [00:35<00:00,  4.91it/s, jaccard=0.717, loss=0.00964]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7166221978997807\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------> here\n",
            "Training is starting for fold=4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
            "100%|██████████| 687/687 [07:14<00:00,  1.58it/s, loss=0.0203]\n",
            "100%|██████████| 172/172 [00:35<00:00,  4.90it/s, jaccard=0.695, loss=0.0104]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.694875414018416\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.0108]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.93it/s, jaccard=0.703, loss=0.0103]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.70296628740658\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.0094]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.92it/s, jaccard=0.707, loss=0.00943]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7067339098954668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:14<00:00,  1.58it/s, loss=0.00862]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.94it/s, jaccard=0.711, loss=0.00943]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7109265704332721\n",
            "\n",
            "Scores without SWA:\n",
            "Fold=0, Jaccard = 0.7163026420895469\n",
            "Fold=1, Jaccard = 0.7108215431021135\n",
            "Fold=2, Jaccard = 0.7158955035127735\n",
            "Fold=3, Jaccard = 0.7166221978997807\n",
            "Fold=4, Jaccard = 0.7109265704332721\n",
            "Mean = 0.7141136914074974\n",
            "Std = 0.002655369837057321\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qYuhGoYmRuQ"
      },
      "source": [
        "# create_folds.py \n",
        "Voici le code qui correspond au fichier create_folds.py sous leur dépôt sur [github](https://github.com/heartkilla/kaggle_tweet/blob/master/src/create_folds.py).\n",
        "\n",
        "* Ce code est utilisé pour échantillonner les données sur 5 plis (de 0 à 4) à chaque fois qu'un pli sera utilisé comme donnée de validation et les 4 autres plis seront utilisés comme données d'entrainement.\n",
        "* On crée une nouvelle colonne **`kfol`** ajoutée à l'ensemble de données **`train.csv`** contenant le fold correspondant.\n",
        "* Tous les plis contiennent **`50898`** ligne de données."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jGYheYomREa"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import model_selection\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = pd.read_csv(\"./data/train.csv\")\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "    df[\"kfold\"] = -1\n",
        "\n",
        "    df = df.sample(frac=1, random_state=50898).reset_index(drop=True)\n",
        "\n",
        "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "\n",
        "    for fold, (trn_, val_) in enumerate(kf.split(X=df, y=df.sentiment.values)):\n",
        "        print(len(trn_), len(val_))\n",
        "        df.loc[val_, 'kfold'] = fold\n",
        "\n",
        "    df.to_csv(\"./data/train_folds.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}