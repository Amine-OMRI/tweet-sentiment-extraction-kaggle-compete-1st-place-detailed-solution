# [Kaggle Tweet Sentiment Extraction Competition](https://www.kaggle.com/c/tweet-sentiment-extraction/leaderboard): 1st place solution (Dark of the Moon team) 

Ce repository contient la solution d√©taill√©e qui ont remport√© la premi√®re place de la comp√©tition kaggle appel√© "Tweet Sentiment Extraction" il y a 5 mois.
  ### l'√©quipe qui a remport√© la comp√©tition, compos√©e de:
   [Heartkilla](https://www.kaggle.com/aruchomu): Artsem Zhyvalkouski Data Scientist</br>
   [Hikkiiii](https://www.kaggle.com/wochidadonggua): Hikkiiii NLP Data scientist Beijing, Beijing, China</br>
   [Theo](https://www.kaggle.com/theoviel): Th√©o Vie Chercheur scientifique √† Paris, √éle-de-France, Franc</br>
   [Cl_ev](https://www.kaggle.com/cl2ev1): Anton travaille dans l'IT London, England, United Kingdom</br>

 ## Pr√©sentation de la comp√©tition 
   La comp√©tition portait sur l'extraction de sentiments, ci-dessous un r√©sum√© sur le sujet :

  "My ridiculous dog is amazing." [sentiment: positive]</br>
  Avec tous les tweets qui circulent √† chaque seconde, il est difficile de dire si le sentiment derri√®re un tweet sp√©cifique aura un impact sur la marque d'une     entreprise ou d'une personne parce qu'il est viral (positif), ou s'il d√©vastera les profits parce qu'il a un ton n√©gatif. Il est important de saisir les           sentiments dans la langue, √† un moment o√π les d√©cisions et les r√©actions sont cr√©√©es et mises √† jour en quelques secondes. Mais quels mots conduisent r√©ellement   √† la description des sentiments ? Dans cette comp√©tition, l'objectif est de choisir la partie du tweet (mot ou phrase) qui refl√®te le sentiment.
  Quels mots des tweets refl√®tent un sentiment positif, n√©gatif ou neutre ? tout en utilisant les outils d'apprentissage automatique ?</br>
  Dans ce concours, ils ont extrait les phrases de la plateforme ["Data for Everyone"](https://appen.com/resources/datasets/). L'ensemble de donn√©es est intitul√©   "Analyse du sentiment": Emotion in Text".</br>
  L'objectif de ce concours est de construire un mod√®le qui peut faire la m√™me chose - examiner le sentiment √©tiquet√© pour un tweet donn√© et d√©terminer quel mot     ou phrase le soutient le mieux.</br>
   ### T√¢che
   - Pour un tweet donn√©, pr√©dire quel mot ou quelle phrase refl√®te le mieux le sentiment √©tiquet√©
   ### Data
  ‚óã Train: 27k tweets</br>
  ‚óã Public / private test: 4k / 8k tweets 
  | Texte (donn√©) | Sentiment (donn√©) | texte_s√©lectionn√© (target) |
  | :---: | :---: | :---: |
  | J'aime beaucoup la chanson Love Story de Taylor Swift | positive | j'aime |
  | je dois r√©cup√©rer mon ordinateur r√©par√© | neutre | je dois r√©cup√©rer mon ordinateur r√©par√© |
  | trop triste, tu vas me manquer ici √† San Diego ! ! | n√©gatif | trop triste |

   ### Evaluation
  La m√©trique utilis√©e dans cette comp√©tition est le [score Jaccard](https://en.wikipedia.org/wiki/Jaccard_index) au niveau des mots. Vous trouverez [ici](https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50) une bonne description de la similarit√© de Jaccard pour les cha√Ænes de caract√®res.</br>
  ![alt text](https://neo4j.com/docs/graph-algorithms/current/images/jaccard.png)

  Une impl√©mentation Python bas√©e sur les liens ci-dessus:
  ```ruby
  def jaccard(str1, str2): 
      a = set(str1.lower().split()) 
      b = set(str2.lower().split())
      c = a.intersection(b)
      return float(len(c)) / (len(a) + len(b) - len(c))
  ```
  La similarit√© ou l'intersection Jaccard sur l'union est d√©finie comme la taille de l'intersection divis√©e par la taille de l'union de deux ensembles. Prenons     l'exemple de deux phrases :</br>
  - Phrase 1 : AI is our friend and it has been friendly</br>
  - Phrase 2 : AI and humans have always been friendly</br>
  ![alt text](https://miro.medium.com/max/463/1*u2ZZPh5er5YbmOg7k-s0-A.png)</br>
  ==> Pour les deux phrases ci-dessus, nous obtenons une similarit√© Jaccard de 5/(5+3+2) = 0,5 qui est la taille de l'intersection de l'ensemble divis√©e par la   taille totale de l'ensemble.


 ## Pr√©sentation de la solution 
  Pratiquement toutes les t√¢ches de l'NLP sont maintenant r√©solues √† l'aide des Transformers et Cette solution est aussi bas√©e sur ces derniers.
  - Les transformateurs (Transformers) tels que BERT, RoBERTa, BART, CamemBERT etc. sont devenus l'√©tat de l'art en NLP
  - pr√©-entra√Æn√©s sur une grande quantit√© de textes
  - Un peu long √† entra√Æner
  - Peut √™tre utilis√© pour ces t√¢ches dans le cadre du NER (Reconnaissance d'entit√©s nomm√©es (Named-entity recognition)) ou de l'QA (Question Answering)
  - la solution bas√©e sur la QA a donn√© les meilleurs r√©sultats

   ![alt text](https://raw.githubusercontent.com/huggingface/transformers/master/docs/source/imgs/transformers_logo_name.png)</br>
   
   l'etat de l'art pour le traitement de langage naturel de pour Pytorch et TensorFlow 2.0 </br>
   ü§ó Transformers (anciennement connu sous le nom de pytorch-transformers et pytorch-pretrained-bert) fournit des architectures √† usage g√©n√©ral (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet...) pour la compr√©hension du langage naturel (TALN / NLP) et la g√©n√©ration du langage naturel (NLG) avec plus de 32 mod√®les pr√©-entra√Æn√©s dans plus de 100 langues et une interop√©rabilit√© profonde entre TensorFlow 2.0 et PyTorch.</br>
   ü§ó Transformers fournit des milliers de mod√®les pr√©-entrain√©s pour effectuer des t√¢ches sur des textes telles que la classification, l'extraction d'informations, la r√©ponse √† des questions (QA), le r√©sum√©, la traduction, la g√©n√©ration de texte, etc. dans plus de 100 langues. </br>
   voici la [documentation](https://github.com/huggingface/transformers) des transformers sur leur repo github.</br>
   Ci-dessous, quelques exemples :
  - [Masked word completion with BERT](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)
  - [Name Entity Recognition with Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)
  - [Text generation with GPT-2](https://huggingface.co/gpt2?text=A+long+time+ago%2C+)
  - [Natural Langugage Inference with RoBERTa](https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)
  - [Summarization with BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)
  - [Question answering with DistilBERT](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)
  - [Translation with T5](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)</br>
  
   **Comment BERT apprend**
   BERT apprend de fa√ßon non supervis√©e, l'entr√©e se suffit √† elle m√™me, pas besoin de labelliser, qualifier quoi que ce soit, on se servira uniquement de l'entr√©e, et de plusieurs mani√®res. Nous appellerons √ßa le MLM pour "Masked language model". Nous allons d√©cortiquer ce que le mod√®le tente d'apprendre.</br>
Notations : **[CLS]** indique un d√©but de s√©quence, **[SEP]** une s√©paration, en g√©n√©ral entre deux phrases, **[MASK]** un mot masqu√©</br>
  **Les mots masqu√©s (MLM):**
  Ici la s√©quence d'entr√©e a √©t√© volontairement oblit√©r√©e d'un mot, le mot masqu√©, et le mod√®le va apprendre √† pr√©dire ce mot masqu√©.</br>
  Entr√©e = [CLS] the man went to [MASK] store [SEP] </br>
  Entr√©e = [CLS] the man [MASK] to the store [SEP]</br>
  **La phrase suivante (NSP):**
  Ici le mod√®le doit d√©terminer si la s√©quence suivante (suivant la s√©paration[SEP]) est bien la s√©quence suivante. Si oui, IsNext sera vrai, le label sera IsNext, si non, le label sera NotNext.</br>
  Entr√©e = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]</br>
  Label = IsNext</br>
  Entr√©e = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]</br>
  Label = NotNext</br>
  [Source ici. Architecture globale de "The Transformer"](https://lesdieuxducode.com/images/blog/pauldenoyes@expaceocom/BERT-Transformer-Architecture-Globale.png)

  ### Question answering solution (QA)
   Dans leur solution, ils ont mis en place diff√©rents m√©canismes, tels que la reconnaissance des entit√©s nomm√©es NER et le Question answering, pour r√©pondre aux questions qui ont le mieux fonctionn√©, ils ont consid√©r√© le sentiment comme la question et la sous-phrase ou le mot comme la r√©ponse, et ils l'envoient au transformateur suivi d'une couche dense / fully connected et une Softmax pour pr√©dire deux ensembles de prababilit√©s.</br>
le premier ensemble contient les probabilit√©s pour chaque token, que le token soit le **D√©but** du texte s√©lectionn√©</br>
le deuxi√®me ensemble contient les probabilit√©s pour chaque token, que le token soit la **Fin** du texte s√©lectionn√©</br>
   - Question: sentiment
   - Answer: texte s√©lectionn√©
    ![alt text](https://github.com/Amine-OMRI/tweet-sentiment-extraction-kaggle-compete-1st-place-detailed-solution/blob/main/model_architecture.png?raw=true)
  ### Les mod√®les utilis√©s
   **Les mod√®les de [Heartkilla](https://www.kaggle.com/aruchomu):**</br>
   RoBERTa-base-squad2, RoBERTa-large-squad2,DistilRoBERTa-base, base XLNet
   - Il a fait un pr√©-entrainement sur SQuAD 2.0</br>
   - Certains d'entre eux sont d√©j√† pr√©-entrain√©s sur SQuAD 2.0, ce qui fait que non seulement le pr√©-entrainement initial fonctionne, mais aussi la t√¢che de pr√©-      entrainement fonctionnais bien.</br>
   - Il a modifi√© l'architecture (Fine tuning) en effectuant une moyenne et un maxpooling (Avg / Max ) de toutes les couches sans embeddings.</br>
   - Il a √©galement effectu√© un Multi Sample Dropout (MSD): cette technique acc√©l√®re l'entrainement et permet une meilleure g√©n√©ralisation pour les r√©seaux de neurones.
   - AdamW: un optimiseur adaptatif avec utilisation d'une √©chelle de taux d'apprentissage pour moduler l'√©volution du taux d'apprentissage de l'optimiseur en fonction du temps [(plus de d√©tails ici)](https://arxiv.org/pdf/1711.05101.pdf).</br>
   - Une loss personnalis√©e : Jaccard-based Soft Labels.</br>
   - Le meilleur mod√®le unique : RoBERTa-base-squad2, 5 fold CV stratified : 0,715</br>   
   **Multi Sample Dropout (MSD):** C'est l'une des techniques qu'ils ont utilis√©es et que je trouve si int√©ressante. En fait, il applique un dropout plusieurs fois avec diff√©rents masques et ensuite il calcule la moyenne des r√©sultats</br>
  Le dropout initial cr√©e un sous-ensemble choisi au hasard (appel√© dropout sample) √† partir des donn√©es d'entr√©e de chaque it√©ration d'entrainement, tandis que le MSD cr√©e plusieurs √©chantillon de dropout. La loss est calcul√©e pour chaque √©chantillon, puis la moyenne des losses des √©chantillons est calcul√©e pour obtenir la Loss finale [(plus de d√©tails ici)](https://arxiv.org/pdf/1905.09788.pdf).</br>
  ![alt text](https://github.com/Amine-OMRI/tweet-sentiment-extraction-kaggle-compete-1st-place-detailed-solution/blob/main/Multi-Sample-Dropout.png?raw=true)</br>
   **Custom loss Jaccard-based Soft Labels:** √âtant donn√© que la Cross Entropy n'optimise pas directement l'indice de Jaccard, Heartkilla a essay√© diff√©rentes fonctions de Loss pour p√©naliser davantage les pr√©dictions lointaines que les pr√©dictions proches. SoftIOU utilis√© dans la segmentation n'a pas aid√©, il a donc trouv√© une Loss personnalis√©e en calculant l'indice de Jaccard au niveau du token. Il a ensuite utilis√© ces nouveaux labels cibles et a optimis√© la divergence. Alpha c'est un param√®tre permettant d'√©quilibrer l'√©tiquetage habituel bas√© sur la Cross Entropy et l'indice de Jaccard.
![alt text](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2000545%2F9341bede28263bcf0e9bb259ac790338%2FScreen%20Shot%202020-05-30%20at%2017.31.22.png?generation=1592405028556842&alt=media)</br>
  Il a √©galement remarqu√© que les probabilit√©s dans ce cas changent assez rapidement, alors il d√©cid√© de corriger un peu le r√©sultat en ajoutant un carr√©.
  ![alt text](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2000545%2F4d1975e293c33c077fd45e6b81d1aa63%2FScreen%20Shot%202020-05-30%20at%2017.29.17.png?generation=1592405079812280&alt=media)</br>
  Cela a parfaitement fonctionn√© pour trois de ses mod√®les, √† l'exception du DistilRoBERTa qui utilisait la pr√©c√©dente version sans le carr√©. Finalement, cette Loss a augment√© tous les mod√®les d'environ 0,003.
  Voici un graphique des probabilit√©s target pour une phrase de 30 tokens avec start_idx=5 et end_idx=25, alpha=0,3.</br>
   ![alt text](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2000545%2Fd746070e62bc05d74f7543785da6df70%2Fplot.jpg?generation=1592405194100691&alt=media)</br>
   
   **Les mod√®les de [Theo](https://www.kaggle.com/theoviel):**</br>
   ‚óè Transformateurs</br>
      ‚óã BERT-base-uncased (CV 0.710)</br>
      ‚óã BERT-large-uncased-wwm (CV 0.710)</br>
      ‚óã DistilBERT (CV 0.705)</br>
      ‚óã ALBERT-large-v2 (CV 0.711)</br>
   ‚óè Architecture</br>
      ‚óã MSD sur la concat√©nation des 8 derniers √©tats cach√©s</br>
   ‚óè Entrainement</br>
      ‚óã Adoucir la cross-entropy cat√©gorielle</br>
      ![alt text](https://github.com/Amine-OMRI/tweet-sentiment-extraction-kaggle-compete-1st-place-detailed-solution/blob/main/Smoothed-categorical-cross-entropy.png?raw=true)</br>
      ‚óã [Taux d'apprentissage discriminatoire](https://arxiv.org/pdf/1801.06146.pdf)</br>
      ![alt text](https://github.com/Amine-OMRI/tweet-sentiment-extraction-kaggle-compete-1st-place-detailed-solution/blob/main/Discriminative-learning-rate.png?raw=true)</br>
      ‚óã **Sequence bucketing** pour acc√©l√©rer l'entrainement : L'id√©e est d'effectuer une mise en bucket (bucketing) du corpus d'entra√Ænement, o√π chaque bucket repr√©sente une plage de longueurs d'√©nonciation et chaque √©chantillon d'entra√Ænement est attribu√© au bucket qui correspond √† sa longueur.Ensuite, un batch est construit en tirant des s√©quences √† partir d'un bucket choisi au hasard. Le concept de bucketing att√©nue d'une certaine mani√®re le probl√®me des zero-padding si des plages de longueur appropri√©es peuvent √™tre d√©finies, tout en permettant un certain niveau de caract√®re al√©atoire au moins lorsque des s√©quences sont s√©lectionn√©es dans un batch.  Toutefois, les buckets doivent √™tre tr√®s grands pour assurer une variabilit√© suffisamment importante au sein des batches. D'un autre c√¥t√©, la fabrication de buckets trop grands augmentera le temps d'entrainement en raison de calculs non pertinents sur des zero-padding s√©quences. Le r√©glage correct de ces hyperparam√®tres est donc d'une importance fondamentale pour un entrainement rapide et robuste des mod√®les acoustiques.</br>
   
   **Les mod√®les de [Cl_ev (Anton)](https://www.kaggle.com/cl2ev1):**</br>
   ‚óè Transformateurs</br>
      ‚óã RoBERTa-base (CV 0.715)</br> 
      ‚óã BERTweet: Un mod√®le linguistique pr√©-entrain√© sur des [Tweets  en anglais](https://arxiv.org/pdf/2005.10200.pdf)</br>
   ‚óè Architecture: Pareil que ceux de Th√©o</br>
   ‚óè Entrainement</br>
      ‚óã Smoothed categorical cross-entropy</br>
      ‚óã Discriminative learning rate</br>
      ‚óã Fichier custom merges.txt pour RoBERTa</br>
   
   **Les mod√®les de [Hikkiiii](https://www.kaggle.com/wochidadonggua):**</br>
    ‚óè Transformateurs</br>
      ‚óã 5fold-roberta-base-squad2(0.712CV) </br> 
      ‚óã 5fold-roberta-large-squad2(0.714CV) </br>
   ‚óè Architecture: 
      ‚óã Ajoutez le sentiment √† la fin du texte
      ‚óã CNN + Couche lin√©aire sur la concat√©nation des derniers **3 hidden states**</br>
   ‚óè Entrainement</br>
      ‚óã Smoothed categorical cross-entropy</br>
      ‚óã Discriminative learning rate</br>
      ‚óã Fichier custom merges.txt pour RoBERTa</br>

   
   ### Tkenisation 
  **Qu'est-ce qu'un tokeniser ?**
  Un tokenizer re√ßoit un flux de caract√®res, le d√©compose en tokens individuels (g√©n√©ralement des mots individuels) et produit un flux de tokens. Par exemple, un   tokenizer d'espacement d√©compose le texte en tokens chaque fois qu'il voit un espacement. Il convertit le texte "Quick brown fox !" en termes ["Quick",  "brown", " fox !"].

 ![alt text](https://camo.githubusercontent.com/541a5e3521cf5b4c84c7ced36628841d8e66d58b7f2e51cded099a18c006d4e9/68747470733a2f2f68756767696e67666163652e636f2f6c616e64696e672f6173736574732f746f6b656e697a6572732f746f6b656e697a6572732d6c6f676f2e706e67)
  D√©velopp√© par huggingdace, propose une impl√©mentation des tokenizers les plus utilis√©s aujourd'hui, en mettant l'accent sur les performances et la polyvalence

  Extr√™mement rapide (√† la fois pour l'entra√Ænement et la tokenisation), gr√¢ce √† l'impl√©mentation de Rust. Il faut moins de 20 secondes pour tokeniser un Go de     texte sur l'unit√© centrale d'un serveur.
    - Facile √† utiliser, mais aussi extr√™mement polyvalent.
    - Con√ßu pour la recherche et la production.
    - La normalisation s'accompagne d'un suivi des alignements. Il est toujours possible d'obtenir la partie de la phrase originale qui correspond √† un jeton donn√©.
    - Effectue tout le pr√©traitement : Tronquer, Tamponner, ajouter les tokens sp√©ciaux dont votre mod√®le a besoin.
  
  
  
  
  
  ## Pour plus de d√©tails sur leur solution
Ils ont fait un discours sur leur solution lors du meetup ODS Paris: [YouTube link](https://www.youtube.com/watch?v=S7soN-y5WMg)<br />
La pr√©sentation de leur solution : [SlideShare link](https://www.slideshare.net/ArtsemZhyvalkouski/kaggle-tweet-sentiment-extraction-1st-place-solution)
