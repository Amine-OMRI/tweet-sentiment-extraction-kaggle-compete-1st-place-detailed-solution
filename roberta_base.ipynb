{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "roberta_base.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BmUw-vAr6c_6",
        "Jro8AaTYD78D",
        "gtM5UFGcEAxb",
        "-_Q6Y5KPFUGh",
        "etmylaX2Fb-j",
        "BrP6RBNKFqd9",
        "ZUTPKlGDGpj8",
        "sPit1dlVHEIs",
        "QDzmFycLG6Rc"
      ],
      "mount_file_id": "1DSDVX5Zs9k8YqjQf2e-7iBNrkk0J43x2",
      "authorship_tag": "ABX9TyNHAWTEjuDp9/ir43LjPiJW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2a5a6a4853c647d5ab4e6f2e421c5b21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ea33407f48d84529a3cedce8ff4eb717",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_86e84d68c27d472b8a7a0d57a6e22c43",
              "IPY_MODEL_a042d53d0d374fbba87c5126bc89edb1"
            ]
          }
        },
        "ea33407f48d84529a3cedce8ff4eb717": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "86e84d68c27d472b8a7a0d57a6e22c43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fb2ad4da78f8468f8b5e0bacdfdb6a44",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 571,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 571,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_67b4d3d78dee4cc39c7679217f24bca9"
          }
        },
        "a042d53d0d374fbba87c5126bc89edb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f7174c2b850b4684b80adf245f073187",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 571/571 [00:49&lt;00:00, 11.6B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b40d221cc11e42abacb6d79d87b64e15"
          }
        },
        "fb2ad4da78f8468f8b5e0bacdfdb6a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "67b4d3d78dee4cc39c7679217f24bca9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f7174c2b850b4684b80adf245f073187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b40d221cc11e42abacb6d79d87b64e15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da3165f8a7bb4f1ba6002a3dd1839b23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ad49480862ce44e696e8ab8d60f4850c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fd76536cf49f4f9e9dcca7b8c6c9c804",
              "IPY_MODEL_97fb8388273247cea7095fce3ccd4789"
            ]
          }
        },
        "ad49480862ce44e696e8ab8d60f4850c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd76536cf49f4f9e9dcca7b8c6c9c804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2fdf7552d81344c6b23bbd1e82f34b80",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 496313727,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 496313727,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6991e98fa56846f98fdf918568cafd89"
          }
        },
        "97fb8388273247cea7095fce3ccd4789": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_79c265a4eeda434db25543b711583219",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 496M/496M [00:11&lt;00:00, 44.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c5c53ebd65b84df79429c42c41a456f0"
          }
        },
        "2fdf7552d81344c6b23bbd1e82f34b80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6991e98fa56846f98fdf918568cafd89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "79c265a4eeda434db25543b711583219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c5c53ebd65b84df79429c42c41a456f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "311e02e4ab79455f96a6a69d882cf44c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8fbea1556436405c82e84a37e15ee0c3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_be8768226ed74c338a38576cc1639b61",
              "IPY_MODEL_f195b12fa5be48fca00f7c2056222609"
            ]
          }
        },
        "8fbea1556436405c82e84a37e15ee0c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "be8768226ed74c338a38576cc1639b61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d40ab0eeaee6478aa0eec0bc7a5d883f",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 172,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 172,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2f1b9d41b21047ec94130b11e14337d6"
          }
        },
        "f195b12fa5be48fca00f7c2056222609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_71e0c96ecb90475a9767aa517b1ff34d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 172/172 [01:13&lt;00:00,  2.33it/s, jaccard=0.716]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_83643d9c675e43d0adf21bb7e7bb4bd2"
          }
        },
        "d40ab0eeaee6478aa0eec0bc7a5d883f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2f1b9d41b21047ec94130b11e14337d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "71e0c96ecb90475a9767aa517b1ff34d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "83643d9c675e43d0adf21bb7e7bb4bd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e67e719ed34a4b07a0591d376313ceeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a19299dbe1cb457fb61a2bd60a044123",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e2037ef67ba3407e9883a32f1a9ae9e9",
              "IPY_MODEL_ef8f28882f6a4689ac35c8b7dc69dc8a"
            ]
          }
        },
        "a19299dbe1cb457fb61a2bd60a044123": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e2037ef67ba3407e9883a32f1a9ae9e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_80baee64c83f4c05b13026fdd6c3928f",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 172,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 172,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_95fe9696a2b74d7ea8c85a71ed0ea401"
          }
        },
        "ef8f28882f6a4689ac35c8b7dc69dc8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1dd0410fc9704a9caa064a47ee8936bc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 172/172 [00:29&lt;00:00,  5.74it/s, jaccard=0.711]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ba08cfa1331d47489b03c860ed273728"
          }
        },
        "80baee64c83f4c05b13026fdd6c3928f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "95fe9696a2b74d7ea8c85a71ed0ea401": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1dd0410fc9704a9caa064a47ee8936bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ba08cfa1331d47489b03c860ed273728": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c649b48206e8461ab986a31eff41663d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_de48ecab2e9a43a2984091f852f12b25",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d9ccaad734f8466f9c609ba4c19ae320",
              "IPY_MODEL_79deeeeeaa964a06bc9f3ad5462ca312"
            ]
          }
        },
        "de48ecab2e9a43a2984091f852f12b25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d9ccaad734f8466f9c609ba4c19ae320": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1790d4b810e6479fb0986a33c7d6d89d",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 172,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 172,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3e0abbd9b372444a9ecb25a616239688"
          }
        },
        "79deeeeeaa964a06bc9f3ad5462ca312": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8388209c0eea421a864e786bf0e88db2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 172/172 [01:10&lt;00:00,  2.42it/s, jaccard=0.716]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e28785f8a2c4d26b9dedb98f993c35c"
          }
        },
        "1790d4b810e6479fb0986a33c7d6d89d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3e0abbd9b372444a9ecb25a616239688": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8388209c0eea421a864e786bf0e88db2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e28785f8a2c4d26b9dedb98f993c35c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6ae593eea4ea4bf29ccb9207bf39a987": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0a8fc24b90b944d1ba355ecbb00bfa58",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dc414fdaee294c9db8c677b27a81707f",
              "IPY_MODEL_b53ea0fc5b7e42f6961cc98b3b257128"
            ]
          }
        },
        "0a8fc24b90b944d1ba355ecbb00bfa58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dc414fdaee294c9db8c677b27a81707f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9f73581c6f304a2381a54c3aded4fb52",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 172,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 172,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ad0874d836e3493cb84f49eb88448e72"
          }
        },
        "b53ea0fc5b7e42f6961cc98b3b257128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_86d93fca9e4a417187f9512e505ccf8c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 172/172 [00:29&lt;00:00,  5.74it/s, jaccard=0.717]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0c571a2eff364548b85ad2d1a1ccc4cd"
          }
        },
        "9f73581c6f304a2381a54c3aded4fb52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ad0874d836e3493cb84f49eb88448e72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "86d93fca9e4a417187f9512e505ccf8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0c571a2eff364548b85ad2d1a1ccc4cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e34c097bd1a8400cab7a702b06a9143c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1eb203e1db144517ae6bf803be68d76b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c555a66b7f44451e82c325428f234bc4",
              "IPY_MODEL_5ac0aaff737641eca49308824bb94603"
            ]
          }
        },
        "1eb203e1db144517ae6bf803be68d76b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c555a66b7f44451e82c325428f234bc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1ec1aa1825834a7c96fddcafac8ee72b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 172,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 172,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ae9af73a41084fc2bbf736f4afd9f5dd"
          }
        },
        "5ac0aaff737641eca49308824bb94603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_24ea29ea46fe45f5876e16e9bbc5daca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 172/172 [29:37&lt;00:00, 10.33s/it, jaccard=0.711]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_806bf2cfe63e4b909522ad2c6b6f6d8c"
          }
        },
        "1ec1aa1825834a7c96fddcafac8ee72b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ae9af73a41084fc2bbf736f4afd9f5dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "24ea29ea46fe45f5876e16e9bbc5daca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "806bf2cfe63e4b909522ad2c6b6f6d8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1023bec17f3048f2994587ab2ca923ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4f836dfd3d614d37a6e12828b9e26dea",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0288fee9c3674c198d5e21621aac5273",
              "IPY_MODEL_56eabcc0b14d4fa6a0b5b2323b3a27a5"
            ]
          }
        },
        "4f836dfd3d614d37a6e12828b9e26dea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0288fee9c3674c198d5e21621aac5273": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9c97ceae839b48029d9f89ed8021d1a2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 571,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 571,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d0732c192a784c53b86a68f3a16254a6"
          }
        },
        "56eabcc0b14d4fa6a0b5b2323b3a27a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_556a4510c3234b38a87c3743b8df9c97",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 571/571 [00:08&lt;00:00, 69.9B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ec4bd0f01be341e88fc41673825fbaca"
          }
        },
        "9c97ceae839b48029d9f89ed8021d1a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d0732c192a784c53b86a68f3a16254a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "556a4510c3234b38a87c3743b8df9c97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ec4bd0f01be341e88fc41673825fbaca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2d6a2ea3e1454d28bd5a9760e19a4ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6cad7320919248cab53cba7a3eb47225",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_18287140c5324a7e85b01460e5e889f5",
              "IPY_MODEL_fb4870326d1545dcaab261730965bc99"
            ]
          }
        },
        "6cad7320919248cab53cba7a3eb47225": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "18287140c5324a7e85b01460e5e889f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_11cd33748ed9445c921d533c922a2128",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 496313727,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 496313727,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4ae80b3c967142b1b773b080ed310264"
          }
        },
        "fb4870326d1545dcaab261730965bc99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_31bf737378c04156a4ced60a3b781572",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 496M/496M [00:07&lt;00:00, 62.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8344b11a1c42484883bb312ebfa677c6"
          }
        },
        "11cd33748ed9445c921d533c922a2128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4ae80b3c967142b1b773b080ed310264": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "31bf737378c04156a4ced60a3b781572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8344b11a1c42484883bb312ebfa677c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amine-OMRI/tweet-sentiment-extraction-kaggle-compete-1st-place-detailed-solution/blob/main/roberta_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOeAFVsEAUTq"
      },
      "source": [
        "La solution a été implémentée en utilisant une version antérieure des bibliothèques \n",
        "\n",
        "1.   tokenizers==0.7.0\n",
        "2.   transformers==2.9.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mk0dtujy2tO"
      },
      "source": [
        "!pip install tokenizers==0.7.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N29Szb61GEts"
      },
      "source": [
        "!pip install transformers==2.9.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfeg6bkPHNbj"
      },
      "source": [
        "!pip install torchcontrib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7Cxkuo8DM_r"
      },
      "source": [
        "[torchcontrib](https://github.com/pytorch/contrib) librairie contient des implémentations d'idées issues de récents articles sur l'apprentissage machine, ici elle a été utilisée pour réaliser #Stochastic Weight Averaging (SWA) Que je vous présenteai ultérieurement dans le Notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVg2eB1FOZqE",
        "outputId": "22c9ba39-d7bf-4264-a2e6-a2f2cde79b43"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmUw-vAr6c_6"
      },
      "source": [
        "#Pour Commencer\n",
        "**Ce notebook représente l'un des modèles de premier niveau réalisé par [Heartkilla](https://www.kaggle.com/aruchomu) : Artsem Zhyvalkouski le code source est sous `src/1st_level/roberta_base/` sur le [repo git](https://github.com/heartkilla/kaggle_tweet/tree/master/src/1st_level/roberta_base) où ils ont mis le code de leur solution**\n",
        "\n",
        "---\n",
        "Pour faire tourner la solution de la compétition, j'ai chargé quelques fichiers sur mon Gdrive qui sont : \n",
        "\n",
        "\n",
        "*   Les fichiers de données :</br>\n",
        "    TRAINING_FILE = /content/drive/MyDrive/very_final/data/**train_folds.csv**</br>\n",
        "    TEST_FILE = /content/drive/MyDrive/very_final/data/**test.csv**</br>\n",
        " \n",
        "\n",
        "*   Les fichiers de configuration du tokeniser :</br>\n",
        "\n",
        "    /content/drive/MyDrive/very_final/roberta_tokenizer/**vocab.json**</br>\n",
        "    /content/drive/MyDrive/very_final/roberta_tokenizer/**merges.txt**</br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyIIjztjWy5O"
      },
      "source": [
        "J'ai également transféré les fichiers de **code** directement sur **Colab** en tant que fichiers d'entrée afin de pouvoir les importer et les utiliser, en même temps que j'ai copié leur code afin de pouvoir le commenter et remplir l'objectif de travail demandé.\n",
        "* config.py\n",
        "* dataset.py\n",
        "* engine.py\n",
        "* evaluate.py\n",
        "* infer.py\n",
        "* models.py\n",
        "* train.py\n",
        "* utils.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jro8AaTYD78D"
      },
      "source": [
        "#config.py\n",
        "\n",
        "---\n",
        "\n",
        "Le fichier **config.py** (configuration) contient les paramètres et les réglages initiaux des modules et des méthodes de la solution. L'importation du fichier config.py permet d'utiliser les variables et les fonctions du fichier config.py dans les autres fichiers de la solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljAHrlvLDKkS"
      },
      "source": [
        "import tokenizers\n",
        "\n",
        "# Paths\n",
        "\n",
        "TOKENIZER_PATH = '/content/drive/MyDrive/very_final/roberta_tokenizer'\n",
        "TRAINING_FILE = '/content/drive/MyDrive/very_final/data/train_folds.csv'\n",
        "TEST_FILE = '/content/drive/MyDrive/very_final/data/test.csv'\n",
        "SUB_FILE = '/content/drive/MyDrive/very_final/data/sample_submission.csv'\n",
        "MODEL_SAVE_PATH = '/content/drive/MyDrive/very_final/roberta_base/model_save'\n",
        "TRAINED_MODEL_PATH = '/content/drive/MyDrive/very_final/roberta_base/model_save'\n",
        "\n",
        "# Model config\n",
        "##    Le tout premier modèle qui a été utilisé dans les modèles de 1er niveau\n",
        "##    est roberta-base pour le modèle d'AQ par deepset.ai (Squad pretrained \n",
        "##    weights)) pré-entrainé sur la base SQuAD 2.0\n",
        "MODEL_CONFIG = 'deepset/roberta-base-squad2'\n",
        "#---------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Model params\n",
        "# Global Seed to initialize the pseudo-random number generator\n",
        "# Pour assurer d'avoir les memes resultats d'une lancée a une autre\n",
        "SEED = 25\n",
        "# Nombre des folds pour l'entrainement par fold\n",
        "N_FOLDS = 5\n",
        "# Nombre d'EPOCHS de lentrainment des modéles \n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 4e-5\n",
        "PATIENCE = None\n",
        "EARLY_STOPPING_DELTA = None\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = 32\n",
        "MAX_LEN = 96  # actually = 86\n",
        "\n",
        "## Expliqué ci-dessous\n",
        "TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab_file=f'{TOKENIZER_PATH}/vocab.json',\n",
        "    merges_file=f'{TOKENIZER_PATH}/merges.txt',\n",
        "    lowercase=True,\n",
        "    add_prefix_space=True)\n",
        "# 768 est la dimension des Embeddings \n",
        "HIDDEN_SIZE = 768\n",
        "N_LAST_HIDDEN = 12\n",
        "HIGH_DROPOUT = 0.5\n",
        "SOFT_ALPHA = 0.6\n",
        "WARMUP_RATIO = 0.25\n",
        "WEIGHT_DECAY = 0.001\n",
        "#Stochastic Weight Averaging (SWA) :\n",
        "# les paramétres de l'optimiser \n",
        "USE_SWA = False\n",
        "SWA_RATIO = 0.9\n",
        "SWA_FREQ = 30"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R42twUwALMX7"
      },
      "source": [
        "Source : **huggingface tokenizers**\n",
        "Fournit une implémentation des tokenizers les plus utilisés aujourd'hui, en mettant l'accent sur les performances et la polyvalence.\n",
        "Utilisé pour :\n",
        "* Entraîner de nouveaux vocabulaires et tokeniser à l'aide de 4 tokenizers pre-made (Bert WordPiece et les 3 versions les plus courantes de BPE).\n",
        "* Extrêmement rapide (tant pour l'entraînement que pour la tokenisation), grâce à l'implémentation de Rust. Il le faut moins de 20 secondes pour convertir un Go de texte en tokens sur le processeur d'un serveur.\n",
        "* Facile à utiliser, mais aussi extrêmement polyvalent.\n",
        "* Conçu pour la recherche et la production.\n",
        "* La normalisation s'accompagne d'un suivi des alignements. Il est toujours possible d'obtenir la partie de la phrase originale qui correspond à un jeton donné.\n",
        "* Effectue tout le prétraitement : Tronquer, Pad, ajouter les tokens spéciaux dont votre modèle a besoin.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lulkLxMkWKEh"
      },
      "source": [
        "\n",
        "Le tokenizer de RoBERTa est basé sur le tokenizer GPT-2, les fichiers vocab/merges sont constitués lors de l'entrainement du BBPE [(Byte-level Byte-Pair-Encoding)](https://arxiv.org/pdf/1909.03341.pdf) et utilisés pour encoder les sentences, le tokenizer tokenize d'abord en se basant sur le fichier **merges.txt**.</br>\n",
        "\n",
        "Voici un exemple :</br>\n",
        "\n",
        "```\n",
        "['What', \"'s\", 'Ġup', 'Ġwith', 'Ġthe', 'Ġtoken', 'izer', '?']</br>\n",
        "\n",
        "```\n",
        "le caractère ```Ġ``` signifie un espace</br>\n",
        "\n",
        "\n",
        "Et ensuite, selon les valeurs dans le fichier **vocab.json**, ces tokens sont alors remplacés par leurs indices :</br>\n",
        "```\n",
        "[   'What',     \"'s\",    'Ġup',  'Ġwith',   'Ġthe', 'Ġtoken',   'izer',      '?']\n",
        "---- becomes ----\n",
        "[     2061,      338,      510,      351,      262,    11241,     7509,       30]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtM5UFGcEAxb"
      },
      "source": [
        "#dataset.py\n",
        "\n",
        "---\n",
        "le fichier dataset.py pour assurer que toutes les données des tweets soient stockées au même endroit et soient utilisées pour charger les données avec le **dataloader** par la suite.\n",
        "\n",
        "* Il contient également une implémentation d'un  Map-style datasets (the **TweetDataset** class)  qui implémente les méthodes **__getitem__()** et **__len__()** et qui représente toutes les propriétés des ids des tweets, les offsets, orig_start/orig_end, start_labels/end_labels, mask, token_type_ids, ...\n",
        "\n",
        "* Egalement une implémentation de la méthode **process_data** qui traitera qui calculera toutes ces propriétés et les extraira des tweets, selected_text, sentiment en utilisant le tokeniser crée dans le fichier config.py.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocl0o4iYUoaa"
      },
      "source": [
        "Voyons voir a quoi ressemblent les données d'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "AAihgxQaUp0e",
        "outputId": "eb61e663-da7a-406a-909f-5c94fb0c916e"
      },
      "source": [
        "import config\n",
        "import pandas as pd\n",
        "df_train = pd.read_csv(config.TRAINING_FILE)\n",
        "df_train.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>kfold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>d0c214ad3a</td>\n",
              "      <td>good mornig to everone... it`s a great morning...</td>\n",
              "      <td>good mornig to everone... it`s a great morning...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7d093817af</td>\n",
              "      <td>LOL. You know me. I aim to please.</td>\n",
              "      <td>I aim to please.</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21eacf7e58</td>\n",
              "      <td>Was at Ruby Skye last night as well! Superb s...</td>\n",
              "      <td>Superb set by Steve.</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>d0f94d66ab</td>\n",
              "      <td>does not like ups much today...</td>\n",
              "      <td>does not like</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a025e21634</td>\n",
              "      <td>Nothing like In `n` Out and a LOST marathon af...</td>\n",
              "      <td>long day of work.</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID  ... kfold\n",
              "0  d0c214ad3a  ...     0\n",
              "1  7d093817af  ...     0\n",
              "2  21eacf7e58  ...     0\n",
              "3  d0f94d66ab  ...     0\n",
              "4  a025e21634  ...     0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvUeziUUXrR6"
      },
      "source": [
        "**===>  `selected_text` est la valeur à prédire**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3zseDwveZ9Z",
        "outputId": "84298617-2a5f-48d1-a42b-6ce7f9377ec8"
      },
      "source": [
        "df_train.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27480, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aArwkeaidCEs",
        "outputId": "4f13af67-6240-40b5-b497-ae06af659663"
      },
      "source": [
        "tweet = df_train.text.values[3]\n",
        "selected_text = df_train.selected_text[3]\n",
        "tweet = ' ' + ' '.join(str(tweet).split())\n",
        "selected_text = ' ' + ' '.join(str(selected_text).split())\n",
        "tweet"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' does not like ups much today...'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lurbGoDRlWTg",
        "outputId": "7f6bb480-2097-48ca-8fdc-c1fe83bf9407"
      },
      "source": [
        "selected_text "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' does not like'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhAPoBb1eJ4_",
        "outputId": "cbb34778-1d64-4845-8e36-979da150c475"
      },
      "source": [
        "len_sel_text = len(selected_text) - 1\n",
        "## récpèrerl'idice de début et de fin de selected_text\n",
        "idx_0 = None\n",
        "idx_1 = None\n",
        "## i : indice du caractére, e c'est le caractére SI e est le caractére \n",
        "## d'indice 1 (just apré lespace quona rajouté)\n",
        "for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
        "  if ' ' + tweet[ind:ind + len_sel_text] == selected_text:\n",
        "    idx_0 = ind\n",
        "    idx_1 = ind + len_sel_text - 1\n",
        "    break\n",
        "print('l\\'indexe de début:',idx_0,'l\\'indexe de fin:', idx_1, 'du text séléctionné qui représente le target')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "l'indexe de début: 1 l'indexe de fin: 13 du text séléctionné qui représente le target\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo9w6FBTzzGV"
      },
      "source": [
        "Assigner `1` à chaque caractère du tweet s'il fait partie du selected_text sinon `0`, comme dans l'exemple ci-dessus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8TAUopaxQwT",
        "outputId": "f0dabed7-cb5a-4513-ce8a-ee8bbfb80f30"
      },
      "source": [
        "# Assign 1 as target for each char in sel_text\n",
        "char_targets = [0] * len(tweet)\n",
        "if idx_0 is not None and idx_1 is not None:\n",
        "  for ct in range(idx_0, idx_1 + 1):\n",
        "    char_targets[ct] = 1\n",
        "char_targets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HSAaSoETzWlq",
        "outputId": "ebd72489-f65f-4093-8fdf-0ec31ea1af15"
      },
      "source": [
        "tweet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' does not like ups much today...'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKrcw3qNzELF",
        "outputId": "e10a9096-b7ab-403d-e2d1-cca7754d4b9c"
      },
      "source": [
        "tokenized_tweet = TOKENIZER.encode(tweet)\n",
        "input_ids_original = tokenized_tweet.ids\n",
        "input_ids_original"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[473, 45, 101, 12744, 203, 452, 734]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKtJb3Hk0t4F"
      },
      "source": [
        "Une **nouvelle méthode** pour les tokenizers : **tokenize_with_offsets**. En plus de renvoyer les tokens, elle renvoie les intervalles dans le texte original auxquels les tokens correspondent, cette méthode nous permet de récupérer le token ou le mot qui correspond a un **id** donné."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIrwQTaSztnP",
        "outputId": "7a9b3be8-2445-46a2-d911-98a18ebd4b8f"
      },
      "source": [
        "tweet_offsets = tokenized_tweet.offsets\n",
        "tweet_offsets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 5), (5, 9), (9, 14), (14, 18), (18, 23), (23, 29), (29, 32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7nI3hgd0PAB"
      },
      "source": [
        "Récupérer le texte (les tokens) original en se basant sur les offsets (leur intervalle) : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ar3yFd101EBi",
        "outputId": "c73b8326-e154-43d3-d3c3-b3da9fa0e523"
      },
      "source": [
        "tweet[tweet_offsets[0][0]:tweet_offsets[0][1]] + tweet[tweet_offsets[1][0]:tweet_offsets[1][1]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' does not'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjfwO6vQ45Pj"
      },
      "source": [
        "Ce code récupère les **target_ids** qui sont les id des tokens cibles qui représentent le texte sélectionné (chaque caractère est représenté (codé) par le chiffre `\"1\"`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUOFWQ1k1GWR",
        "outputId": "b8f8ad02-1c95-4063-fdef-cd20e2c392e0"
      },
      "source": [
        "target_ids = []\n",
        "for i, (offset_0, offset_1) in enumerate(tweet_offsets):\n",
        "  print(i,'offset (',offset_0,',', offset_1,')')\n",
        "  print(char_targets[offset_0:offset_1])\n",
        "  print(sum(char_targets[offset_0:offset_1]))\n",
        "  if sum(char_targets[offset_0:offset_1]) > 0:\n",
        "    target_ids.append(i)\n",
        "print('target_ids : ',target_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 offset ( 0 , 5 )\n",
            "[0, 1, 1, 1, 1]\n",
            "4\n",
            "1 offset ( 5 , 9 )\n",
            "[1, 1, 1, 1]\n",
            "4\n",
            "2 offset ( 9 , 14 )\n",
            "[1, 1, 1, 1, 1]\n",
            "5\n",
            "3 offset ( 14 , 18 )\n",
            "[0, 0, 0, 0]\n",
            "0\n",
            "4 offset ( 18 , 23 )\n",
            "[0, 0, 0, 0, 0]\n",
            "0\n",
            "5 offset ( 23 , 29 )\n",
            "[0, 0, 0, 0, 0, 0]\n",
            "0\n",
            "6 offset ( 29 , 32 )\n",
            "[0, 0, 0]\n",
            "0\n",
            "target_ids :  [0, 1, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmpceIfOAIwz"
      },
      "source": [
        "* **Soft Jaccard labels :**</br>\n",
        "Custom loss Jaccard-based Soft Labels: Étant donné que la Cross Entropy n'optimise pas directement l'indice de Jaccard, Heartkilla a essayé différentes fonctions de Loss pour pénaliser davantage les prédictions lointaines que les prédictions proches, il a donc trouvé une Loss personnalisée en calculant l'indice de Jaccard au niveau du token. Il a ensuite utilisé ces nouveaux labels cibles et a optimisé la divergence.</br> \n",
        "Alpha c'est un paramètre permettant d'équilibrer l'étiquetage habituel basé sur la Cross Entropy et l'indice de Jaccard </br>\n",
        "<img src = 'https://camo.githubusercontent.com/3925753ce615ec71960dad457401aedefc7b611a2b11a3cb86eb060772dce880/68747470733a2f2f7777772e676f6f676c65617069732e636f6d2f646f776e6c6f61642f73746f726167652f76312f622f6b6167676c652d757365722d636f6e74656e742f6f2f696e626f7825324632303030353435253246393334316265646532383236336263663065396262323539616337393033333825324653637265656e25323053686f74253230323032302d30352d3330253230617425323031372e33312e32322e706e673f67656e65726174696f6e3d3135393234303530323835353638343226616c743d6d65646961'></br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5aOqwjyBi6m"
      },
      "source": [
        "## La mesure d'évaluation qui a été mentionnée sur le présent de la competition \n",
        "def jaccard_array(a, b):answer\n",
        "    \"\"\"Calculates Jaccard on arrays.\"\"\"\n",
        "    a = set(a)\n",
        "    b = set(b)\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCHfYQVO51D_"
      },
      "source": [
        "import numpy as np\n",
        "targets_start = target_ids[0]\n",
        "targets_end = target_ids[-1]\n",
        "n = len(input_ids_original)\n",
        "## id des tokens dans le tweet\n",
        "sentence = np.arange(n)\n",
        "## id  des tokens qui forment le label (le selected_text)\n",
        "answer = sentence[targets_start:targets_end + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR-6ZPNr6FmR",
        "outputId": "8b115a18-fc34-40b7-86d4-d2f58d423895"
      },
      "source": [
        "sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ecJSNNGiewI"
      },
      "source": [
        "Le tableaux retourné a ce niveau représente les indices du target (text séléctionné) dans la sentence orginal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBehZdAO6I1s",
        "outputId": "4e45f7d4-3fdf-4955-df4f-ffa42550fe78"
      },
      "source": [
        "answer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDUUnvWKjKQb"
      },
      "source": [
        "C'est bien la target selected_text: '` does not like` '"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alRiyywnGFXw",
        "outputId": "1367f68c-2af8-4b85-8922-b419c5e21b33"
      },
      "source": [
        "sentence[targets_start:targets_end + 1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QKyPhqaB-sl",
        "outputId": "39ae3138-76a2-4e75-90b4-9cd97bc12b27"
      },
      "source": [
        "for i in range(targets_end + 1):\n",
        "  ## calculate the jaccard indexe\n",
        "  ## answer = array([0, 1, 2]) qui est les indice du atrget selected_text: 'does not like'\n",
        "  jac = jaccard_array(answer, sentence[i:targets_end + 1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jaccard indexe du token (0,2) =1.0\n",
            "jaccard indexe du token (1,2) =0.6666666666666666\n",
            "jaccard indexe du token (2,2) =0.3333333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijAn528-kDk6"
      },
      "source": [
        "jaccard indexe du token (0,2) =1.0 puisque le texte correspondant à l'index de **(0)** jusqu'à la target_end **(2)** dans la phrase originale (`' does not like ups much today...'`) est exactement le taget qui est `'does not like'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtoxHWZvBmY9",
        "outputId": "3b7cc9a6-1ee9-4aff-db1a-3a939984c872"
      },
      "source": [
        "start_labels = np.zeros(n)\n",
        "for i in range(targets_end + 1):\n",
        "    ## calculate the jaccard indexe \n",
        "    jac = jaccard_array(answer, sentence[i:targets_end + 1])\n",
        "    print('jaccard indexe du token ('+ str(i) +','+ str(targets_end)+') ='+ str(jac))\n",
        "    start_labels[i] = jac + jac ** 2\n",
        " \n",
        "## Alpha est un paramètre d'équilibre entre la CE et le Jaccard-based labeling\n",
        "start_labels = (1 - config.SOFT_ALPHA) * start_labels / start_labels.sum()\n",
        "start_labels[targets_start] += config.SOFT_ALPHA\n",
        "start_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jaccard indexe du token (0,2) =1.0\n",
            "jaccard indexe du token (1,2) =0.6666666666666666\n",
            "jaccard indexe du token (2,2) =0.3333333333333333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.825, 0.125, 0.05 , 0.   , 0.   , 0.   , 0.   ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRAPl9oODntS"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import config\n",
        "\n",
        "## La mesure d'évaluation qui a été mentionnée sur le présent de la competition \n",
        "def jaccard_array(a, b):\n",
        "    \"\"\"Calculates Jaccard on arrays.\"\"\"\n",
        "    a = set(a)\n",
        "    b = set(b)\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "\n",
        "def process_data(tweet, selected_text, sentiment,\n",
        "                 tokenizer, max_len):\n",
        "    \"\"\"Preprocesses one data sample and returns a dict\n",
        "    with targets and other useful info.\n",
        "    \"\"\"\n",
        "    ## Pour un tweet donné:\n",
        "    ## récpèrer le text des tweet sous forme d'une str séparée par espace (commence par ' ' déja)\n",
        "    tweet = ' ' + ' '.join(str(tweet).split())\n",
        "    ## récpèrer le text des selected_text\n",
        "    selected_text = ' ' + ' '.join(str(selected_text).split())\n",
        "\n",
        "    ## récpèrer le len de selected_text (-1 puisque python commence à partir de 0)\n",
        "    len_sel_text = len(selected_text) - 1\n",
        "\n",
        "    ## récpèrer l'idice de début et de fin de selected_text\n",
        "    idx_0 = None\n",
        "    idx_1 = None\n",
        "    ## i : indice du caractére, e c'est le caractére SI e est le caractére \n",
        "    ## d'indice 1 (just apré lespace qu'ona rajouté au début de selected_text)\n",
        "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
        "        ## récupérer l'idx de début et fin du selected_text dans le text du tweet\n",
        "        if ' ' + tweet[ind:ind + len_sel_text] == selected_text:\n",
        "            idx_0 = ind\n",
        "            idx_1 = ind + len_sel_text - 1\n",
        "            break\n",
        "\n",
        "    ## Assignez 1 à chaque caractère du tweet s'il fait partie du selected_text\n",
        "    ## sinon 0, comme dans l'exemple ci-dessus.    \n",
        "    char_targets = [0] * len(tweet)\n",
        "    if idx_0 is not None and idx_1 is not None:\n",
        "        for ct in range(idx_0, idx_1 + 1):\n",
        "            char_targets[ct] = 1\n",
        "\n",
        "    ## tokeniser le texte du tweet en utilisant le tokeniser que nous avons \n",
        "    ## créé sur le fichier config.py\n",
        "    tokenized_tweet = tokenizer.encode(tweet)\n",
        "    ## récupérer les indices affectés par le tokeniser à chaque jeton\n",
        "    input_ids_original = tokenized_tweet.ids\n",
        "    ## Cette methode \".offsets\" permet de récupérer les intervalles dans le\n",
        "    ## texte original auxquels les tokens correspondent.\n",
        "    tweet_offsets = tokenized_tweet.offsets\n",
        "\n",
        "    ## Ce code récupère les target_ids qui sont les id des tokens cibles qui\n",
        "    ## représentent le texte sélectionné (chaque caractère est représenté par 1)\n",
        "    target_ids = []\n",
        "    for i, (offset_0, offset_1) in enumerate(tweet_offsets):\n",
        "        if sum(char_targets[offset_0:offset_1]) > 0:\n",
        "            target_ids.append(i)\n",
        "    ## idx début du text du target\n",
        "    targets_start = target_ids[0]\n",
        "    ## idx fin du text du target\n",
        "    targets_end = target_ids[-1]\n",
        "\n",
        "    # Sentimadd_prefix_spaceent 'word' id in vocab\n",
        "    ## Encoder le feature de sentiment\n",
        "    sentiment_id = {'positive': 1313,\n",
        "                    'negative': 2430,\n",
        "                    'neutral': 7974}\n",
        "\n",
        "    # Soft Jaccard labels\n",
        "    #C'est la méthode d'étiquetage personnalisée qui a été adoptée par les compétiteurs. \n",
        "    # ----------------------------------\n",
        "    n = len(input_ids_original)\n",
        "    sentence = np.arange(n)\n",
        "    answer = sentence[targets_start:targets_end + 1]\n",
        "    start_labels = np.zeros(n)\n",
        "    for i in range(targets_end + 1):\n",
        "        jac = jaccard_array(answer, sentence[i:targets_end + 1])\n",
        "        start_labels[i] = jac + jac ** 2\n",
        "        \n",
        "    ## Alpha est un paramètre d'équilibre entre l'étiquetage (labeling) Cross Enthropy \n",
        "    ## habituel et l'étiquetage basé sur la carte Jaccard (Jaccard-based labeling).\n",
        "    start_labels = (1 - config.SOFT_ALPHA) * start_labels / start_labels.sum()\n",
        "    start_labels[targets_start] += config.SOFT_ALPHA\n",
        "\n",
        "    end_labels = np.zeros(n)\n",
        "    for i in range(targets_start, n):\n",
        "        jac = jaccard_array(answer, sentence[targets_start:i + 1])\n",
        "        end_labels[i] = jac + jac ** 2\n",
        "    end_labels = (1 - config.SOFT_ALPHA) * end_labels / end_labels.sum()\n",
        "    end_labels[targets_end] += config.SOFT_ALPHA\n",
        "    ## Les nouveaux labels qui seront utilisés pour améliorer et garantir que le modèle apprendra correctement\n",
        "    start_labels = [0, 0, 0, 0] + list(start_labels) + [0]\n",
        "    end_labels = [0, 0, 0, 0] + list(end_labels) + [0]\n",
        "    # ----------------------------------\n",
        "\n",
        "    ## l'input pour RoBERTa\n",
        "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + \\\n",
        "                [2] + input_ids_original + [2]\n",
        "    ## Pas de types de token dans RoBERTa (tous a 0)\n",
        "    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_original) + 1)\n",
        "    ## Mask de l'input sans padding\n",
        "    mask = [1] * len(token_type_ids)\n",
        "    ## Identifiants des caractères de début et de fin pour chaque mot, y compris les nouveaux tokens\n",
        "    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n",
        "    ## Ids des mots dans le tweet qui ont un caractère cible, y compris les nouveaux tokens\n",
        "    targets_start += 4\n",
        "    targets_end += 4\n",
        "    orig_start = 4\n",
        "    orig_end = len(input_ids_original) + 3\n",
        "\n",
        "    ## Avant que RoBERTa puisse traiter ces données en entrée, nous devrons rendre \n",
        "    ## tous les vecteurs de même taille en ajoutant (padding ) des phrases plus courtes avec le token id 0. \n",
        "    ## Après le padding, nous avons une matrice / un tenseur \n",
        "    ## qui est prêt à être passé à RoBERTa.\n",
        "    ## Input padding: new mask, token type ids, tweet offsets\n",
        "    ## s'il y'en à du padding \n",
        "    padding_len = max_len - len(input_ids)\n",
        "    if padding_len > 0:\n",
        "      ## on récupère les input_ids, mask, token_type_ids, tweet_offsets, end_offsets\n",
        "      input_ids = input_ids + ([1] * padding_len)\n",
        "      mask = mask + ([0] * padding_len)\n",
        "      token_type_ids = token_type_ids + ([0] * padding_len)\n",
        "      tweet_offsets = tweet_offsets + ([(0, 0)] * padding_len)\n",
        "      start_labels = start_labels + ([0] * padding_len)\n",
        "      end_labels = end_labels + ([0] * padding_len)\n",
        "    ## Compute le targets_select\n",
        "    targets_select = [0] * len(token_type_ids)\n",
        "    for i in range(len(targets_select)):\n",
        "        if i in target_ids:\n",
        "            targets_select[i + 4] = 1\n",
        "\n",
        "    ## la sortie pour un tweet donné\n",
        "    return {'ids': input_ids,\n",
        "            'mask': mask,\n",
        "            'token_type_ids': token_type_ids,\n",
        "            'start_labels': start_labels,\n",
        "            'end_labels': end_labels,\n",
        "            'orig_start': orig_start,\n",
        "            'orig_end': orig_end,\n",
        "            'orig_tweet': tweet,\n",
        "            'orig_selected': selected_text,\n",
        "            'sentiment': sentiment,\n",
        "            'offsets': tweet_offsets,\n",
        "            'targets_select': targets_select}\n",
        "\n",
        "\n",
        "## Une classe de pratique pour que toutes les données des tweets soient stockées \n",
        "## au même endroit et qui sera utilisé pour charger les donéées avec dataloader aprés\n",
        "\n",
        "## Map-style datasets\n",
        "## A map-style dataset is one that implements the __getitem__() and __len__() \n",
        "## protocols, and represents a map from (possibly non-integral) indices/keys to data samples.\n",
        "class TweetDataset:\n",
        "    '''\n",
        "    définir un objet(classe) qui contient toutes les données des tweets et implémenter \n",
        "    la méthode (pre-buil) _len_  qui retourne le nombre de tweets et la méthode \n",
        "    __getitem__ qui traite les données des tweets et calcule toutes les  tenseur \n",
        "    (torch.tensor) qui sera alimenté par le modèle : 'ids', 'mask', 'token_type_ids', \n",
        "    'start_labels', 'end_labels', 'orig_start', 'orig_end', 'orig_tweet', \n",
        "    'orig_selected', 'sentiment', 'offsets', 'targets_select'.\n",
        "    '''\n",
        "    def __init__(self, tweets, sentiments, selected_texts):\n",
        "        self.tweets = tweets\n",
        "        self.sentiments = sentiments\n",
        "        self.selected_texts = selected_texts\n",
        "        self.max_len = config.MAX_LEN\n",
        "        self.tokenizer = config.TOKENIZER\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tweets)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        \"\"\"Returns preprocessed data sample as dict with\n",
        "        data converted to tensors.\n",
        "        \"\"\"\n",
        "        data = process_data(self.tweets[item],\n",
        "                            self.selected_texts[item],\n",
        "                            self.sentiments[item],\n",
        "                            self.tokenizer,\n",
        "                            self.max_len)\n",
        "\n",
        "        return {'ids': torch.tensor(data['ids'], dtype=torch.long),\n",
        "                'mask': torch.tensor(data['mask'], dtype=torch.long),\n",
        "                'token_type_ids': torch.tensor(data['token_type_ids'],\n",
        "                                               dtype=torch.long),\n",
        "                'start_labels': torch.tensor(data['start_labels'],\n",
        "                                             dtype=torch.float),\n",
        "                'end_labels': torch.tensor(data['end_labels'],\n",
        "                                           dtype=torch.float),\n",
        "                'orig_start': data['orig_start'],\n",
        "                'orig_end': data['orig_end'],\n",
        "                'orig_tweet': data['orig_tweet'],\n",
        "                'orig_selected': data['orig_selected'],\n",
        "                'sentiment': data['sentiment'],\n",
        "                'offsets': torch.tensor(data['offsets'], dtype=torch.long),\n",
        "                'targets_select': torch.tensor(data['targets_select'],\n",
        "                                               dtype=torch.float)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OkiOiOzHEql"
      },
      "source": [
        "deux techniques ont été utilisées afin d'assurer la même longueur des séquences d'entrée:\n",
        "* **Le masquage (Masking):** est un moyen d'indiquer aux couches de traitement des séquences que certains blocs sont manquants dans une entrée et qu'ils doivent donc être ignorés lors du traitement des données.\n",
        "\n",
        "* **Le padding** est une forme spéciale de masquage où les étapes masquées se trouvent au au début d'une séquence. Le remplissage (padding) vient de la nécessité de coder les données de la séquence en lots consécutifs (contiguous batches): afin que toutes les séquences d'un batch soient conformes à une longueur standard donnée, il est nécessaire de remplir ou de tronquer certaines séquences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_Q6Y5KPFUGh"
      },
      "source": [
        "# engine.py\n",
        "\n",
        "---\n",
        "Le fichier engine.py contient une definition de la fonction loss ainsi que l'implementation des deux fonction:</br>\n",
        "* train : qui permet de mettre le model en mode entrainment\n",
        "* eval: qui permet de mettre le model en mode evaluation \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2UMFZtzFUgB"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import tqdm\n",
        "\n",
        "import utils\n",
        "\n",
        "## Definition de la fonction loss\n",
        "def loss_fn(start_logits, end_logits,\n",
        "            start_positions, end_positions):\n",
        "    ## Appliquer la fonction \\log(\\text{Softmax}(x))log(Softmax(x)) à une entrée \n",
        "    ## n-dimensionnelle Tenseur ici dim = 1\n",
        "    m = torch.nn.LogSoftmax(dim=1)\n",
        "    ## La mesure de la divergence de Kullback-Leibler est une mesure de distance\n",
        "    ## utile pour les distributions continues  \n",
        "    loss_fct = torch.nn.(KLDivLoss)\n",
        "    ## calculer la loss par apport a la prédiction du caractére de début du target(selected_text)\n",
        "    start_loss = loss_fct(m(start_logits), start_positions)\n",
        "    ## calculer la loss par apport a la prédiction du caractére de fin du target\n",
        "    end_loss = loss_fct(m(end_logits), end_positions)\n",
        "    ## La valeur de loss totale et la somme des deux loss par apprt a la prédiction \n",
        "    ## caractére de debut et fin du target\n",
        "    total_loss = (start_loss + end_loss)\n",
        "    return total_loss\n",
        "\n",
        "## définition de la fonction train\n",
        "def train_fn(data_loader, model, optimizer, device, scheduler=None):\n",
        "    ## model.train() permet de mettre le modèle en mode train (il calcule les gradients)\n",
        "    model.train()\n",
        "    ## Permet de stocke la valeur moyenne actuelle et applique deux méthodes :\n",
        "    ## reset : qui remet toutes les valeurs à zéro \n",
        "    ## update : qui met à jour l'objet en y ajoutant de nouvelles valeurs, ici il s'agit de la valeur de la loss\n",
        "    losses = utils.AverageMeter()\n",
        "   \n",
        "    ## tqdm nous permettre de créer une progressbar en fonction de la longueur des données\n",
        "    tk0 = tqdm.tqdm(data_loader, total=len(data_loader))\n",
        "\n",
        "    for bi, d in enumerate(tk0):\n",
        "        ## recupérer l'id du tweet\n",
        "        ids = d['ids']\n",
        "        ## récupérer les ids des tokens\n",
        "        token_type_ids = d['token_type_ids']\n",
        "        ## récupérer le mask du tweet\n",
        "        mask = d['mask']\n",
        "        ## la valeur du start/end lable calculer en utilisant Jaccard-based labeling\n",
        "        start_labels = d['start_labels']\n",
        "        end_labels = d['end_labels']\n",
        "\n",
        "        ## nn.Module.to function permet de déplacer le modèle\\tensors vers le GPU\n",
        "        ids = ids.to(device, dtype=torch.long)\n",
        "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        mask = mask.to(device, dtype=torch.long)\n",
        "        start_labels = start_labels.to(device, dtype=torch.float)\n",
        "        end_labels = end_labels.to(device, dtype=torch.float)\n",
        "\n",
        "        ## mettre les gradients à zéro avant de commencer à faire de la backpropragation  \n",
        "        model.zero_grad()\n",
        "\n",
        "        ## applique un forward pass et récuperer l'output\n",
        "        outputs_start, outputs_end = \\\n",
        "            model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
        "        ## Calculer la valeur de la loss\n",
        "        loss = loss_fn(outputs_start, outputs_end,\n",
        "                       start_labels, end_labels)\n",
        "        ## Calculer les gradiants\n",
        "        loss.backward()\n",
        "        ## Ajuster les poids de notre modele\n",
        "        optimizer.step()\n",
        "        ## un programmateur de taux d'apprentissage basé sur le temps\n",
        "        ## - il est contrôlé par le paramètre de décroissance(decay) de l'optimiser\n",
        "        scheduler.step()\n",
        "        ## mettre a jour la valeur sauvegardé de la loss \n",
        "        losses.update(loss.item(), ids.size(0))\n",
        "        tk0.set_postfix(loss=losses.avg)\n",
        "\n",
        "## définition de la fonction de l'évaluation\n",
        "def eval_fn(data_loader, model, device):\n",
        "    ## model.eval() met le modèle en mode évaluation (il calcule pas les gradients)\n",
        "    model.eval()\n",
        "    ## récupérer la valeur de la loss\n",
        "    losses = utils.AverageMeter()\n",
        "    ## récupérer la valeur de la métric de jaccards\n",
        "    jaccards = utils.AverageMeter()\n",
        "    ## Le wrapper \"with torch.no_grad()\" met temporairement tous les tensors avec require_grad à false\n",
        "    with torch.no_grad():\n",
        "        ## passer les donnnée d'évaluation avec une progressbar\n",
        "        tk0 = tqdm.tqdm(data_loader, total=len(data_loader))\n",
        "        for bi, d in enumerate(tk0):\n",
        "            ids = d['ids']\n",
        "            token_type_ids = d['token_type_ids']\n",
        "            mask = d['mask']\n",
        "            start_labels = d['start_labels']\n",
        "            end_labels = d['end_labels']\n",
        "            orig_start = d['orig_start']\n",
        "            orig_end = d['orig_end']\n",
        "            orig_selected = d['orig_selected']\n",
        "            orig_tweet = d['orig_tweet']\n",
        "            offsets = d['offsets']\n",
        "            \n",
        "            ## nn.Module.to function permet de déplacer le modèle\\tensors vers le GPU\n",
        "            ids = ids.to(device, dtype=torch.long)\n",
        "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            start_labels = start_labels.to(device, dtype=torch.float)\n",
        "            end_labels = end_labels.to(device, dtype=torch.float)\n",
        "\n",
        "            outputs_start, outputs_end = \\\n",
        "                model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
        "            loss = loss_fn(outputs_start, outputs_end,\n",
        "                           start_labels, end_labels)\n",
        "            ## récupérer les outputs start/stop prédites\n",
        "            outputs_start = outputs_start.cpu().detach().numpy()\n",
        "            outputs_end = outputs_end.cpu().detach().numpy()\n",
        "            ## lancé le calcul de lindices de jaccard qui permet d'evaluer le modele\n",
        "            jaccard_scores = []\n",
        "            for px, tweet in enumerate(orig_tweet):\n",
        "                ## recupérer la valeur réelle du target\n",
        "                selected_tweet = orig_selected[px]\n",
        "                jaccard_score, _ = \\\n",
        "                    utils.calculate_jaccard(original_tweet=tweet,\n",
        "                                            target_string=selected_tweet,\n",
        "                                            start_logits=outputs_start[px, :],\n",
        "                                            end_logits=outputs_end[px, :],\n",
        "                                            orig_start=orig_start[px],\n",
        "                                            orig_end=orig_end[px],\n",
        "                                            offsets=offsets[px])\n",
        "                jaccard_scores.append(jaccard_score)\n",
        "            ## mettre a jour la valeur de jaccard\n",
        "            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
        "            ## pareil pour la valeur de la loss\n",
        "            losses.update(loss.item(), ids.size(0))\n",
        "            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n",
        "\n",
        "    print(f'Jaccard = {jaccards.avg}')\n",
        "\n",
        "    return jaccards.avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etmylaX2Fb-j"
      },
      "source": [
        "# evaluate.py\n",
        "\n",
        "---\n",
        "La même implémentation de la fonction d'évaluation qui a été implémentée dans le fichier **engin.py**, celle-ci peut être utilisée pour effectuer une évaluation directement en utilisant le fichier **evaluate.py**.**bold text**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oReUS_pFcZN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490,
          "referenced_widgets": [
            "2a5a6a4853c647d5ab4e6f2e421c5b21",
            "ea33407f48d84529a3cedce8ff4eb717",
            "86e84d68c27d472b8a7a0d57a6e22c43",
            "a042d53d0d374fbba87c5126bc89edb1",
            "fb2ad4da78f8468f8b5e0bacdfdb6a44",
            "67b4d3d78dee4cc39c7679217f24bca9",
            "f7174c2b850b4684b80adf245f073187",
            "b40d221cc11e42abacb6d79d87b64e15",
            "da3165f8a7bb4f1ba6002a3dd1839b23",
            "ad49480862ce44e696e8ab8d60f4850c",
            "fd76536cf49f4f9e9dcca7b8c6c9c804",
            "97fb8388273247cea7095fce3ccd4789",
            "2fdf7552d81344c6b23bbd1e82f34b80",
            "6991e98fa56846f98fdf918568cafd89",
            "79c265a4eeda434db25543b711583219",
            "c5c53ebd65b84df79429c42c41a456f0",
            "311e02e4ab79455f96a6a69d882cf44c",
            "8fbea1556436405c82e84a37e15ee0c3",
            "be8768226ed74c338a38576cc1639b61",
            "f195b12fa5be48fca00f7c2056222609",
            "d40ab0eeaee6478aa0eec0bc7a5d883f",
            "2f1b9d41b21047ec94130b11e14337d6",
            "71e0c96ecb90475a9767aa517b1ff34d",
            "83643d9c675e43d0adf21bb7e7bb4bd2",
            "e67e719ed34a4b07a0591d376313ceeb",
            "a19299dbe1cb457fb61a2bd60a044123",
            "e2037ef67ba3407e9883a32f1a9ae9e9",
            "ef8f28882f6a4689ac35c8b7dc69dc8a",
            "80baee64c83f4c05b13026fdd6c3928f",
            "95fe9696a2b74d7ea8c85a71ed0ea401",
            "1dd0410fc9704a9caa064a47ee8936bc",
            "ba08cfa1331d47489b03c860ed273728",
            "c649b48206e8461ab986a31eff41663d",
            "de48ecab2e9a43a2984091f852f12b25",
            "d9ccaad734f8466f9c609ba4c19ae320",
            "79deeeeeaa964a06bc9f3ad5462ca312",
            "1790d4b810e6479fb0986a33c7d6d89d",
            "3e0abbd9b372444a9ecb25a616239688",
            "8388209c0eea421a864e786bf0e88db2",
            "6e28785f8a2c4d26b9dedb98f993c35c",
            "6ae593eea4ea4bf29ccb9207bf39a987",
            "0a8fc24b90b944d1ba355ecbb00bfa58",
            "dc414fdaee294c9db8c677b27a81707f",
            "b53ea0fc5b7e42f6961cc98b3b257128",
            "9f73581c6f304a2381a54c3aded4fb52",
            "ad0874d836e3493cb84f49eb88448e72",
            "86d93fca9e4a417187f9512e505ccf8c",
            "0c571a2eff364548b85ad2d1a1ccc4cd",
            "e34c097bd1a8400cab7a702b06a9143c",
            "1eb203e1db144517ae6bf803be68d76b",
            "c555a66b7f44451e82c325428f234bc4",
            "5ac0aaff737641eca49308824bb94603",
            "1ec1aa1825834a7c96fddcafac8ee72b",
            "ae9af73a41084fc2bbf736f4afd9f5dd",
            "24ea29ea46fe45f5876e16e9bbc5daca",
            "806bf2cfe63e4b909522ad2c6b6f6d8c"
          ]
        },
        "outputId": "98e79f68-c3d7-4978-c812-dcd75770c65d"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import tqdm.autonotebook as tqdm\n",
        "\n",
        "import utils\n",
        "import config\n",
        "import models\n",
        "import dataset\n",
        "\n",
        "## définition de la fonction de l'évaluation\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    ## récupérer la valeur de la métric de jaccards\n",
        "    jaccards = utils.AverageMeter()\n",
        "    ## comme j'avais commenter sur le code ci-dessous\n",
        "    ## Le wrapper \"with torch.no_grad()\" met temporairement tous les tensors avec require_grad à false\n",
        "    with torch.no_grad():\n",
        "        tk0 = tqdm.tqdm(data_loader, total=len(data_loader))\n",
        "        for bi, d in enumerate(tk0):\n",
        "            ids = d['ids']\n",
        "            token_type_ids = d['token_type_ids']\n",
        "            mask = d['mask']\n",
        "            start_labels = d['start_labels']\n",
        "            end_labels = d['end_labels']\n",
        "            orig_start = d['orig_start']\n",
        "            orig_end = d['orig_end']\n",
        "            orig_selected = d['orig_selected']\n",
        "            orig_tweet = d['orig_tweet']\n",
        "            offsets = d['offsets']\n",
        "            ## déplacer le modèle\\tensors vers le GPU/CPU\n",
        "            ids = ids.to(device, dtype=torch.long)\n",
        "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            start_labels = start_labels.to(device, dtype=torch.float)\n",
        "            end_labels = end_labels.to(device, dtype=torch.float)\n",
        "\n",
        "            outputs_start, outputs_end = \\\n",
        "                model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
        "\n",
        "            outputs_start = outputs_start.cpu().detach().numpy()\n",
        "            outputs_end = outputs_end.cpu().detach().numpy()\n",
        "\n",
        "            jaccard_scores = []\n",
        "            for px, tweet in enumerate(orig_tweet):\n",
        "                selected_tweet = orig_selected[px]\n",
        "                jaccard_score, _ = \\\n",
        "                    utils.calculate_jaccard(original_tweet=tweet,\n",
        "                                            target_string=selected_tweet,\n",
        "                                            start_logits=outputs_start[px, :],\n",
        "                                            end_logits=outputs_end[px, :],\n",
        "                                            orig_start=orig_start[px],\n",
        "                                            orig_end=orig_end[px],\n",
        "                                            offsets=offsets[px])\n",
        "                jaccard_scores.append(jaccard_score)\n",
        "\n",
        "            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
        "            tk0.set_postfix(jaccard=jaccards.avg)\n",
        "\n",
        "    return jaccards.avg\n",
        "\n",
        "\n",
        "def run(fold):\n",
        "    ## Lecture des données de l'entrainement\n",
        "    dfx = pd.read_csv(config.TRAINING_FILE)\n",
        "\n",
        "    ## Lecture des données de la validation\n",
        "    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n",
        "    ## Les types de tenseurs CUDA, qui implémentent la même fonction que les \n",
        "    ## tenseurs CPU, mais qui utilisent les GPU pour le calcul\n",
        "    device = torch.device('cuda')\n",
        "    ## le modèle hérite de la class PreTrainedModel\n",
        "    ## c'est un modéle pré-entrainé sur Squad2\n",
        "    ## le modéle précisé dans la class config c'est bien 'deepset/roberta-base-squad2' \n",
        "    model_config = transformers.RobertaConfig.from_pretrained(config.MODEL_CONFIG)\n",
        "    ##  Pour assurer que le modèle rendre tous les hidden_state (weights).\n",
        "    model_config.output_hidden_states = True\n",
        "    \n",
        "    ## Crée une instance de la classe TweetModel avec la config crée just avant\n",
        "    model = models.TweetModel(conf=model_config)\n",
        "    model.to(device)\n",
        "    ## La fonction load_state_dict() prend un objet du dictionnaire, tet elle\n",
        "    ## charge le state_dict sérialisé et sauvegardé du modèle\n",
        "    model.load_state_dict(torch.load(\n",
        "        f'{config.TRAINED_MODEL_PATH}/model_{fold}.bin'))\n",
        "    \n",
        "    ## model.eval() met le modèle en mode évaluation (il calcule pas les gradients)\n",
        "    model.eval()\n",
        "\n",
        "    ## Préparé les tweets de validation selon la methode dataset.TweetDataset() qui prépare toutes les données des tweets\n",
        "    valid_dataset = dataset.TweetDataset(\n",
        "        tweets=df_valid.text.values,\n",
        "        sentiments=df_valid.sentiment.values,\n",
        "        selected_texts=df_valid.selected_text.values)\n",
        "    ## chargement des données de validation en utilisant dataLoader de Pytorch \n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=config.VALID_BATCH_SIZE,\n",
        "        num_workers=4,\n",
        "        shuffle=False)\n",
        "    ## Le fait de définir l'argument num_workers comme un nombre entier positif\n",
        "    ## activera le chargement de données multiprocessus avec le nombre spécifié de processus de chargement des travailleurs\n",
        "    ## calculer l'indice de jaccard\n",
        "    jaccard = eval_fn(valid_data_loader, model, device)\n",
        "\n",
        "    return jaccard\n",
        "\n",
        "##  if __name__ == \"main\" ' bloc pour empêcher l'exécution de (certain) code lors\n",
        "##  de l'importation du module. En bref, __name__ est une variable définie pour \n",
        "##  chaque script qui définit si le script est exécuté en tant que module principal \n",
        "##  ou s'il est exécuté en tant que module importé.\n",
        "if __name__ == '__main__':\n",
        "    utils.seed_everything(config.SEED)\n",
        "    ## Lise qui va contenir le score de chaque folds\n",
        "    fold_scores = []\n",
        "    ## N_FOLDS est a 5\n",
        "    for i in range(config.N_FOLDS):\n",
        "        fold_score = run(i)\n",
        "        fold_scores.append(fold_score)\n",
        "    ## Afficher les score de chaque folds et le score moyen\n",
        "    for i in range(config.N_FOLDS):\n",
        "        print(f'Fold={i}, Jaccard = {fold_scores[i]}')\n",
        "    print(f'Mean = {np.mean(fold_scores)}')\n",
        "    print(f'Std = {np.std(fold_scores)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a5a6a4853c647d5ab4e6f2e421c5b21",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=571.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da3165f8a7bb4f1ba6002a3dd1839b23",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=496313727.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "311e02e4ab79455f96a6a69d882cf44c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=172.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e67e719ed34a4b07a0591d376313ceeb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=172.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c649b48206e8461ab986a31eff41663d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=172.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ae593eea4ea4bf29ccb9207bf39a987",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=172.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e34c097bd1a8400cab7a702b06a9143c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=172.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold=0, Jaccard = 0.7163026420895469\n",
            "Fold=1, Jaccard = 0.7108215431021135\n",
            "Fold=2, Jaccard = 0.7158955035127735\n",
            "Fold=3, Jaccard = 0.7166221978997807\n",
            "Fold=4, Jaccard = 0.7109265704332721\n",
            "Mean = 0.7141136914074974\n",
            "Std = 0.002655369837057321\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-Eedd3Jg3c6"
      },
      "source": [
        "**TORCH.UTILS.DATA**\n",
        "Au cœur de l'utilitaire de chargement de données PyTorch se trouve la classe ```torch.utils.data.DataLoader```. Elle représente un Python itérable sur un ensemble de données, avec le support de:</br>\n",
        "* map-style et iterable-style datasets,\n",
        "\n",
        "* la personnalisation de l'ordre de chargement des données,\n",
        "\n",
        "* le dosage automatique,\n",
        "\n",
        "* chargement de données à un ou plusieurs processus,\n",
        "\n",
        "* épinglage automatique de la mémoire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrP6RBNKFqd9"
      },
      "source": [
        "# infer.py\n",
        "\n",
        "---\n",
        "Ce code représente l'implémentation d'une Forward passe pour prédire le texte sélectionné (start/end_labels) des tweets donné, on peut dire que cela effectue la même tâche que la méthode .predict() en ML puisque ya pas la notion de l'apprentissage c'est just une propagation dans le modele et récupération de l'output.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtO5TLyqFp-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "459773e4-c234-4cfb-c1d4-9665f468a2f0"
      },
      "source": [
        "import pickle\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "import tqdm\n",
        "\n",
        "import config\n",
        "import models\n",
        "import dataset\n",
        "import utils\n",
        "\n",
        "\n",
        "def run():\n",
        "    df_test = pd.read_csv(config.TEST_FILE)\n",
        "    df_test.loc[:, 'selected_text'] = df_test.text.values\n",
        "\n",
        "    device = torch.device('cuda')\n",
        "    model_config = transformers.RobertaConfig.from_pretrained(\n",
        "        config.MODEL_CONFIG)\n",
        "    model_config.output_hidden_states = True\n",
        "\n",
        "    fold_models = []\n",
        "    for i in range(config.N_FOLDS):\n",
        "        model = models.TweetModel(conf=model_config)\n",
        "        model.to(device)\n",
        "        model.load_state_dict(torch.load(\n",
        "            f'{config.TRAINED_MODEL_PATH}/model_{i}.bin'),\n",
        "            strict=False)\n",
        "        model.eval()\n",
        "        fold_models.append(model)\n",
        "    ## TweetDataset est un map-style et iterable-style datasets\n",
        "    test_dataset = dataset.TweetDataset(\n",
        "        tweets=df_test.text.values,\n",
        "        sentiments=df_test.sentiment.values,\n",
        "        selected_texts=df_test.selected_text.values)\n",
        "    ## data_loader permet d'automatiser le chargement des données\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        shuffle=False,\n",
        "        batch_size=config.VALID_BATCH_SIZE,\n",
        "        num_workers=4) ## shuffle=False puisuqe on est en mode evaluation donc pas besoin d'un chuffle\n",
        "\n",
        "    char_pred_test_start = []\n",
        "    char_pred_test_end = []\n",
        "    ## Pas de calcu des gradiants, c'est un forward pass de notre modèle\n",
        "    with torch.no_grad():\n",
        "        tk0 = tqdm.tqdm(data_loader, total=len(data_loader))\n",
        "        for bi, d in enumerate(tk0):\n",
        "            ids = d['ids']\n",
        "            token_type_ids = d['token_type_ids']\n",
        "            mask = d['mask']\n",
        "            orig_tweet = d['orig_tweet']\n",
        "            offsets = d['offsets']\n",
        "\n",
        "            ids = ids.to(device, dtype=torch.long)\n",
        "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "\n",
        "            outputs_start_folds = []\n",
        "            outputs_end_folds = []\n",
        "            for i in range(config.N_FOLDS):\n",
        "                outputs_start, outputs_end = \\\n",
        "                    fold_models[i](ids=ids,\n",
        "                                   mask=mask,\n",
        "                                   token_type_ids=token_type_ids)\n",
        "                outputs_start_folds.append(outputs_start)\n",
        "                outputs_end_folds.append(outputs_end)\n",
        "\n",
        "            outputs_start = sum(outputs_start_folds) / config.N_FOLDS\n",
        "            outputs_end = sum(outputs_end_folds) / config.N_FOLDS\n",
        "\n",
        "            outputs_start = torch.softmax(outputs_start, dim=-1).cpu().detach().numpy()\n",
        "            outputs_end = torch.softmax(outputs_end, dim=-1).cpu().detach().numpy()\n",
        "            ## Affecter les prababilitées de l'output  outputs_start/outputs_end au char\n",
        "            ## pour passer au char level puisque le Transformers sont token level\n",
        "            ## chaque caractére prends la probavilitée affecté au token auquel il appartient\n",
        "            for px, tweet in enumerate(orig_tweet):\n",
        "                char_pred_test_start.append(\n",
        "                    utils.token_level_to_char_level(tweet, offsets[px], outputs_start[px]))\n",
        "                char_pred_test_end.append(\n",
        "                    utils.token_level_to_char_level(tweet, offsets[px], outputs_end[px]))\n",
        "    ## Serialiser et sauvegarder les output de la prédiction\n",
        "    with open('/content/drive/MyDrive/very_final/pickles/roberta-char_pred_test_start.pkl', 'wb') as handle:\n",
        "        pickle.dump(char_pred_test_start, handle)\n",
        "    with open('/content/drive/MyDrive/very_final/pickles/roberta-char_pred_test_end.pkl', 'wb') as handle:\n",
        "        pickle.dump(char_pred_test_end, handle)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 111/111 [01:02<00:00,  1.78it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUTPKlGDGpj8"
      },
      "source": [
        "# models.py\n",
        "\n",
        "---\n",
        "ce fichier contient une implémentation de la classe de modèle **TweetModel** qui héritée des transformateurs **BertPreTrainedModel** et la méthode forward qui récupère la sortie logits juste avant la couche des embeddings et effectue un Max-pooling et un Average_pooling\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0kVj0sFGo6k"
      },
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "import config\n",
        "\n",
        "\n",
        "class TweetModel(transformers.BertPreTrainedModel):\n",
        "    ## Instantaition du modele\n",
        "    def __init__(self, conf):\n",
        "        super(TweetModel, self).__init__(conf)\n",
        "        self.roberta = transformers.RobertaModel.from_pretrained(\n",
        "            config.MODEL_CONFIG,\n",
        "            config=conf)\n",
        "        self.high_dropout = torch.nn.Dropout(config.HIGH_DROPOUT)\n",
        "        self.classifier = torch.nn.Linear(config.HIDDEN_SIZE * 2, 2)\n",
        "\n",
        "        torch.nn.init.normal_(self.classifier.weight, std=0.02)\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        # sequence_output of N_LAST_HIDDEN + Embedding states\n",
        "        # (N_LAST_HIDDEN + 1, batch_size, num_tokens, 768)\n",
        "        _, _, out = self.roberta(ids, attention_mask=mask,\n",
        "                                 token_type_ids=token_type_ids)\n",
        "        \n",
        "        ## Récupére les valeus de toutes les couches sans la couche des embeddings.\n",
        "        out = torch.stack(\n",
        "            tuple(out[-i - 1] for i in range(config.N_LAST_HIDDEN)), dim=0)\n",
        "        ## Avg pooling\n",
        "        out_mean = torch.mean(out, dim=0)\n",
        "        ## Max pooling\n",
        "        out_max, _ = torch.max(out, dim=0)\n",
        "        out = torch.cat((out_mean, out_max), dim=-1)\n",
        "\n",
        "\n",
        "        # Multisample Dropout: https://arxiv.org/abs/1905.09788 expliqué just en bas\n",
        "        ## logit céest la couche qui vient just avant la couche Dense\n",
        "        logits = torch.mean(torch.stack([\n",
        "            self.classifier(self.high_dropout(out))\n",
        "            for _ in range(5)\n",
        "        ], dim=0), dim=0)\n",
        "        ## puique on'a deux output dans la loits (start_lable / end_label)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "\n",
        "        # (batch_size, num_tokens)\n",
        "        ## .squeeze() pou applatire (flatteniser) les Nd tensors\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        return start_logits, end_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5zCx9T1lqBD"
      },
      "source": [
        "**Multi Sample Dropout (MSD):** C'est l'une des techniques qu'ils ont utilisées et que je trouve si intéressante. En fait, il applique un dropout plusieurs fois avec différents masques et ensuite il calcule la moyenne des résultats</br>\n",
        "  Le dropout initial crée un sous-ensemble choisi au hasard (appelé dropout sample) à partir des données d'entrée de chaque itération d'entrainement, tandis que le MSD crée plusieurs échantillon de dropout. La loss est calculée pour chaque échantillon, puis la moyenne des losses des échantillons est calculée pour obtenir la Loss finale [(plus de détails ici)](https://arxiv.org/pdf/1905.09788.pdf).</br>\n",
        "  ![alt text](https://github.com/Amine-OMRI/tweet-sentiment-extraction-kaggle-compete-1st-place-detailed-solution/blob/main/Multi-Sample-Dropout.png?raw=true)</br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPit1dlVHEIs"
      },
      "source": [
        "# utils.py\n",
        "\n",
        "---\n",
        "\n",
        "le fichier **utils.py**contient toutes les implémentations de toutes les fonctions qui seront utilisées dans de nombreux fichiers de code, telle que :\n",
        "* la fonction qui fixe le seed global **seed_everything** \n",
        "la fonction qui calcule les probabilités de niveau de caractères token_level_to_char_level\n",
        "* la fonction qui calcule la métrique de l'évaluation mentionnée dans les énoncées de la competition, qui est **jaccard**\n",
        "* la fonction qui calcule le score final du Jaccard en utilisant les prédictions **calculate_jaccard**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xc2lgxiHTsQ"
      },
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "\n",
        "\n",
        "def token_level_to_char_level(text, offsets, preds):\n",
        "    probas_char = np.zeros(len(text))\n",
        "    for i, offset in enumerate(offsets):\n",
        "        if offset[0] or offset[1]:\n",
        "            probas_char[offset[0]:offset[1]] = preds[i]\n",
        "\n",
        "    return probas_char\n",
        "\n",
        "\n",
        "def jaccard(str1, str2):\n",
        "    \"\"\"Original metric implementation.\"\"\"\n",
        "    a = set(str1.lower().split())\n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "\n",
        "def get_best_start_end_idx(start_logits, end_logits,\n",
        "                           orig_start, orig_end):\n",
        "    \"\"\"Return best start and end indices following BERT paper.\"\"\"\n",
        "    best_logit = -np.inf\n",
        "    best_idxs = None\n",
        "    start_logits = start_logits[orig_start:orig_end + 1]\n",
        "    end_logits = end_logits[orig_start:orig_end + 1]\n",
        "    for start_idx, start_logit in enumerate(start_logits):\n",
        "        for end_idx, end_logit in enumerate(end_logits[start_idx:]):\n",
        "            logit_sum = start_logit + end_logit\n",
        "            if logit_sum > best_logit:\n",
        "                best_logit = logit_sum\n",
        "                best_idxs = (orig_start + start_idx,\n",
        "                             orig_start + start_idx + end_idx)\n",
        "    return best_idxs\n",
        "\n",
        "\n",
        "def calculate_jaccard(original_tweet, target_string,\n",
        "                      start_logits, end_logits,\n",
        "                      orig_start, orig_end,\n",
        "                      offsets, \n",
        "                      verbose=False):\n",
        "    \"\"\"Calculates final Jaccard score using predictions.\"\"\"\n",
        "    start_idx, end_idx = get_best_start_end_idx(\n",
        "        start_logits, end_logits, orig_start, orig_end)\n",
        "\n",
        "    filtered_output = ''\n",
        "    for ix in range(start_idx, end_idx + 1):\n",
        "        filtered_output += original_tweet[offsets[ix][0]:offsets[ix][1]]\n",
        "        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n",
        "            filtered_output += ' '\n",
        "\n",
        "    # Return orig tweet if it has less then 2 words\n",
        "    if len(original_tweet.split()) < 2:\n",
        "        filtered_output = original_tweet\n",
        "\n",
        "    if len(filtered_output.split()) == 1:\n",
        "        filtered_output = filtered_output.replace('!!!!', '!')\n",
        "        filtered_output = filtered_output.replace('..', '.')\n",
        "        filtered_output = filtered_output.replace('...', '.')\n",
        "\n",
        "    filtered_output = filtered_output.replace('ïï', 'ï')\n",
        "    filtered_output = filtered_output.replace('¿¿', '¿')\n",
        "\n",
        "    jac = jaccard(target_string.strip(), filtered_output.strip())\n",
        "    return jac, filtered_output\n",
        "\n",
        "\n",
        "class AverageMeter:\n",
        "    \"\"\"Computes and stores the average and current value.\"\"\"\n",
        "    ## Permet de stocke la valeur moyenne actuelle et applique deux méthodes :\n",
        "    ## reset : qui remet toutes les valeurs à zéro \n",
        "    ## update : qui met à jour l'objet en y ajoutant de nouvelles valeurs, ici il s'agit de la valeur de la loss\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDzmFycLG6Rc"
      },
      "source": [
        "#train.py\n",
        "\n",
        "---\n",
        "Ce fichier de code exécute l'entrainement sur les données d'entranement et de la validation sur les données de validation et affiche les valeurs de jaccard et la loss\n",
        "\n",
        "1) Il a utilisé le GPU Colab Pro pour RoBERTa-large et il a fallu environ 6h pour l'entraîner avec 5 folds et 4 époques sans optimisation particulière. \n",
        "\n",
        "[Hikkiiii](https://www.kaggle.com/wochidadonggua) a également entraîner RoBERTa-large, 2V100, APEX(O1), il a fallu environ 220s par époque \n",
        "\n",
        "2) RoBERTa-base-squad2 est disponible pré-entrainé par [HuggingFace.](https://huggingface.co/deepset/roberta-base-squad2)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkXID0zIxpE5"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import torch\n",
        "import torchcontrib\n",
        "\n",
        "import config\n",
        "import dataset\n",
        "import models\n",
        "import engine\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HT0b6QoG61j"
      },
      "source": [
        "Une autre technique qui a été utilisée comme **Optimiser** l'est :\n",
        "* **SWA :** la technique SWA [(Stochastic Weight Averaging)](https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/) récemment proposée, et sa nouvelle implémentation dans torchcontrib. La SWA est une procédure simple qui améliore la généralisation du deep learning sur la Descente de Gradient Stochastique (SGD) sans coût supplémentaire, et peut être utilisée en remplacement de tout autre **optimiseur dans PyTorch**. Le SWA a une large gamme d'applications et de fonctionnalités.</br>\n",
        "Il a été démontré que SWA améliore considérablement la généralisation des tâches de vision par ordinateur, y compris les VGG, les ResNets, les Wide ResNets et les DenseNets sur ImageNet et les CIFAR benchmarks.\n",
        "\n",
        "En bref, le SWA effectue une moyenne égale des poids traversés par le SGD avec un programme d'apprentissage modifié."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1023bec17f3048f2994587ab2ca923ff",
            "4f836dfd3d614d37a6e12828b9e26dea",
            "0288fee9c3674c198d5e21621aac5273",
            "56eabcc0b14d4fa6a0b5b2323b3a27a5",
            "9c97ceae839b48029d9f89ed8021d1a2",
            "d0732c192a784c53b86a68f3a16254a6",
            "556a4510c3234b38a87c3743b8df9c97",
            "ec4bd0f01be341e88fc41673825fbaca",
            "2d6a2ea3e1454d28bd5a9760e19a4ca7",
            "6cad7320919248cab53cba7a3eb47225",
            "18287140c5324a7e85b01460e5e889f5",
            "fb4870326d1545dcaab261730965bc99",
            "11cd33748ed9445c921d533c922a2128",
            "4ae80b3c967142b1b773b080ed310264",
            "31bf737378c04156a4ced60a3b781572",
            "8344b11a1c42484883bb312ebfa677c6"
          ]
        },
        "id": "xS47DeGUGyt1",
        "outputId": "1bcfe209-c4d3-425e-a5a4-811fac7f4167"
      },
      "source": [
        "## Ce code permet de lancé l'entrainement sur les 5 folds \n",
        "def run(fold):\n",
        "   \n",
        "    dfx = pd.read_csv(config.TRAINING_FILE)\n",
        "    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n",
        "    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n",
        "\n",
        "    train_dataset = dataset.TweetDataset(\n",
        "        tweets=df_train.text.values,\n",
        "        sentiments=df_train.sentiment.values,\n",
        "        selected_texts=df_train.selected_text.values)\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.TRAIN_BATCH_SIZE,\n",
        "        num_workers=4,\n",
        "        shuffle=True)\n",
        "\n",
        "    valid_dataset = dataset.TweetDataset(\n",
        "        tweets=df_valid.text.values,\n",
        "        sentiments=df_valid.sentiment.values,\n",
        "        selected_texts=df_valid.selected_text.values)\n",
        "\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=config.VALID_BATCH_SIZE,\n",
        "        num_workers=4,\n",
        "        shuffle=False)\n",
        "\n",
        "    device = torch.device('cuda')\n",
        "    model_config = transformers.RobertaConfig.from_pretrained(\n",
        "        config.MODEL_CONFIG)\n",
        "    model_config.output_hidden_states = True\n",
        "    model = models.TweetModel(conf=model_config)\n",
        "    model = model.to(device)\n",
        "    print(\"------------------> here\")\n",
        "    \n",
        "    ## Nombre d'iteration c'est le nombre des données d'entré (tweets) divisé par\n",
        "    ## la taille du batch dans le fichier config.py\n",
        "    num_train_steps = int(\n",
        "        len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
        "    ## Récupérer les paramètres de l'optimizer\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "\n",
        "    ## puisqu'il est recommandé d'utiliser cet optimiseur pour le fine tuning\n",
        "    ## (modification sur l'architecture), puisque c'est ainsi que le modèle a \n",
        "    ## été entraîné et de conserver les mêmes comportements que ceux mentionnés \n",
        "    ## sur le repo git de BERT (https://github.com/google-research/bert/blob/master/optimization.py#L65)\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    optimizer_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer\n",
        "                    if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': config.WEIGHT_DECAY},\n",
        "        {'params': [p for n, p in param_optimizer\n",
        "                    if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': 0.0}]\n",
        "    ## AdamW: un optimiseur adaptatif avec utilisation d'une échelle de taux\n",
        "    ## d'apprentissage pour moduler l'évolution du taux d'apprentissage de\n",
        "    ## l'optimiseur en fonction du temps \n",
        "    base_opt = transformers.AdamW(optimizer_parameters,\n",
        "                                  lr=config.LEARNING_RATE)\n",
        "    ## SWA : la technique SWA (Stochastic Weight Averaging) est présenter au dessus de cette cellule\n",
        "    optimizer = torchcontrib.optim.SWA(\n",
        "        base_opt,\n",
        "        swa_start=int(num_train_steps * config.SWA_RATIO),\n",
        "        swa_freq=config.SWA_FREQ,\n",
        "        swa_lr=None)\n",
        "    \n",
        "    scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=int(num_train_steps * config.WARMUP_RATIO),\n",
        "        num_training_steps=num_train_steps)\n",
        "\n",
        "    print(f'Training is starting for fold={fold}')\n",
        "\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        engine.train_fn(train_data_loader, model, optimizer,device, scheduler=scheduler)\n",
        "        jaccard = engine.eval_fn(valid_data_loader, model, device)\n",
        "\n",
        "    if config.USE_SWA:\n",
        "        optimizer.swap_swa_sgd()\n",
        "\n",
        "    torch.save(model.state_dict(),\n",
        "               f'{config.MODEL_SAVE_PATH}/model_{fold}.bin')\n",
        "\n",
        "    return jaccard\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    utils.seed_everything(seed=config.SEED)\n",
        "\n",
        "    fold_scores = []\n",
        "    for i in range(config.N_FOLDS):\n",
        "        fold_score = run(i)\n",
        "        fold_scores.append(fold_score)\n",
        "\n",
        "    print('\\nScores without SWA:')\n",
        "    for i in range(config.N_FOLDS):\n",
        "        print(f'Fold={i}, Jaccard = {fold_scores[i]}')\n",
        "    print(f'Mean = {np.mean(fold_scores)}')\n",
        "    print(f'Std = {np.std(fold_scores)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1023bec17f3048f2994587ab2ca923ff",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=571.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d6a2ea3e1454d28bd5a9760e19a4ca7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=496313727.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------> here\n",
            "Training is starting for fold=0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "100%|██████████| 687/687 [07:04<00:00,  1.62it/s, loss=0.0218]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.92it/s, jaccard=0.703, loss=0.01]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7029905612369929\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:14<00:00,  1.58it/s, loss=0.0104]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.92it/s, jaccard=0.712, loss=0.00929]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7118255485009135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.00902]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.93it/s, jaccard=0.717, loss=0.00939]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7170824580463284\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.00808]\n",
            "100%|██████████| 172/172 [00:35<00:00,  4.89it/s, jaccard=0.716, loss=0.00943]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7163026420895469\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------> here\n",
            "Training is starting for fold=1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
            "100%|██████████| 687/687 [07:14<00:00,  1.58it/s, loss=0.0196]\n",
            "100%|██████████| 172/172 [00:35<00:00,  4.90it/s, jaccard=0.696, loss=0.0099]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.6963974280475891\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.59it/s, loss=0.01]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.94it/s, jaccard=0.705, loss=0.00921]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7054961819176117\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.0088]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.94it/s, jaccard=0.711, loss=0.00945]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7110974016990879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:14<00:00,  1.58it/s, loss=0.00799]\n",
            "100%|██████████| 172/172 [00:35<00:00,  4.90it/s, jaccard=0.711, loss=0.00939]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7108215431021135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------> here\n",
            "Training is starting for fold=2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.0199]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.95it/s, jaccard=0.701, loss=0.0105]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7007900525880214\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.59it/s, loss=0.0103]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.92it/s, jaccard=0.707, loss=0.00936]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7070440963345973\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.00895]\n",
            "100%|██████████| 172/172 [00:35<00:00,  4.91it/s, jaccard=0.713, loss=0.00921]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7134881141772793\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.00821]\n",
            "100%|██████████| 172/172 [00:35<00:00,  4.91it/s, jaccard=0.716, loss=0.00934]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7158955035127735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------> here\n",
            "Training is starting for fold=3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
            "100%|██████████| 687/687 [07:14<00:00,  1.58it/s, loss=0.0211]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.92it/s, jaccard=0.694, loss=0.0106]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.6944610752763127\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.0102]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.93it/s, jaccard=0.709, loss=0.00972]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7094754330307022\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.00889]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.92it/s, jaccard=0.717, loss=0.00957]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7172411822420536\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.00803]\n",
            "100%|██████████| 172/172 [00:35<00:00,  4.91it/s, jaccard=0.717, loss=0.00964]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7166221978997807\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------> here\n",
            "Training is starting for fold=4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
            "100%|██████████| 687/687 [07:14<00:00,  1.58it/s, loss=0.0203]\n",
            "100%|██████████| 172/172 [00:35<00:00,  4.90it/s, jaccard=0.695, loss=0.0104]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.694875414018416\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.0108]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.93it/s, jaccard=0.703, loss=0.0103]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.70296628740658\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:13<00:00,  1.58it/s, loss=0.0094]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.92it/s, jaccard=0.707, loss=0.00943]\n",
            "  0%|          | 0/687 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7067339098954668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 687/687 [07:14<00:00,  1.58it/s, loss=0.00862]\n",
            "100%|██████████| 172/172 [00:34<00:00,  4.94it/s, jaccard=0.711, loss=0.00943]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard = 0.7109265704332721\n",
            "\n",
            "Scores without SWA:\n",
            "Fold=0, Jaccard = 0.7163026420895469\n",
            "Fold=1, Jaccard = 0.7108215431021135\n",
            "Fold=2, Jaccard = 0.7158955035127735\n",
            "Fold=3, Jaccard = 0.7166221978997807\n",
            "Fold=4, Jaccard = 0.7109265704332721\n",
            "Mean = 0.7141136914074974\n",
            "Std = 0.002655369837057321\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}